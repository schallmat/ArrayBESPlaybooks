{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BES Playbooks The Playbooks will support development and modernization efforts within the BES AFLCMC/HIA Logistic Division as it transforms its methodologies to procure, develop and implement specialized IT products using Agile methodology and best practices as specified via Enterprise Logistics IT.","title":"BES Playbooks"},{"location":"#bes-playbooks","text":"The Playbooks will support development and modernization efforts within the BES AFLCMC/HIA Logistic Division as it transforms its methodologies to procure, develop and implement specialized IT products using Agile methodology and best practices as specified via Enterprise Logistics IT.","title":"BES Playbooks"},{"location":"agile/","text":"1 Overview 1.1 Purpose Agile Playbook Benefits Consistent use of BEST practices and templates Establishes AF Agile Body of Knowledge - continually improved by AF community Provides \"Quick Start\" for new programs adopting agile working within AF Provides non-technical professionals with an understanding of how to use agile Builds confidence and effective communications across Government-Industry team Identifies and reduces or eliminates barriers Transforms culture from one focused on traditional processes to one focused on timely customer capability delivery. 1.2 Audience While this Agile Playbook provides value to all personnel involved in a new agile project startup, the primary audience for this document is government project management teams determining how to enable an agile development environment. From that perspective, the Agile Playbook seeks to enable the government team for a startup or in-progress project to gain a better understanding of the steps necessary to proactively establish the regulatory and process guidelines that are conducive to an agile approach. To do this, the agile playbook will focus primarily on providing a better understanding of the following areas and how they enable the success of an agile project: -Contracts -Resources -Communications 1.3 References DoDI 5000.75, Business Systems Requirements and Acquisition, Feb 2, 2017 US Air Force Enterprise Logistics Flight Plan v2.0 (ELFP), April 2016 Enterprise Logistics Technology Annex v1.0 (ELFP), June 2016 AFGM2018-63-146-01 Rapid Acquisition Activities 13 June 2018 AFMAN 63-144 Business Capability Requirements, Compliance, and System Acquisition 25 July 2018 AFPAM 63-123 Product Support Business Case Analysis 1 July 2017","title":"CDRL 0001 Agile - 1 Overview"},{"location":"agile/#1-overview","text":"","title":"1 Overview"},{"location":"agile/#11-purpose","text":"","title":"1.1 Purpose"},{"location":"agile/#agile-playbook-benefits","text":"Consistent use of BEST practices and templates Establishes AF Agile Body of Knowledge - continually improved by AF community Provides \"Quick Start\" for new programs adopting agile working within AF Provides non-technical professionals with an understanding of how to use agile Builds confidence and effective communications across Government-Industry team Identifies and reduces or eliminates barriers Transforms culture from one focused on traditional processes to one focused on timely customer capability delivery.","title":"Agile Playbook Benefits"},{"location":"agile/#12-audience","text":"While this Agile Playbook provides value to all personnel involved in a new agile project startup, the primary audience for this document is government project management teams determining how to enable an agile development environment. From that perspective, the Agile Playbook seeks to enable the government team for a startup or in-progress project to gain a better understanding of the steps necessary to proactively establish the regulatory and process guidelines that are conducive to an agile approach. To do this, the agile playbook will focus primarily on providing a better understanding of the following areas and how they enable the success of an agile project: -Contracts -Resources -Communications","title":"1.2 Audience"},{"location":"agile/#13-references","text":"DoDI 5000.75, Business Systems Requirements and Acquisition, Feb 2, 2017 US Air Force Enterprise Logistics Flight Plan v2.0 (ELFP), April 2016 Enterprise Logistics Technology Annex v1.0 (ELFP), June 2016 AFGM2018-63-146-01 Rapid Acquisition Activities 13 June 2018 AFMAN 63-144 Business Capability Requirements, Compliance, and System Acquisition 25 July 2018 AFPAM 63-123 Product Support Business Case Analysis 1 July 2017","title":"1.3 References"},{"location":"agileappend_a/","text":"6 APPENDIX A - KEY EVENTS PLAYS 6.1 Scrum 6.1.1 Backlog Refinement Formal Collaboration Session 6.1.2 Sprint Planning 6.1.3 Sprint Execution 6.1.4 Sprint Review 6.1.5 Sprint Retrospective 6.2 Kanban 6.2.1 Strategy Review/Release Management 6.2.2 Stand-up 6.2.3 Replenishment Meeting 6.2.4 Delivery Planning Meeting 6.2.5 Operations Review 6.2.6 Service Delivery Review 6.2.7 Risk Review 6.3 XP 6.3.1 Backlog Refinement Formal Collaboration Session 6.3.2 Iteration Planning 6.3.3 Iteration 6.3.4 Iteration Review 6.3.5 Iteration Retrospective","title":"-6 Appendix A - Key Events Plays"},{"location":"agileappend_a/#6-appendix-a-key-events-plays","text":"","title":"6 APPENDIX A - KEY EVENTS PLAYS"},{"location":"agileappend_a/#61-scrum","text":"","title":"6.1 Scrum"},{"location":"agileappend_a/#611-backlog-refinement-formal-collaboration-session","text":"","title":"6.1.1 Backlog Refinement Formal Collaboration Session"},{"location":"agileappend_a/#612-sprint-planning","text":"","title":"6.1.2 Sprint Planning"},{"location":"agileappend_a/#613-sprint-execution","text":"","title":"6.1.3 Sprint Execution"},{"location":"agileappend_a/#614-sprint-review","text":"","title":"6.1.4 Sprint Review"},{"location":"agileappend_a/#615-sprint-retrospective","text":"","title":"6.1.5 Sprint Retrospective"},{"location":"agileappend_a/#62-kanban","text":"","title":"6.2 Kanban"},{"location":"agileappend_a/#621-strategy-reviewrelease-management","text":"","title":"6.2.1 Strategy Review/Release Management"},{"location":"agileappend_a/#622-stand-up","text":"","title":"6.2.2 Stand-up"},{"location":"agileappend_a/#623-replenishment-meeting","text":"","title":"6.2.3 Replenishment Meeting"},{"location":"agileappend_a/#624-delivery-planning-meeting","text":"","title":"6.2.4 Delivery Planning Meeting"},{"location":"agileappend_a/#625-operations-review","text":"","title":"6.2.5 Operations Review"},{"location":"agileappend_a/#626-service-delivery-review","text":"","title":"6.2.6 Service Delivery Review"},{"location":"agileappend_a/#627-risk-review","text":"","title":"6.2.7 Risk Review"},{"location":"agileappend_a/#63-xp","text":"","title":"6.3 XP"},{"location":"agileappend_a/#631-backlog-refinement-formal-collaboration-session","text":"","title":"6.3.1 Backlog Refinement Formal Collaboration Session"},{"location":"agileappend_a/#632-iteration-planning","text":"","title":"6.3.2 Iteration Planning"},{"location":"agileappend_a/#633-iteration","text":"","title":"6.3.3 Iteration"},{"location":"agileappend_a/#634-iteration-review","text":"","title":"6.3.4 Iteration Review"},{"location":"agileappend_a/#635-iteration-retrospective","text":"","title":"6.3.5 Iteration Retrospective"},{"location":"agileappend_b/","text":"7 APPENDIX B - KEY PERSONNEL PLAYS 7.1 Agile Lead 7.2 Product Owner 7.3 Business Analyst","title":"-7 Appendix B - Key Personnel Plays"},{"location":"agileappend_b/#7-appendix-b-key-personnel-plays","text":"","title":"7 APPENDIX B - KEY PERSONNEL PLAYS"},{"location":"agileappend_b/#71-agile-lead","text":"","title":"7.1 Agile Lead"},{"location":"agileappend_b/#72-product-owner","text":"","title":"7.2 Product Owner"},{"location":"agileappend_b/#73-business-analyst","text":"","title":"7.3 Business Analyst"},{"location":"agileappend_c/","text":"8 APPENDIX C - KEY LINKS The following links are intended to provide further clarification on some of the prevailing concepts within this playbook. 8.1 Agile in General 8.2 Agile Tools: 8.2.1 VSTS / TFS Overview: https://msdn.microsoft.com/en-us/library/ms364062(v=vs.80).aspx Backlog Management: Work Mangement: Communication Management: Continuous Integration: 8.2.2 Atlassian (JIRA) Overview: https://confluence.atlassian.com/jirasoftwarecloud/jira-software-overview-779293724.html Backlog Management: Work Management: Communication Management: Continuous Integration:","title":"-8 Appendix C - Key Links"},{"location":"agileappend_c/#8-appendix-c-key-links","text":"The following links are intended to provide further clarification on some of the prevailing concepts within this playbook.","title":"8 APPENDIX C - KEY LINKS"},{"location":"agileappend_c/#81-agile-in-general","text":"","title":"8.1 Agile in General"},{"location":"agileappend_c/#82-agile-tools","text":"","title":"8.2 Agile Tools:"},{"location":"agileappend_c/#821-vsts-tfs","text":"Overview: https://msdn.microsoft.com/en-us/library/ms364062(v=vs.80).aspx Backlog Management: Work Mangement: Communication Management: Continuous Integration:","title":"8.2.1 VSTS / TFS"},{"location":"agileappend_c/#822-atlassian-jira","text":"Overview: https://confluence.atlassian.com/jirasoftwarecloud/jira-software-overview-779293724.html Backlog Management: Work Management: Communication Management: Continuous Integration:","title":"8.2.2 Atlassian (JIRA)"},{"location":"agileappend_d/","text":"9 APPENDIX D - AGILE TERMINOLOGY Source: https://www.agilealliance.org/agile101/agile-glossary/ A Acceptance Test Driven Development (ATDD) Acceptance Test Driven Development (ATDD) involves team members with different perspectives (customer, development, testing) collaborating to write acceptance tests in advance of implementing the corresponding functionality. Acceptance Testing An acceptance test is a formal description of the behavior of a software product, generally expressed as an example or a usage scenario. A number of different notations and approaches have been proposed for such examples or scenarios. In many cases the aim is that it should be possible to automate the execution of such tests by a software tool, either ad-hoc to the development team or off the shelf. Antipattern Antipatterns are common solutions to common problems where the solution is ineffective and may result in undesired consequences. Automated Build In the context of software development, build refers to the process that converts files and other assets under the developers' responsibility into a software product in its final or consumable form. The build is automated when these steps are repeatable, require no direct human intervention, and can be performed at any time with no information other than what is stored in the source code control repository. B Backlog A backlog is an ordered list of items representing everything that may be needed to deliver a specific outcome. There are different types of backlogs depending on the type of item they contain and the approach being used. Backlog Grooming Backlog grooming is when the PO and some, or all, of the rest of the team refine the backlog on a regular basis to ensure the backlog contains the appropriate items, that they are prioritized, and that the items at the top of the backlog are ready for delivery. Behavior Driven Development (BDD) BDD is a practice where members of the team discuss the expected behavior of a system in order to build a shared understanding of expected functionality. Burndown Chart Burndown charts and burnup charts track the amount of output (in terms of hours, story points, or backlog items) a team has completed across an iteration or a project. Business Agility Business agility is the ability of an organization to sense changes internally or externally and respond accordingly in order to deliver value to its customers. C Collective Ownership Collective code ownership is the explicit convention that every team member can make changes to any code file as necessary: either to complete a development task, to repair a defect, or to improve the code's overall structure. Continuous Deployment Continuous deployment aims to reduce the time elapsed between writing a line of code and making that code available to users in production. To achieve continuous deployment, the team relies on infrastructure that automates and instruments the various steps leading up to deployment, so that after each integration successfully meeting these release criteria, the live application is updated with new code. Continuous Integration Continuous Integration is the practice of merging code changes into a shared repository several times a day in order to release a product version at any moment. This requires an integration procedure which is reproducible and automated. CRC Cards Class Responsibility Collaborator (CRC) Cards are an object oriented design technique teams can use to discuss what a class should know and do and what other classes it interacts with. Customer Development Customer development is a four-step framework that provides a way to use a scientific approach to validate assumptions about your product and business. D Daily Meeting The daily meeting is one of the most commonly practiced Agile techniques and presents opportunity for a team to get together on a regular basis to coordinate their activities. Definition of Done The definition of done is an agreed upon list of the activities deemed necessary to get a product increment, usually represented by a user story, to a done state by the end of a sprint. Definition of Ready Definition of Ready involves creating clear criteria that a user story must meet before being accepted into an upcoming iteration. This is typically based on the INVEST matrix. E Epic An epic is a large user story. Estimation In software development, an \"estimate\" is the evaluation of the effort necessary to carry out a given development task; this is most often expressed in terms of duration. Exploratory Testing Exploratory testing is, more than strictly speaking a \"practice,\" a style or approach to testing software which is often contrasted to \"scripted testing.\" Extreme Programming Extreme Programming (XP) is an agile software development framework that aims to produce higher quality software, and higher quality of life for the development team. XP is the most specific of the agile frameworks regarding appropriate engineering practices for software development. F Facilitation A facilitator is a person who chooses or is given the explicit role of conducting a meeting. Frequent Releases An Agile team frequently releases its product into the hands of end users, listening to feedback, whether critical or appreciative. G Given When Then The Given-When-Then formula is a template intended to guide the writing of acceptance tests for a User Story: (Given) some context, (When) some action is carried out, (Then) a particular set of observable consequences should obtain. H Heartbeat Retrospective The team meets regularly to reflect on the most significant events that occurred since the previous such meeting, and identify opportunities for improvement. I Incremental Development In an Agile context, Incremental Development is when each successive version of a product is usable, and each builds upon the previous version by adding user-visible functionality. Information Radiators \"Information radiator\" is the term for any of a number of visual displays which a team places in a highly visible location, so that all team members can see the latest information at a glance. Integration \"Integration\" (or \"integrating\") refers to any efforts still required for a project team to deliver a product suitable for release as a functional whole. INVEST The acronym INVEST stands for a set of criteria used to assess the quality of a user story. If the story fails to meet one of these criteria, the team may want to reword it. Iteration An iteration is a timebox during which development takes place. The duration may vary from project to project and is usually fixed. Iterative Development Agile projects are iterative insofar as they intentionally allow for \"repeating\" software development activities, and for potentially \"revisiting\" the same work products (the phrase \"planned rework\" is sometimes used; refactoring is a good example). K Kanban The Kanban Method is a means to design, manage and improve flow for knowledge work and allows teams to start where they are to drive evolutionary change. Kanban Board A Kanban Board is a visual workflow tool consisting of multiple columns. Each column represents a different stage in the workflow process. L Lead Time Lead Time is the time between a customer order and delivery. In software development, it can also be the time between a requirement made and its fulfillment. M Milestone Retrospective A Milestone Retrospective is a team's detailed analysis of the project's significant events after a set period of time or at the project's end. Minimum Marketable Feature (MMF) A Minimum Marketable Feature is a small, self-contained feature that can be developed quickly and that delivers significant value to the user. Minimum Viable Product (MVP) A Minimum Viable Product is, as Eric Ries said, the \"version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort.\" Mob Programming Mob Programming is a software development approach where the whole team works on the same thing, at the same time, in the same space, and at the same computer. Mock Objects Mock Objects (commonly used in the context of crafting automated unit tests) consist of instantiating a test-specific version of a software component. N Niko-niko Calendar A Niko-niko Calendar is updated daily with each team member's mood for that day. Over time the calendar reveals patterns of change in the moods of the team, or of individual members. P Pair Programming Pair programming consists of two programmers sharing a single workstation (one screen, keyboard and mouse among the pair). Personas Personas are synthetic biographies of fictitious users of the future product. Planning Poker An approach to estimation used by Agile teams. Each team member \"plays\" a card bearing a numerical value corresponding to a point estimation for a user story. Points (estimates in) Agile teams generally prefer to express estimates in units other than the time-honored \"man-hours.\" Possibly the most widespread unit is \"story points.\" Product Owner (PO) The PO is a role created by the Scrum Framework responsible for making sure the team delivers the desired outcome. Project Chartering A high-level summary of the project's key success factors displayed on one wall of the team room as a flipchart-sized sheet of paper. Q Quick Design Session When \"simple design\" choices have far-reaching consequences, two or more developers meet for a quick design session at a whiteboard. R Refactoring Refactoring consists of improving the internal structure of an existing program's source code, while preserving its external behavior. Relative Estimation Relative estimation consists of estimating tasks or user stories by comparison or by grouping of items of equivalent difficulty. Role-feature-reason The \"role-feature-reason\" template is one of the most commonly recommended aids to write user stories: As a ... I want ... So that ... Rule of Simplicity Rules of Simplicity is a set of criteria, in priority order, proposed by Kent Beck to judge whether some source code is \"simple enough.\" S Scrum Scrum is a process framework used to manage product development and other knowledge work. Scrum Master The scrum master is responsible for ensuring the team lives agile values and principles and follows the practices that the team agreed they would use. Scrum of Scrums A technique to scale Scrum up to large groups (over a dozen people), consisting of dividing the groups into Agile teams of 5-10. Sign Up for Tasks Members of an Agile development team normally choose which tasks to work on, rather than being assigned work by a manager. Simple Design A team adopting the \"simple design\" practice bases its software design strategy on a set of \"simple design\" principles. Sprint Planning Sprint planning is an event that occurs at the beginning of a sprint where the team determines the product backlog items they will work on during that sprint. Story Mapping Story mapping consists of ordering user stories along two independent dimensions. Story Splitting Splitting consists of breaking up one user story into smaller ones, while preserving the property that each user story separately has measurable business value. Sustainable Pace The team aims for a work pace that they would be able to sustain indefinitely. T Task Board The most basic form of a task board is divided into three columns labeled \"To Do,\" \"In Progress,\" and \"Done.\" Cards are placed in the columns to reflect the current status of that task. Test Driven Development (TDD) \"Test-driven development\" is a style of programming in which three activities are tightly interwoven: coding, testing (in the form of writing unit tests) and design (in the form of refactoring). Team A \"team\" in the Agile sense is a small group of people, assigned to the same project or effort, nearly all of them on a full-time basis. Team Room The team (ideally the whole team, including the PO or domain expert) has the use of a dedicated space for the duration of the project, set apart from other groups' activities. Three C's \"Card, Conversation, Confirmation\" is a formula that captures the components of a User Story. Three Amigos Three amigos refers to the primary perspectives to examine an increment of work before, during, and after development. Those perspectives are Business, Development, and Testing. Three Questions The daily meeting is structured around some variant of the following three questions: What have you completed? What will you do next? What is getting in your way? Timebox A timebox is a previously agreed period of time during which a person or a team works steadily towards completion of some goal. U Ubiquitous Language Striving to use the vocabulary of a given business domain, not only in discussions about the requirements for a software product, but in discussions of design as well and all the way into \"the product's source code itself.\" Unit Testing A unit test is a short program fragment written and maintained by the developers on the product team, which exercises some narrow part of the product's source code and checks the results. Usability Testing Usability testing is an empirical, exploratory technique to answer questions such as \"how would an end user respond to our software under realistic conditions?\" User Stories In consultation with the customer or PO, the team divides up the work to be done into functional increments called \"user stories.\" V Velocity At the end of each iteration, the team adds up effort estimates associated with user stories that were completed during that iteration. This total is called velocity. Version Control Version control is not strictly an Agile \"practice\" insofar as it is now widespread in the industry as a whole. But it is mentioned here for several reasons.","title":"-9 Appendix D - Agile Terminology"},{"location":"agileappend_d/#9-appendix-d-agile-terminology","text":"Source: https://www.agilealliance.org/agile101/agile-glossary/ A Acceptance Test Driven Development (ATDD) Acceptance Test Driven Development (ATDD) involves team members with different perspectives (customer, development, testing) collaborating to write acceptance tests in advance of implementing the corresponding functionality. Acceptance Testing An acceptance test is a formal description of the behavior of a software product, generally expressed as an example or a usage scenario. A number of different notations and approaches have been proposed for such examples or scenarios. In many cases the aim is that it should be possible to automate the execution of such tests by a software tool, either ad-hoc to the development team or off the shelf. Antipattern Antipatterns are common solutions to common problems where the solution is ineffective and may result in undesired consequences. Automated Build In the context of software development, build refers to the process that converts files and other assets under the developers' responsibility into a software product in its final or consumable form. The build is automated when these steps are repeatable, require no direct human intervention, and can be performed at any time with no information other than what is stored in the source code control repository. B Backlog A backlog is an ordered list of items representing everything that may be needed to deliver a specific outcome. There are different types of backlogs depending on the type of item they contain and the approach being used. Backlog Grooming Backlog grooming is when the PO and some, or all, of the rest of the team refine the backlog on a regular basis to ensure the backlog contains the appropriate items, that they are prioritized, and that the items at the top of the backlog are ready for delivery. Behavior Driven Development (BDD) BDD is a practice where members of the team discuss the expected behavior of a system in order to build a shared understanding of expected functionality. Burndown Chart Burndown charts and burnup charts track the amount of output (in terms of hours, story points, or backlog items) a team has completed across an iteration or a project. Business Agility Business agility is the ability of an organization to sense changes internally or externally and respond accordingly in order to deliver value to its customers. C Collective Ownership Collective code ownership is the explicit convention that every team member can make changes to any code file as necessary: either to complete a development task, to repair a defect, or to improve the code's overall structure. Continuous Deployment Continuous deployment aims to reduce the time elapsed between writing a line of code and making that code available to users in production. To achieve continuous deployment, the team relies on infrastructure that automates and instruments the various steps leading up to deployment, so that after each integration successfully meeting these release criteria, the live application is updated with new code. Continuous Integration Continuous Integration is the practice of merging code changes into a shared repository several times a day in order to release a product version at any moment. This requires an integration procedure which is reproducible and automated. CRC Cards Class Responsibility Collaborator (CRC) Cards are an object oriented design technique teams can use to discuss what a class should know and do and what other classes it interacts with. Customer Development Customer development is a four-step framework that provides a way to use a scientific approach to validate assumptions about your product and business. D Daily Meeting The daily meeting is one of the most commonly practiced Agile techniques and presents opportunity for a team to get together on a regular basis to coordinate their activities. Definition of Done The definition of done is an agreed upon list of the activities deemed necessary to get a product increment, usually represented by a user story, to a done state by the end of a sprint. Definition of Ready Definition of Ready involves creating clear criteria that a user story must meet before being accepted into an upcoming iteration. This is typically based on the INVEST matrix. E Epic An epic is a large user story. Estimation In software development, an \"estimate\" is the evaluation of the effort necessary to carry out a given development task; this is most often expressed in terms of duration. Exploratory Testing Exploratory testing is, more than strictly speaking a \"practice,\" a style or approach to testing software which is often contrasted to \"scripted testing.\" Extreme Programming Extreme Programming (XP) is an agile software development framework that aims to produce higher quality software, and higher quality of life for the development team. XP is the most specific of the agile frameworks regarding appropriate engineering practices for software development. F Facilitation A facilitator is a person who chooses or is given the explicit role of conducting a meeting. Frequent Releases An Agile team frequently releases its product into the hands of end users, listening to feedback, whether critical or appreciative. G Given When Then The Given-When-Then formula is a template intended to guide the writing of acceptance tests for a User Story: (Given) some context, (When) some action is carried out, (Then) a particular set of observable consequences should obtain. H Heartbeat Retrospective The team meets regularly to reflect on the most significant events that occurred since the previous such meeting, and identify opportunities for improvement. I Incremental Development In an Agile context, Incremental Development is when each successive version of a product is usable, and each builds upon the previous version by adding user-visible functionality. Information Radiators \"Information radiator\" is the term for any of a number of visual displays which a team places in a highly visible location, so that all team members can see the latest information at a glance. Integration \"Integration\" (or \"integrating\") refers to any efforts still required for a project team to deliver a product suitable for release as a functional whole. INVEST The acronym INVEST stands for a set of criteria used to assess the quality of a user story. If the story fails to meet one of these criteria, the team may want to reword it. Iteration An iteration is a timebox during which development takes place. The duration may vary from project to project and is usually fixed. Iterative Development Agile projects are iterative insofar as they intentionally allow for \"repeating\" software development activities, and for potentially \"revisiting\" the same work products (the phrase \"planned rework\" is sometimes used; refactoring is a good example). K Kanban The Kanban Method is a means to design, manage and improve flow for knowledge work and allows teams to start where they are to drive evolutionary change. Kanban Board A Kanban Board is a visual workflow tool consisting of multiple columns. Each column represents a different stage in the workflow process. L Lead Time Lead Time is the time between a customer order and delivery. In software development, it can also be the time between a requirement made and its fulfillment. M Milestone Retrospective A Milestone Retrospective is a team's detailed analysis of the project's significant events after a set period of time or at the project's end. Minimum Marketable Feature (MMF) A Minimum Marketable Feature is a small, self-contained feature that can be developed quickly and that delivers significant value to the user. Minimum Viable Product (MVP) A Minimum Viable Product is, as Eric Ries said, the \"version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort.\" Mob Programming Mob Programming is a software development approach where the whole team works on the same thing, at the same time, in the same space, and at the same computer. Mock Objects Mock Objects (commonly used in the context of crafting automated unit tests) consist of instantiating a test-specific version of a software component. N Niko-niko Calendar A Niko-niko Calendar is updated daily with each team member's mood for that day. Over time the calendar reveals patterns of change in the moods of the team, or of individual members. P Pair Programming Pair programming consists of two programmers sharing a single workstation (one screen, keyboard and mouse among the pair). Personas Personas are synthetic biographies of fictitious users of the future product. Planning Poker An approach to estimation used by Agile teams. Each team member \"plays\" a card bearing a numerical value corresponding to a point estimation for a user story. Points (estimates in) Agile teams generally prefer to express estimates in units other than the time-honored \"man-hours.\" Possibly the most widespread unit is \"story points.\" Product Owner (PO) The PO is a role created by the Scrum Framework responsible for making sure the team delivers the desired outcome. Project Chartering A high-level summary of the project's key success factors displayed on one wall of the team room as a flipchart-sized sheet of paper. Q Quick Design Session When \"simple design\" choices have far-reaching consequences, two or more developers meet for a quick design session at a whiteboard. R Refactoring Refactoring consists of improving the internal structure of an existing program's source code, while preserving its external behavior. Relative Estimation Relative estimation consists of estimating tasks or user stories by comparison or by grouping of items of equivalent difficulty. Role-feature-reason The \"role-feature-reason\" template is one of the most commonly recommended aids to write user stories: As a ... I want ... So that ... Rule of Simplicity Rules of Simplicity is a set of criteria, in priority order, proposed by Kent Beck to judge whether some source code is \"simple enough.\" S Scrum Scrum is a process framework used to manage product development and other knowledge work. Scrum Master The scrum master is responsible for ensuring the team lives agile values and principles and follows the practices that the team agreed they would use. Scrum of Scrums A technique to scale Scrum up to large groups (over a dozen people), consisting of dividing the groups into Agile teams of 5-10. Sign Up for Tasks Members of an Agile development team normally choose which tasks to work on, rather than being assigned work by a manager. Simple Design A team adopting the \"simple design\" practice bases its software design strategy on a set of \"simple design\" principles. Sprint Planning Sprint planning is an event that occurs at the beginning of a sprint where the team determines the product backlog items they will work on during that sprint. Story Mapping Story mapping consists of ordering user stories along two independent dimensions. Story Splitting Splitting consists of breaking up one user story into smaller ones, while preserving the property that each user story separately has measurable business value. Sustainable Pace The team aims for a work pace that they would be able to sustain indefinitely. T Task Board The most basic form of a task board is divided into three columns labeled \"To Do,\" \"In Progress,\" and \"Done.\" Cards are placed in the columns to reflect the current status of that task. Test Driven Development (TDD) \"Test-driven development\" is a style of programming in which three activities are tightly interwoven: coding, testing (in the form of writing unit tests) and design (in the form of refactoring). Team A \"team\" in the Agile sense is a small group of people, assigned to the same project or effort, nearly all of them on a full-time basis. Team Room The team (ideally the whole team, including the PO or domain expert) has the use of a dedicated space for the duration of the project, set apart from other groups' activities. Three C's \"Card, Conversation, Confirmation\" is a formula that captures the components of a User Story. Three Amigos Three amigos refers to the primary perspectives to examine an increment of work before, during, and after development. Those perspectives are Business, Development, and Testing. Three Questions The daily meeting is structured around some variant of the following three questions: What have you completed? What will you do next? What is getting in your way? Timebox A timebox is a previously agreed period of time during which a person or a team works steadily towards completion of some goal. U Ubiquitous Language Striving to use the vocabulary of a given business domain, not only in discussions about the requirements for a software product, but in discussions of design as well and all the way into \"the product's source code itself.\" Unit Testing A unit test is a short program fragment written and maintained by the developers on the product team, which exercises some narrow part of the product's source code and checks the results. Usability Testing Usability testing is an empirical, exploratory technique to answer questions such as \"how would an end user respond to our software under realistic conditions?\" User Stories In consultation with the customer or PO, the team divides up the work to be done into functional increments called \"user stories.\" V Velocity At the end of each iteration, the team adds up effort estimates associated with user stories that were completed during that iteration. This total is called velocity. Version Control Version control is not strictly an Agile \"practice\" insofar as it is now widespread in the industry as a whole. But it is mentioned here for several reasons.","title":"9 APPENDIX D - AGILE TERMINOLOGY"},{"location":"agileappend_e/","text":"10 APPENDIX E - GLOSSARY","title":"-10 Appendix E - Glossary"},{"location":"agileappend_e/#10-appendix-e-glossary","text":"","title":"10 APPENDIX E - GLOSSARY"},{"location":"agilearch/","text":"4 Agile Architecture 4.1 Agile Management Tools (AMT) 4.1.1 Purpose: While the environment is established by stakeholder management and contractual obligations, it is also necessary to establish the physical infrastructure (toolset) necessary to enable agile. With that in mind, this section will focus on the provisioning of an infrastructure that supplies 4 general functions: a) Backlog Management - the tools necessary to capture and refine requirements as well as allows the PO to prioritize the different work efforts to provide the most value. This is the tool that maintains a prioritized and organized listing of the work items which need to be done for the project. b) Work management - the tools necessary to execute the specific methodology which will be employed by the development team (Scrum, Kanban, XP, hybrid). This is the tool which provides the team the ability to collaborate on the development of their work items (Scrum Board, Kanban Board, etc). c) Communications Management - the tools necessary to communicate the status of the development effort to internal and external stakeholders. It includes the ability to create reports, provide metrics (does not define the metrics themselves), and implement collaborative dashboards with information which is relevant (i.e. work items complete, work items remaining, identified risks, identified issues, test status, etc). The actual capabilities of the tool which will be used will be based on the communications needs of the stakeholders involved (the evolution of a dashboard or report is often iterative in nature as communication needs are better refined). d) Continuous Integration - While not a necessity, a continuous integration tool makes agile run much more efficiently. The concept of iterative deliveries to the customer requires a mechanism which allows for continuous inputs by the developers to the code to provide smaller increments versus the big bang development approach of waiting till everything is done. With continuous integration, you get an iterative product of better quality based on the integrated automated testing functionality built into the tool (will cover automated testing methodologies and benefits in a companion playbook). Each section below will include the recommended capabilities required of the enabling tools (general in nature - not tied to specific methodology) One note - the infrastructure tools used by the HIA community are on the Atlassian set of agile products as well as Team Foundation Server (TFS) / Visual Studios Team Server (VSTS). The table below shows a quick overview of the differences and similarities between the two in reference to enabling an agile framework. Links describing how to enable the management systems below within the VSTS/TFS and the Atlassian Products are found in Appendix C - Key Links. 4.1.2 Backlog Management When considering which tool to use, the following functionalities should be provided: Capture and refine requirements into nested Product Backlog Items (PBIs) - Epics, Features, User Stories Organize and prioritize those requirements Provide a reporting capability to show necessary details to understand the status of those Product Backlog Items 4.1.3 Work Management (i.e. Release Roadmap/Scrum Boards/Kanban Boards) The following functionalities should be provided: Capability to establish an agile execution board: Scrum - Sprint Board Kanban - Kanban Board XP / Hybrid - Agile Board Ability to refine the work in progress (add and update the status details of the different work items - including tasks, user stories, etc.) Ability to assign responsible parties to the different work items (while agile emphasizes the establishment of responsibility when capacity is available, there needs to be a method to monitor which work items have a responsible resource and which ones are still available to be worked on) Capability to report necessary details to understand the status of work in progress (metrics analysis will be provided later in the discussion of communication tools - here we are looking at status of individual versus aggregate work items). 4.1.4 Communication Management Recommend functions include the ability to produce the following: - Dashboards - Metric Analysis Reports: - Release Roadmaps - These will be discussed later in the methodology, but the release roadmap provides an overview of when features are expected to be complete. In terms of project management, the release roadmap can be used to establish the schedule for the project. 4.1.5 Continuous Integration Architecture and Management Recommended functionality for enabling continuous integration in any toolset: - Version control tool (code repository management) - Instrumented or scripted build process - Trigger capability for implementing a build and test cycle based on code check-in - Automated testing implementation capability - Automated alerts, in case of a failed test, back to the developers so that they can resolve issue immediately (provide feedback on issues)","title":"-4 Agile Architecture"},{"location":"agilearch/#4-agile-architecture","text":"","title":"4 Agile Architecture"},{"location":"agilearch/#41-agile-management-tools-amt","text":"","title":"4.1 Agile Management Tools (AMT)"},{"location":"agilearch/#411-purpose","text":"While the environment is established by stakeholder management and contractual obligations, it is also necessary to establish the physical infrastructure (toolset) necessary to enable agile. With that in mind, this section will focus on the provisioning of an infrastructure that supplies 4 general functions: a) Backlog Management - the tools necessary to capture and refine requirements as well as allows the PO to prioritize the different work efforts to provide the most value. This is the tool that maintains a prioritized and organized listing of the work items which need to be done for the project. b) Work management - the tools necessary to execute the specific methodology which will be employed by the development team (Scrum, Kanban, XP, hybrid). This is the tool which provides the team the ability to collaborate on the development of their work items (Scrum Board, Kanban Board, etc). c) Communications Management - the tools necessary to communicate the status of the development effort to internal and external stakeholders. It includes the ability to create reports, provide metrics (does not define the metrics themselves), and implement collaborative dashboards with information which is relevant (i.e. work items complete, work items remaining, identified risks, identified issues, test status, etc). The actual capabilities of the tool which will be used will be based on the communications needs of the stakeholders involved (the evolution of a dashboard or report is often iterative in nature as communication needs are better refined). d) Continuous Integration - While not a necessity, a continuous integration tool makes agile run much more efficiently. The concept of iterative deliveries to the customer requires a mechanism which allows for continuous inputs by the developers to the code to provide smaller increments versus the big bang development approach of waiting till everything is done. With continuous integration, you get an iterative product of better quality based on the integrated automated testing functionality built into the tool (will cover automated testing methodologies and benefits in a companion playbook). Each section below will include the recommended capabilities required of the enabling tools (general in nature - not tied to specific methodology) One note - the infrastructure tools used by the HIA community are on the Atlassian set of agile products as well as Team Foundation Server (TFS) / Visual Studios Team Server (VSTS). The table below shows a quick overview of the differences and similarities between the two in reference to enabling an agile framework. Links describing how to enable the management systems below within the VSTS/TFS and the Atlassian Products are found in Appendix C - Key Links.","title":"4.1.1 Purpose:"},{"location":"agilearch/#412-backlog-management","text":"When considering which tool to use, the following functionalities should be provided: Capture and refine requirements into nested Product Backlog Items (PBIs) - Epics, Features, User Stories Organize and prioritize those requirements Provide a reporting capability to show necessary details to understand the status of those Product Backlog Items","title":"4.1.2 Backlog Management"},{"location":"agilearch/#413-work-management-ie-release-roadmapscrum-boardskanban-boards","text":"The following functionalities should be provided: Capability to establish an agile execution board: Scrum - Sprint Board Kanban - Kanban Board XP / Hybrid - Agile Board Ability to refine the work in progress (add and update the status details of the different work items - including tasks, user stories, etc.) Ability to assign responsible parties to the different work items (while agile emphasizes the establishment of responsibility when capacity is available, there needs to be a method to monitor which work items have a responsible resource and which ones are still available to be worked on) Capability to report necessary details to understand the status of work in progress (metrics analysis will be provided later in the discussion of communication tools - here we are looking at status of individual versus aggregate work items).","title":"4.1.3 Work Management (i.e. Release Roadmap/Scrum Boards/Kanban Boards)"},{"location":"agilearch/#414-communication-management","text":"Recommend functions include the ability to produce the following: - Dashboards - Metric Analysis Reports: - Release Roadmaps - These will be discussed later in the methodology, but the release roadmap provides an overview of when features are expected to be complete. In terms of project management, the release roadmap can be used to establish the schedule for the project.","title":"4.1.4 Communication Management"},{"location":"agilearch/#415-continuous-integration-architecture-and-management","text":"Recommended functionality for enabling continuous integration in any toolset: - Version control tool (code repository management) - Instrumented or scripted build process - Trigger capability for implementing a build and test cycle based on code check-in - Automated testing implementation capability - Automated alerts, in case of a failed test, back to the developers so that they can resolve issue immediately (provide feedback on issues)","title":"4.1.5 Continuous Integration Architecture and Management"},{"location":"agileframe/","text":"5 Agile Framework 5.1 Part I - Establish the Product Backlog (PB) and Constraints: 5.1.1 Establish the Product Backlog For a government project the initial list of requirements is established within the contract and will be used as the foundation for the initial PB development (and captured within the appropriate Backlog Management tool). The establishment of the PB is comparable to the establishment of a language - in this case it is the language of \"value\" that provides for understanding between the government and the development team. For this \"value\" language - the individual PB components provide the words and grammar (think of Epics as Paragraphs, Features as sentences and User Stories/Cards as words), while the order of the backlog based on prioritization of \"value\" provides the context of the product story. This initial section focuses on the creation of the Product Backlog. Later sections will discuss how the PB is prioritized to provide the most value up front as well as how it evolves based on a changing environment to continue to remain value relevant. As the PB will generally follow a rolling-wave planning process with more detail provided as it becomes available or necessary, it can initially be setup with larger organizational elements (epics and features) which will be iteratively refined later. A recommended nested organization for these components include: Epics - These include functionality which can take one or more releases to complete. For government contracts, these can be linked to a Contract Line Item Number for easier reference to cost tracking and earned value metrics. Features - These include functionality components that will generally take more than one iteration to complete and provide the functionality to the users through releases. Product Backlog Items (PBI) - These include the product increments which consolidate to provide the functionality of the feature and generally can be accomplished within a single iteration. PBIs can be decomposed into specific tasks with hours estimated for completion. The main component of the PB is the PBI that may be named something different depending on the framework (for Scrum and XP can be a user story, for Kanban can be a simple work item or card). In translating a requirement to a PBI, it is necessary to understand that the PBI can contain 1 or more requirements as it is based on the customer functional need (can be captured in government use case documents). However, while functional requirements will most likely include BES Process Directory requirements terms including \"shall\" which dictates the provision of a functional capability, they may also include conditional requirements indicated by the \"must\", \"must not\" and \"required\" requirements descriptions indicating additional performance requirements or constraints). Additional non-functional requirements to be considered include: Preparation and conduct of design reviews Hardware setup (Different environments) Tool setup (Software tools) Network Setup (establishing connectivity - access requirements, ports and protocols) UAT, Pre-Prod, Prod transitions (can be facilitated by continuous integration and Devops) Documentation (User Guides / CDRL's / Training Materials Regression testing (can be facilitated by automated testing) Cybersecurity testing The typical PBI contains a minimum of 3 elements - Title: Ensure this ties to the logic of the work (many times the titles of a PBI can cause confusion between the PO and development team if improperly stated). Description: Explains the customers need and the functionality required. Acceptance Criteria: Explains the conditions under which the PO will accept the work item as complete (note that there are additional criteria which will be discussed in presenting a \"Definition of Done\"). These criteria should be testable (I.e. should be specific enough to be able to qualify as complete or not complete). A typical PBI descriptions contains the following elements: \"As a...\" - Defines the perspective of the user who needs the functionality. \"I need...\" - Defines the functionality needed. \"So that...\" - Defines the why of the functionality (enables the developers' ingenuity as they may have a better solution which still meets the \"why\" of the user's request though the how may be something different). Example: As a maintenance scheduler I need to be able to review the Daily Schedule So that I can review the events scheduled for the day and the status of their completion Additional information can be included within the description. One recommendation is to include the requirement number(s) from the original contract in this section to facilitate the creation of the Requirement's Traceability Matrix necessary for design reviews and deliveries. While the PO is the primary owner of the backlog, the initial creation of the backlog is most effective when completed in a collaborative manner. While the term Sprint 0 is not an official Scrum term, it does provide a context for doing the preparation work necessary to initiate an agile methodology (without the PBI's it is not possible to do the work). Whatever it is called, there is a preparation session which is necessary to prime the pump of the agile framework by building the initial PB. Here is a recommended approach to doing this which ends with the government System Functional Review (SFR) which established the PB as the Functional Baseline: Preparation Phases (Phases are provided here as timeline will have to adjust based on size and complexity of requirements as well as whether preliminary planning has been done...i.e. feature driven planning): Phase 1: (Government and Development Team collaboration preparation) Government provides initial Requirements Traceability Matrix (RTM). Government provides initial feature backlog linked to RTM to development team (with minimum of description filled out and rough draft of acceptance criteria). Development team reviews feature descriptions and provides feature backlog feedback to government at end of the phase with recommended revised acceptance criteria and questions of clarification for review prior to physical collaboration. Phase 2 (Physical Collaboration - government and development team) Collaboratively refine features with end-state of initial Title, Description, and Acceptance Criteria agreed on by government and development team Conduct breakout sessions as necessary to provide additional details to development team necessary to make initial estimates of feature sizes Development team makes initial estimates of feature sizes (T-Shirt sizing). Development team prepares for SFR. 5.1.2 System Functional Review (SFR) The SFR is a multi-disciplined system-level technical review ensures that the functional baseline is established and has a reasonable expectation of satisfying the requirements of draft capability development document (CDD) within the current budget and schedule. It completes the process of defining the system-level technical requirements that are documented in the system performance specification. According to the BPD, a successful completion of SFR provides a sound technical basis for proceeding into preliminary design. If the above collaboration is done correctly, this review becomes a \"value language\" confirmation brief ensuring that the PB correctly reflects the requirements of the contract and are consistent with cost (program budget), risk and other system constraints. Audience: Functional Review Board (FRB) members Objectives: Reconfirm that Capability Package Requirements are linked to PB Features derived during the preparation event above (Sprint 0) Initial identification of system-level document changes which will need to be addressed during design reviews (see CDRL section (link within main document)) Establish Functional Baseline. Add Feature list to the Program Product Backlog to be considered in Release Planning for development based on PMO priorities and value determination. Determine based on Release Planning when each Feature should be considered within either the Preliminary Design Review or the Critical Design Review. Outputs: Required inputs for a Configuration Control Board (CCB). Approved PB and Functional Baseline based on Configuration Control Directive (CCD) from CCB. Updated system release roadmap with new features. Updated Requirement Traceability Matrix with links from requirements to PB. Identified system-level CDRLs to update for design reviews based on PB contents. Identified features for design reviews (Preliminary Design Review [PDR] and Critical Design Review [CDR]) 5.1.3 Release Management Release management is about determining what is expected to be in each release and what comes next for the development team as well as identifying PBI dependencies early enough so that they can be rectified prior to becoming impediments during development. The PB includes those features approved in the functional baseline by the PMO during the System Functional Release (SFR) conducted at the end of Sprint 0. The features are prioritized within the PB by the PMO and considered by a Release Planning committee for inclusion in upcoming releases. Those features which are selected for inclusion in upcoming releases are updated on the Release Roadmap based on their tentative estimate of completion. The Release Roadmap is a high-level view of the PB features, with a loose time frame for when the team will refine and develop those features. Proposed agenda for Release Planning meeting: Objective Overview Review Current Release Roadmap (Starting point) Review Current Status of Development Streams (Establish current status which may have changed since last release planning session) Determine dates for: Next Release (Feature focus for the CDR) Next Release (+1) (Feature focus for the PDR) Review Product Backlog Feature Priority Lists (Confirm priorities) Confirm Features for Next Release Identify Features for Next Release (+1) Update Release Roadmap (Use information from session to update the release roadmap) Discuss any release process improvement steps (Continuous Improvement) 5.1.4 System Engineering Design Reviews Link to DAU System Engineering Overview (Add to links appendix - will not be contained in base document): https://www.dau.mil/guidebooks/Shared%20Documents%20HTML/Chapter%203%20Systems%20Engineering.aspx#toc83 A secondary advantage of implementing a pro-active release management system is the ability to establish what features will be reviewed at what level for Agile-based design review gates (here we are talking about Preliminary and Critical Design Reviews but not in the level of detail that was expected within traditional development engineering management plans as Agile is about doing architectural design \"just in time\" and iteratively \"just enough\"). The Design Reviews can be synchronized with the above release management process to fulfill their primary objectives based on two different perspectives: Preliminary Design Review (PDR): Focus : The features included in the release after the upcoming release (long-term perspective). Objectives : Provide sufficient confidence in the preliminary design's integration with necessary system components to serve as the starting point for Agile incremental design during development. Provide technical confidence that the capability need can be satisfied within cost goals based on the schedule included in the release roadmap. Confirms that high-level design decisions are consistent with the user's performance, schedule needs, and the validated Capability Development Document (CDD). Establishes the allocated baseline (based on features), which is placed under formal configuration control. Agile Revisions : Identify dependencies on external requirements or other features to ensure proper sequencing and identify issues for resolution prior to them becoming impediments (identifies feature risks early). Determine required infrastructure requirements so they can be acquired prior to development. Agenda Template : Review Release Roadmap Review Features for the release after the immediate upcoming release. Review Feature description Review Feature acceptance criteria Review Features system-level design integration (focused on necessary design integration of feature capability) Review Feature dependencies (I.e. external interface requirements, additional software requirements) - these will become an initial set of assumptions for risk management (the goal is to mitigate the risks so that the assumption will not impede our development efforts later) Review Feature CDRL requirements (what CDRLs will need to be updated based on the completion of the feature development) Review Feature size estimates (establishes metric for estimating duration) Revise Release Roadmap as necessary based on updated information Critical Design Review (CDR): Focus : The features included in the next (upcoming) release (short-term perspective). Objectives : Establishes the initial product baseline (based on features), which is placed under formal configuration control. Confirms the high-level system design is expected to meet system performance requirements Confirms the system is on track to achieve affordability and should-cost goals as evidenced by the design documentation Establishes requirements and system interfaces for enabling system elements such as support equipment, training system, maintenance and data systems. Agile Revisions : Confirms that dependencies identified in the PDR have a resolution strategy that will be completed in time to enable development of the dependent features (will not become impediments during development). Confirms that features are ready for further refinement into work items for the development team. Agenda Template : Review Release Roadmap Review Features for the upcoming release. Review Feature description Review Feature acceptance criteria Review Features system-level design integration and system requirements: UI change requirements (mock-ups if available) Middle tier requirements (services) Data structure requirements Review Feature dependencies to ensure they have been resolved or a feasible resolution strategy is in place prior to beginning work on the feature (these address the initial assumptions determined during the PDR) Review Feature CDRL requirements - update as required Review Feature size estimates (establishes metric for estimating duration) - update based on additional information Revise Release Roadmap as necessary based on updated information 5.1.5 Refine the Product Backlog (Features -> Initial PBIs) In general, the refinement process establishes the \"development language\" between the Development Team and the Government through their designated PO. Feature refinement should be conducted about 2 months prior to working on the first PBI of the feature. The objectives of feature refinement are to: Break down feature to individual PBIs (user stories, cards, work items, etc) with, at a minimum, a description and acceptance criteria; Provide an initial estimate of the complexity of the work item (sizing estimates can be associated with established costs and used for EV metrics); Gain the formal approval of the PO to include the refined work items into the Product Backlog. Feature refinement in preparation for development work conducted between the government PO and Development Team: Normally takes 1-2 hours per feature if done correctly (the more discussion, the better clarity of understanding between the government and the development team increasing the likelihood that the development product will meet government expectations). Refine initial title, description, acceptance criteria at the feature - level based on knowledge acquired since design review - this is the focus of the PMO input - provides the WHAT of the feature. Identify the logic of the work. Identify the initial user stories. Ensure that the stories have a description and acceptance criteria which provide a common reference for the team to produce an initial sizing estimate (provides metric for EV - can be updated later). Identify infrastructure requirements (i.e. suites) necessary for development. Three questions to guide Feature Refinement Discussion and what they are attempting to elicit: What do you want to see at the end? (Acceptance Criteria) What are the high-level steps to get there? (Logic) What are the sub-steps for each of the high-level steps? (PBIs) 5.1.6 Prioritize the Product Backlog Agile is about providing value to the government customer as quickly as possible. Less valuable work is ranked lower in the priority scale. Thus, after refinement of the features into PBIs within the PB, it is necessary to rank them based on the value they provide. There are multiple considerations in evaluating the value of each individual PBI including: What provides most value functionally to the government user? What impact does the PBI have on other items within the backlog (i.e. are their dependencies between work items)? Cost / impact assessment of each PBI. Learning - Are their efficiencies to be gained for the remaining work items by completing a specific story earlier (i.e. taking a smaller item which is easier to do to establish the templates and processes for doing more complex work items of a similar nature later but at a more rapid pace)? Are their negative impacts to other PBIs by not completing one earlier? Regulatory deadlines (not considered agile, but is a consideration for prioritization) It is the responsibility of the PO, as the representative of the government to continuously reprioritize the PB to provide the most value to the government based on the current environment (especially as new PBIs are added or removed from the PB). Prioritization considerations to avoid (as they are normally short-term perspectives which may reduce long-term value): - Highest Ranking Person in the Room - Flavor of the Day 5.1.7 Constraints Analysis The above sections focus on refining requirements into work items, but there are several; other areas of regulatory constraints within the governmental regulatory environment that should be addressed prior to determining as well as implementing an Agile development methodology. The following is a non-exhaustive list of items that may impact the execution of development based on scope (these provide a basis for discussion between the Government and Development Team to determine which will be enforced and to what level): External testing requirements such as DT&E and QT&E (capacity may be required to prepare and support testing - need to delineate what falls to the development team and what will be done by the government) Additional BPD Design, Test, Functional Readiness Reviews (all come with a preparation and documentation overhead - will need to discuss expectations and agile modifications based on iterative versus up front design) Development environment limitations (discussion around whether the contractor will control the environment or whether it is an externally administered environment - I.e. the Capabilities Integration Environment (CIE)) Access requirements (security requirements may dictate which personnel resources can be allocated to a team) Re-usability and compatibility requirements (establishes the boundaries of initiative for the developers - \"can be innovative within these limits\") Cybersecurity requirements (the pre-requisite cybersecurity requirements may limit development options) 5.1.8 Change Management While there is an inherent flexibility within Agile methodologies to accept change specifically through backlog grooming and PBI refinement, there is still the challenge of differentiating between contractual scope additions versus simple requirement refinement. The differentiation of what will be left within the authority of the PO to approve through backlog refinement and what requirement changes need formal approval in the form of a Configuration Change Directive (CCD) should be established prior to the beginning of development to ensure proper steps are taken within the established constraints to adapt to change. When a formal configuration change is necessary, the government Configuration Control Board (CCB) with input from the Functional Review Board (FRB) should: Review the list of any new user requests, deficiency reports and change requests received in the project space; Assess the impact of a change; Assign a priority to the change; Assign a business value to the change; and Implement a CCD to insert an approved work item for the change into the Product Backlog (upon completion of any contractual modification requirements to accept in the new work) 5.2 Part II - Establish the Methodology: 5.2.1 Definition of \"Ready\" and \"done\" Prior to implementing a specific development methodology, the definitions for the development entry and exit criteria need to be defined. Thus, prior to beginning development through any of the methodologies, two definitions should be agreed upon between the PO and development team in reference to individual PBIs: Deifinition of Ready Purpose Describes materials and topics that must be included and addressed in well written backlog items (I.e. Scrum / XP User Stories, Kanban cards) Used to evaluate whether or not a PBI has been appropriately elaborated and is ready for development Helps ensure that backlog items are complete and understandable before being scheduled for an iteration. EXAMPLE DEFINITION OF READY CHECKLIST Description clearly states the who (\"As a\"), what (\"I want to\"), and why (\"so that\") are for the backlog item PO has approved the backlog item Acceptance criteria is clearly defined Scenarios and expected outcomes are clearly defined Artifacts defining user interface requirements are included (if applicable) Business rules are referenced or included (if applicable) Dependencies are identified Development Team has reviewed and confirmed they understand the backlog item Backlog item is appropriately estimated (I.e. sized for scrum) \"Definition of Done\" for the PBI is understood Definition of Done - Purpose - The \"Definition of Done\" describes criterion that must be met by each committed User Story or Defect to development before the PO can validate that the PBI is accepted. - Definition is used to help ensure that the developed product is consistent with the associated backlog item, is high quality, and is ready for production testing. - \"Done\" establishes quality norms and assures the PO that major defects are not likely to be identified during production testing EXAMPLE DEFINITION OF DONE CHECKLIST Unit testing and module testing are complete All acceptance criteria defined within the PBI are met Test results have been reviewed with the PO The PO concurs with the adequacy of the testing The backlog item has passed acceptance testing by the PO The development product has been demonstrated and accepted by the PO 5.2.2 Scrum This section provides a template for the scrum process. As it is a template, government and development teams are encouraged to adapt the process based on team structure, culture and regulatory constraints to make it more efficient. To that effect, this section is prescriptive versus directive in nature. Also note that we have transitioned from the general agile term of PBI to user stories as they relate to Scrum. Scrum Methodology: Scrum Events The five key Scrum events are: Product Backlog Refinement - Based on the project release roadmap and PO priorities, there are two components to refinement: Feature Refinement - Breaking down features and setting priorities (this process was explained in Section 5.1.5 above under feature refinement) User Story Refinement - Grooming user stories so that they are ready to be accepted into a sprint Sprint Planning: Determining the goals and related stories for a sprint Sprint Execution - Developing and testing stories so they are done Sprint Review - Providing feedback to improve the product Sprint Retrospective - Determining how to improve the process Product Backlog Refinement Feature Refinement Session (2 Sprints early) : Coordinated by the Scrum Master with the Solution Architect and Customer System Engineer (and POs if available) to: Ensure that the feature acceptance criteria are still correct for upcoming features originally determined during Sprint 0 activities. Ensure that all dependencies (possible future impediments) have a resolution strategy prior to development. Ensure that design tenets are understood within the context of the feature work. User Story Refinement Collaboration (Between Feature and User Story Refinement Sessions) : Team designated Business Analyst works collaboratively with the PO to refine story priority and the acceptance criteria necessary to accomplish the associated feature functionality. Team testers can also add necessary positive test cases based on draft acceptance criteria to the story (facilitates test driven development). Refinement Session - User Stories (1 Sprint early) : Business Analyst discusses each story with the development team to ensure understanding of delivery requirements. The PO is included in this refinement session, but it is the development team that is being introduced to the stories. The PO is there to confirm through listening to the team's conversations with the Business Analyst that his/her intent is understood within the context of the story. This refinement session is done to ensure that each story meets the established Definition of Ready prior to being accepted into the sprint for development. Meeting the Definition of Ready : The acronym INVEST provides a high-level cross-check of whether a story is Ready for development. It is normally part of the last check done on a story during either Refinement Day activities or as part of Sprint Planning for priority stories that have been updated since Refinement Day. While it is presented here under the Scrum section, it can be used with modification (I.e. Kanban work flow estimates versus sprint estimates) within the other agile frameworks as a final check for each PBI). The acronym represents the following criteria: (I) Independent - Story contains no dependencies on unavailable external resources or preparatory user story completion (N) Negotiable - Three perspectives must be represented in refinement of the user story to meet the PO's intent - Business Analyst, Developer, Tester - if they have not been involved in refinement, a key perspective on the complexity of the story is missing from the preparatory analysis. (V) Valuable - Established by the prioritization given by the POs. (E) Estimable - Estimates provide a gage for the acceptance of future work levels into a sprint as well as for forecasting future work timelines - they gain value over time as the team evolves. (S) Small - User stories should be small enough to complete within a sprint while large enough to provide observable value to the PO. (T) Testable - Positive manual test cases will be added to appropriate user stories prior to being taken into the sprint to account for successful achievement of acceptance criteria. As necessary, these tests will be augmented during the sprint with additional negative manual and automated tests. Based on the high-level of interaction between the team and the PO in this option, each story should have as a minimum 3 touch points with the PO to confirm that PO's intent is maintained throughout refinement. At a minimum, these touch points are the verification of the acceptance criteria in the documentation, the answering of team questions during team refinement of stories, and the final acceptance of the story as ready for development At this point the story is considered \"ready\" for acceptance into a sprint during sprint planning. Sprint Planning Sprint Planning (Appendix A) Planning centers around the presence of the PO within the Planning meeting. During this meeting, the PO will review the current list of Ready user stories to ensure that they are still valuable (relevant), as well as reprioritize the Backlog based on existing value as necessary (this facilitates the team's acceptance of work into the sprint as they simply take user stories from the top of the list within their established sprint capacity. Sprint Execution Sprint Execution (Appendix A) While developers and testers work to complete and validate the work required for each user story, Business Analysts and, if necessary, POs are available for clarification of requirements to ensure that the work produced meets the intent of the customer. If refinement has been done correctly, sprint execution consists of developers completing their work, testers executing the tests developed prior to the sprint (with the addition of any necessary negative test cases), and BAs and testers reviewing the completed work with the PO in order to receive early feedback or formal closure of the story as meeting the teams \"Definition of Done\". The user story acceptance criteria is the foundation for this definition. The story points assigned to sprint user stories not designated as \"Done\" are not counted when measuring sprint completion velocity. Sprint Review Sprint Review (Appendix A) Story acceptance as Done is completed by the PO only (the same person who approves the acceptance criteria must be the person who approves work against the approved acceptance criteria). If the PO believes that the work does not satisfy the customer requirements, they have to make a decision: if the work does not meet the acceptance criteria, they can reject the work as not done and provide additional clarification; if the work meets the acceptance criteria, but the PO wants additional revisions, additional user stories are created and prioritized within the existing backlog. The key to the review is that it is interactive between the PO and Development Team - the focus being to improve the product. Sprint Retrospective Sprint Retrospective (Appenidx A) This is where the POs and the Development Team focus on improving the Agile process. Critical to understanding the concept of the retrospective is that learning always occurs during these events (Successful sprints can indicate items to sustain; failed sprints provide opportunities to learn where the team can improve). One note to remember in the retrospective is that the process should facilitate the development efforts - not impede them (hence why this document is presented as a template - it needs to be adjusted to fit the needs of the agile environment in which it exists which is a combination of the people, processes, tools, relationships and levels of trust involved in the project). 5.3 Kanban Considerations for establishing a Kanban process: Establish the Kanban Board - Visualize the workflow to organize, optimize, and track the work item development progress (allows for transparency of status) Limit Work in Progress (WIP) - Kanban is about flow. The more items in progress, the more risk of nothing getting done. It is better to have a limited WIP that emphasizes the Lean pull system. That is, when a resource has completed work on a work item and moved it to the next column, s/he pulls the next work item from the column before it. A starting rule for each policy column is limit WIP to amount of column resources + 1 (i.e. for a development column - the WIP would be the number of developers on the team plus one - that allows for a developer who hits an impediment to stop work on that work item and pull another one from those completed in the previous column while the Agile Lead works to remove the impediment. This allows for the flow to continue). Key points in Kanban: Minimize Lead Time (the time a work item is in any specific stage / column) Minimize Cycle Time (the time for the entire work flow cycle to complete for a single work item) - since establishing a cycle time allows for estimation of work items, but not all work items are the same complexity - one methodology for differentiating the work item complexity is to provide multiple swim lanes for tracking work progress - One for work items which have an expected cycle time of 1 week One for 2-week items One for 1-month items Optional - an expedited swim lane (similar to the express lane at a supermarket) for high-priority work items that should take precedence over all others (this will be mentioned again as a strategy for using Kanban to manage Operations and Maintenance work items) Manage flow - The Agile Lead works with the development team to ensure that workflow issues are identified and resolved (i.e. if there is a backup in the workflow, an additional resource may be needed to relieve pressure on the bottleneck column) Make process policies explicit - Each column on the Kanban Board should have an explicit policy which indicates when the work in that column is complete (policy = that column's \"Definition of Done\") Improve the process - In collaboration between the development team and PMO - there should be a focus on continuous improvement of the process (re-define the policies, determine whether columns should be added, determine whether columns should be deleted) While Scrum focuses on a cross-functional team, Kanban focuses more on aligning the resources with the policies. A simple representation of a Kanban Board (this also provides a quick start-up template for HIA products) is: Templated steps / policies for an initial Development Team Kanban board: Backlog : PMO PO prioritizes the Work Items in the Backlog Identify : Development Team Business Analyst in coordination with the PMO PO will identify / refine the description and acceptance criteria for the work item PO Approved : PMO PO will approve the description and acceptance criteria (Confirms that the work meets functional requirements - think mini-PDR) Design : Development Team's Senior Developer will complete draft solution design Cyber Assessment : Developer's cybersecurity representative (ICW PMO Cybersecurity) will complete security assessment of solution PMO Engineer Approved : PMO Engineer will approve design of solution (Confirm that solution meets technical requirements - think mini-CDR) - work item now \"ready\" for development. Develop / Implement : Developers implement the solution Test : Development team testers and cybersecurity will conduct testing and update documentation as needed Pending Acceptance : Development team will demonstrate completed work item functionality to PMO Done : PMO will accept work item as complete (meets the Definition of Done) An Operations and Maintenance team provides an example of a project built for Kanban. The backlog would contain the ticket items requiring resolution with the priority of work established by a government Product Owner. This approach allows refinement of last minute prioritized work items identified by system users through the workflow's column policies identified above (the process of making the work item \"ready\" is captured within the workflow). Also, if the Kanban board swim lanes are setup as described above, the board will contain an expedite lane for flowing through critical fixes requiring short duration resolution. (Note: Another example would be the cybersecurity team handling the implementation of required TCNO/IAVAs.) 5.3.1 Kanban Cadences While Scrum Events focus on execution of the time-boxed process, the key Kanban events (cadences - named for their formal rhythm) focus on organizing, optimizing, and tracking the development process. The cadences indicated below are interdependent as represented in the figure above - some meetings inform others, while feedback from other meetings drive changes to the process. Strategy Review (Release Management) - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview: A Strategy Review session is held to periodically review the current status of the development effort as well as revise the Release Roadmap to ensure that it correctly reflects current value and priorities. The question answered during this meeting is: Are we doing the right thing? The Release Management process described before represents an example of the events and timelines reflected within this cadence. Example cadence : Quarterly or based on release period - should look at current and next release and how the PB reflects that plan / strategy. Stand-ups - Focused on the Kanban board and managing workflow Play: Link to Play below in the Appendix Overview: While the focus of the Scrum stand-up meeting is on the individual work and how it relates to the team's ability to meet sprint (time-boxed) goals (What did I do yesterday? What will I do today? What is impeding my work?), the focus of the stand-up for Kanban is managing the flow of work items among the Development Team resources to make it as efficient as possible (managing workflow to minimize the time any piece of work is in any stage of development). While the stand-up can be done daily, it is not a requirement of Kanban as the trigger for work to be pulled into the next column is automatic (completion of the column policy for that work item and next column resource availability). The cadence recommendation of a week is a goal - the team will most likely start out in a daily cadence until the workflow is better understood. Example cadence : Initially daily. Can evolve to once a week to coordinate the efforts of the team and resolve work-in-progress bottlenecks. Replenishment Meeting - Similar to the Sprint planning sessions - these meetings ensure that the work items which will be brought into the development stream are prioritized and \"ready\" to be worked on. Play: Link to Play below in the Appendix Overview: The replenishment meeting ensures that there is \"ready\" work prioritized in the backlog to be brought into the workflow. The battle rhythm for when these meetings are necessary are dependent on the workflow (the recommendation is to start by having this monthly and then determine whether it should happen more or less often). It should take the work that the government has identified (refined down to the work item level) and ensure that the work is \"ready\" to enter into the workflow. It is the last step in determining which prioritized work is up next and ensuring that the work still adds value to the government. Example cadence : Once every month (or as necessary to ensure that there is a \"ready\" backlog of work to feed the development stream). Delivery Planning Meeting - This meeting coordinates product hand-off based on current work delivered between multiple entities: Developers, Product Owners, Testers, Trainers, Users (can be facilitated with establishment of Continuous Delivery process and structure). Play: Link to Play below in the Appendix Overview: The Delivery Planning meeting is a discussion between multiple entities of the results of the actual workflow (while the Strategy Meeting establishes a plan, the Delivery Planning Meeting looks at the reality). An assessment is made of the Kanban board to determine what work in progress will be completed by a specific time to pass it on to the next entity in the chain. It is about establishing expectation management so that external agencies can plan for events which are dependent on the contents of the delivery. These include: PMO ensuring that reviewers are ready to conduct the Physical Configuration Audits and appropriate document reviews, The LDTO agency coordinating testing events based on specific RALOT. Users providing timely feedback; Functionals coordinating training events so that released functionality into production can be used. Example cadence : Once every month to update downfield receivers of the product (highly dependent on the release timeline - i.e. a release once a quarter might only require this meeting once a quarter). Operations Review - How do we improve the process from the organization's point of view Play: Link to Play below in the Appendix Overview: This review is similar to a Scrum retrospective (how to improve the process) but looks at the release management system from an organizational point of view. It looks at improving the entire system as it relates to incorporating the Kanban team workflow methodology into the government review and release system. While the Delivery Planning Meeting focuses on coordinating the efficient release of the product, this meeting is about improving the release process based on lessons learned. It looks back on each release from the perspective of all stakeholders to continuously improve the system for the next release. Example cadence : Once every 3 months (or by exception) following the Release Delivery Service Delivery Review - Review of the release product from the user's point of view. Play: Link to Play below in the Appendix Overview: This review is conducted to determine if the items being produced are actually meeting the user's needs. The Service Delivery Review focuses on what was produced from the end-user's point of view and whether it fulfills their functional needs as well as quality standards. It provides a direct feedback chain from the end user to allow the PMO PO insight into requirement and prioritization changes, as well as the development team to understand better how their product is being viewed by the users of the system (including their satisfaction and concerns). Example cadence: Once every 3 months (or by exception) following the Release Delivery. Frequency will vary based on user first look events or actual releases to the field - need to provide end-user enough time to review delivered product. Risk Review - Done as necessary to review all identified risks to determine the status of risk mitigation measures and issue / impediment resolution. Play: Link to Play below in the Appendix Overview: The Risk Review discusses the probability and impact of planning assumptions being wrong and what steps the PMO and development team are taking to minimize those risks. It focuses on what steps are being taken to minimize the probability of false assumptions impacting delivery cost or schedule. This review focuses on reduces uncertainties in the system in order to establish predictability and reliability in the system which enables trust to form within the different organizational entities including the PMO, development team, external testers, users, etc. Example cadence : Once every 2 weeks (or combined with PMO Risk Management Meeting) 5.4 Extreme Programming (XP) XP's primary contribution to the software development world is an interdependent collection of engineering practices that teams can use to be more effective and produce higher quality code. Many teams adopting agile start by using a different framework and when they identify the need for more?disciplined engineering practices they adopt several if not all of the engineering practices espoused by XP. The recommended methodology here is to begin with the Scrum Framework Events listed before and then adopt as necessary the XP engineering practices which enhance the process. General differences between Scrum and XP: 1) Scrum uses sprints which are normally 2-4 weeks long. XP uses iterations which are normally 1-2 weeks long. 2) Sprint backlogs are normally sacred while XP embraces more flexibility if a higher priority work item comes in (understanding that it will replace an equivalent work item already accepted into the iteration). 3) If the team identifies some stories that they are unable to estimate because they don't understand all of the technical considerations involved, they can introduce a spike to do some focused research on that particular story or a common aspect of multiple stories. Spikes are short, time-boxed time frames set aside for the purposes of doing research on a particular aspect of the project. Spikes can occur before regular iterations start or alongside ongoing iterations. 4) XP focuses on practice excellence. The method prescribes a small number of absolutely essential practices and encourages teams to perform those practices as good as they possibly can, almost to the extreme. This is where the name comes from. Not because the practices themselves are necessarily radical rather that teams continuously focus so intently on continuously improving their ability to perform those few practices. With that in mind, the initial recommended XP methodology will focus on the following initial events with selected engineering practices being employed during execution of these events: 1) Product Backlog Refinement 2) Iteration Planning 3) Iteration 4) Iteration Review 5) Iteration Retrospective The core of XP is the interconnected set of software development practices listed below. While it is possible to do these practices in isolation, many teams have found some practices reinforce the others and should be done in conjunction to fully eliminate the risks you often face in software development. The XP Practices have changed a bit since they were initially introduced. The original twelve practices are listed below. If you would like more information about how these practices were originally described, you can visit?http://ronjeffries.com/xprog/what-is-extreme-programming/. Planning Games Small Releases Metaphor Simple Design Testing Refactoring Pair Programming Collective Ownership Continuous Integration 40-hour week On-site Customer Coding Standard Below are the descriptions of the practices as described in the second edition of Extreme Programming Explained - Embrace?Change.?These descriptions include refinements based on experiences of many who practice extreme programming and reflect a more practical set of practices. Sit Together . Since communication is one of the five values of XP, and most people agree that face to face conversation is the best form of communication, have the team sit together in the same space without barriers to communication, such as cubicle walls (note that there are always exception to this rule for remote workers - but these exceptions should be minimized). Whole Team . A cross functional group of people with the necessary roles for a product form a single team. This means people with a need as well as all the people who play some part in satisfying that need all work together on a daily basis to accomplish a specific outcome. Informative Workspace . Set up the team space to facilitate face to face communication. Allow people to have some privacy when they need it, and make the work of the team transparent to each other and to interested parties outside the team. Utilize Information Radiators to actively communicate up-to-date information. Energized Work . Teams are most effective at software development when they are focused and free from distractions.?Energized work means taking steps to make sure the team is able physically and mentally to get into a focused state. This means do not overwork the team (surges should be minimized). Pair Programming . Pair Programming means all production software is developed by two people sitting at the same machine. The idea behind this practice is that two brains and four eyes are better than one brain and two eyes. This practice effectively provides a continuous code review and quicker response to nagging problems that may impede one person doing it on their own.?Teams that have used pair programming have found that it improves quality and does not actually take twice as long because they are able to work through problems quicker, and they stay more focused on the task at hand, thereby creating less code to accomplish the same thing. Stories . Describe what the product should do in terms meaningful to customers and users. These?stories?are intended to be short descriptions of things users want to be able to do with the product that can be used for planning and serve as reminders for more detailed conversations when the team gets around to realizing that particular story. Weekly Cycle . The Weekly Cycle is synonymous to?an?iteration. In the case of XP, the team meets on the first day of the week to reflect on progress to date, the customer picks the stories they would like delivered in that week, and the team determines how they will approach those stories. The goal by the end of the week is to have running tested features that realize the selected stories. The intent behind the time boxed delivery period is to produce something to show to the PMO for feedback. Quarterly Cycle . The Quarterly Cycle is synonymous to a release. The purpose is to keep the detailed work of each weekly cycle in context of the overall project. The Customer lays out the overall plan for the team in terms of features desired within a particular quarter, which provides the team with a view of the forest while they are in the trees, and it also helps the customer to work with other stakeholders who may need some idea of when features will be available. Remember when planning a quarterly cycle the information about any particular story is at a relatively high level, the order of story delivery within a quarterly cycle can change and the stories included in the quarterly cycle may change. Revisiting the plan following each iteration provides an opportunity to keep everyone informed as soon as those changes become apparent to keep surprises to a minimum. Slack . The idea behind slack in XP terms is to add some low priority tasks or stories in your weekly and quarterly cycles that can be dropped if the team gets behind on more important tasks or stories. Put another way, account for the inherent variability in estimates to make sure you leave yourself a good chance of meeting your forecasts. Ten-Minute Build . The goal with the Ten-Minute Build is to automatically build the whole system and run all of the tests in ten minutes. The founders of XP suggested a 10 minute time frame because if a team has a build that takes longer than that, it is less likely to be run on a frequent basis, thus introducing longer time between errors. This practice encourages the team to automate the build and test process to run on a regular basis. This practice supports the practice of Continuous Integration and is supported by the practice of Test First Development. Continuous Integration . Continuous Integration?is a practice where code changes are immediately tested when they are added to a larger code base. The benefit of this practice is the development team can catch and fix integration issues sooner. Most teams dread the code integration step because of the inherent discovery of conflicts and issues that result. Most teams take the approach \"If it hurts, avoid it as long as possible\". Practitioners of XP suggest \"if it hurts, do it more often\". The reasoning behind that approach is that if the development team experiences problems every time they integrate code, the more frequently they integrate, the smaller the changes and the easier to determine the source of the problem. This practice requires is highly dependent on Ten Minute Build and Test First Development. - Test-First Programming. Instead of following the normal path of: Develop code -> write tests -> run tests the practice of Test-First Programming follows the path of?Test Driven Development (TDD): Write failing automated test -> run failing test -> develop code to make test pass -> run test -> repeat As with Continuous Integration, Test-First Programming reduces the feedback cycle for developers to identify and resolve issues, thereby decreasing the number of bugs that get introduced into production. Incremental Design . The practice of?Incremental Design?suggests that the team does a little bit of work up front to understand the proper breadth-wise perspective of the system design, and then dives into the details of a particular aspect of that design when it delivers specific features. This approach reduces the cost of changes and allows the team to make design decisions when necessary based on the most current information available. The practice of Refactoring was originally listed among the 12 core, but was incorporated into the practice of Incremental Design. Refactoring is an excellent practice to use to keep the design simple, and one of the most recommended uses of refactoring is to remove duplication of processes. The biggest impact on instituting this practice is determining the scope of the governments formal design reviews (PDR/CDR) as this practice provides the inputs to these activities during the actual development iteration. 5.4.1 XP Events Using the Scrum framework for the baseline of events to start the XP process, the following events are summarized here to avoid redundancy with the Scrum section above. Only key differences will be presented here along with a recommendation of which engineering processes could be incorporated into these events (note that many of the engineering practices can be employed in multiple events - below is only one recommendation): a. Product Backlog Refinement - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : Backlog refinement focuses on providing a prioritized set of \"ready\" (see Definition of Ready above) user stories for the development team to accept into the next iteration as well as a \"ready\" reserve set of user stories which are available to the team during the iteration in case additional capacity becomes available. XP differences : Introduction of Spike to do focused research on stories which require more clarity. Applicable Engineering Practices : Quarterly Cycle, Stories, Incremental Design b. **Iteration Planning - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : During this meeting, the Product Owner will review the current list of Ready user stories to ensure that they are still valuable (relevant), as well as reprioritize the Backlog based on existing value as necessary (this facilitates the team's acceptance of work into the iteration as they simply take user stories from the top of the list within their established iteration capacity. XP differences : XP uses iterations which are normally 1-2 weeks long. Applicable Engineering Practices : Weekly Cycle, Slack c. Iteration - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : Iteration execution consists of developers completing their work, testers executing the tests developed prior to the iteration (with the addition of any necessary negative test cases), and BAs and testers reviewing the completed work with the Product Owner in order to receive early feedback or formal closure of the story as meeting the teams \"Definition of Done\". XP differences : Focus of XP is on Test-First Programming and Pair Programming in the execution stage (tests fail at the beginning because the code is not developed - working in pairs, the developers develop the code to pass the tests, when development testing is done - the team testers do a final verification of the initial tests along with any additional negative tests they have written. This process can also be facilitated by implementing a Continuous Integration system to maximize the use of automated testing. Applicable Engineering Practices : Sit Together, Informative Workspace, Energized Work, Pair Programming, Test-First Programming, Continuous Integration, Ten-Minute Build d. Iteration Review - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : The iteration review is a presentation to Stakeholders of the completed increment based on acceptance by the Product Owner. The key to the review is that it is interactive between the Stakeholders and Development Team - the focus being to provide a feedback mechanism to improve the product. XP differences : None. Applicable Engineering Practices : None e. Iteration Retrospective - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : This is where the Product Owners and the Development Team focus on improving the Agile process. Critical to understanding the concept of the retrospective is that learning always occurs during these events whether the iteration was successful or not. XP differences : Focus will not only be on events - it will include the team's selected engineering practices. Applicable Engineering Practices : Whole Team 5.5 Part III - Establish the Team The following section provides a general overview of the roles required for effective execution of the different agile methodologies presented in this playbook. Note that these are \"roles\", and, based on the team size, can be implemented by one or more people. Also, the same person can have multiple roles (i.e. a BA and a Scrum Master) though this might not lend to optimal effectiveness. 5.5.1 Scrum Roles and Responsibilities The relationship between development team members and the PMO is critical as understanding between the two will breed the trust necessary for successful execution of an agile process. Product Owners : These are representatives of the PMO with the necessary functional knowledge of the system to be able to collaborate effectively with the development team. One of the key aspects within the government environment is that often, the Product Owner is a collection of individuals representing different perspective (Program Management, Functionality, Engineering, and Cybersecurity are examples of these perspectives). Whether there are 1 or more people representing this position, individual or group must have the ability to make decisions on the requirements to effectively collaborate with the business analyst and the development team. Note that a government Project Manager may fulfill the role of the Product Owner, but they must have the functional knowledge to allow for effective clarification of requirements to the development team. POs are responsible for establishing: a. Requirements / Acceptance Criteria b. Priorities c. Clarification on Requirements for the Development Team d. Approval of \"Done\" for stories. Development Teams : Formed around the specific projects, their purpose is to create quality code that produces value for the government. They have the following components (the size and composition of each team will vary based on the development requirements within a release): a. Scrum Master - They are the Process Advisor to the PO. b. Business Analyst(s) - Responsible for gathering the requirements and translating them into documentation which not only reflects the intent of the PO but are also understandable and executable by the developers within the team. c. Developers (General) - Responsible for - focus is functionality to the users. e. Tester / Quality Assurance (QA) - Responsible for implementation of quality within the team. 5.5.2 Kanban Roles and Responsibilities For Scrum, the Product Owner, Scrum Master and Development team roles must be assigned. In Kanban, the workflow dictates the role requirements. The refinement work can be done prior to entering the workflow (thus requiring a government Product Owner and a team Business Analyst) or the work can be part of the workflow (i.e. a column for refinement). Depending on the policies of the Kanban board workflow, resources will be allocated which can work on the work items within the column. Optimally, there would be specialists as necessary for the different workflow aspects and a couple of generalists could work to ease bottlenecks within the workflow. Additionally, while an Agile Lead familiar with Kanban would maintain the discipline within the system and would also enable faster optimization of the system, a Project Manager could also serve as the team lead for managing the workflow. However, to provide a checklist to begin with in implementing a Kanban team (and to provide flexibility to adapt to or adopt other methodologies), the recommended roles for a Kanban Team startup would be: Agile Lead / Coach - Similar to Scrum Master above but experienced in the execution of Kanban. Product Owner - Same as Product Owner in Scrum. Development Team Members - Determined as necessary to enable the workflow (i.e. business analyst, developers, testers, cybersecurity personnel as necessary based on the different policies of the Kanban Board). Key here is the team is not required to be cross-functional so can be formed around specialists and generalists as necessary. 5.5.3 XP Roles and Responsibilities Although Extreme Programming specifies engineering practices for the development team to follow, it does not really establish specific roles for the people on the team.?Depending on the reference material on roles in XP, there is either no guidance, or there is a description of how roles typically found in more traditional projects behave on Extreme Programming projects. Here are four most common roles associated with Extreme Programming:? ? The Customer - The Customer role in XP is almost exactly the same as the Product Owner Role in Scrum (same responsibilities stated above). ? ? The Developer - Because XP does not have much need for role definition, everyone on the team (with the exception of the customer and a couple of secondary roles listed below) is labeled a developer. ? The Coach - This role is similar to the Scrum Master role for Scrum. The fundamental difference is that this Agile lead should have experience with XP Practices.? The main value of the coach is that they have gone through it before and can help the team maintain practice discipline and avoid process mistakes.? 5.5.4 Roles and Responsibilities Plays Since the core roles are essentially the same between the different agile frameworks, there will only be one play for each of the roles common to all frameworks. We will not be discussing developers and testers / quality assurance here as these are roles common to IT development environment. However, when drafting job requirements, the following should be added in the preferred qualifications: Experience in agile development methodologies (Scrum, Kanban, and/or XP) The following agile related roles have plays associated with them: Agile Lead (includes Coach and Scrum Master) Product Owner Business Analyst The contents of each role play is the following: Recommended skill sets Recommended qualifications Recommended certifications General Responsibility Overview Link: Appendix B: Key Personnel Plays","title":"-5 Agile Framework"},{"location":"agileframe/#5-agile-framework","text":"","title":"5 Agile Framework"},{"location":"agileframe/#51-part-i-establish-the-product-backlog-pb-and-constraints","text":"","title":"5.1 Part I - Establish the Product Backlog (PB) and Constraints:"},{"location":"agileframe/#511-establish-the-product-backlog","text":"For a government project the initial list of requirements is established within the contract and will be used as the foundation for the initial PB development (and captured within the appropriate Backlog Management tool). The establishment of the PB is comparable to the establishment of a language - in this case it is the language of \"value\" that provides for understanding between the government and the development team. For this \"value\" language - the individual PB components provide the words and grammar (think of Epics as Paragraphs, Features as sentences and User Stories/Cards as words), while the order of the backlog based on prioritization of \"value\" provides the context of the product story. This initial section focuses on the creation of the Product Backlog. Later sections will discuss how the PB is prioritized to provide the most value up front as well as how it evolves based on a changing environment to continue to remain value relevant. As the PB will generally follow a rolling-wave planning process with more detail provided as it becomes available or necessary, it can initially be setup with larger organizational elements (epics and features) which will be iteratively refined later. A recommended nested organization for these components include: Epics - These include functionality which can take one or more releases to complete. For government contracts, these can be linked to a Contract Line Item Number for easier reference to cost tracking and earned value metrics. Features - These include functionality components that will generally take more than one iteration to complete and provide the functionality to the users through releases. Product Backlog Items (PBI) - These include the product increments which consolidate to provide the functionality of the feature and generally can be accomplished within a single iteration. PBIs can be decomposed into specific tasks with hours estimated for completion. The main component of the PB is the PBI that may be named something different depending on the framework (for Scrum and XP can be a user story, for Kanban can be a simple work item or card). In translating a requirement to a PBI, it is necessary to understand that the PBI can contain 1 or more requirements as it is based on the customer functional need (can be captured in government use case documents). However, while functional requirements will most likely include BES Process Directory requirements terms including \"shall\" which dictates the provision of a functional capability, they may also include conditional requirements indicated by the \"must\", \"must not\" and \"required\" requirements descriptions indicating additional performance requirements or constraints). Additional non-functional requirements to be considered include: Preparation and conduct of design reviews Hardware setup (Different environments) Tool setup (Software tools) Network Setup (establishing connectivity - access requirements, ports and protocols) UAT, Pre-Prod, Prod transitions (can be facilitated by continuous integration and Devops) Documentation (User Guides / CDRL's / Training Materials Regression testing (can be facilitated by automated testing) Cybersecurity testing The typical PBI contains a minimum of 3 elements - Title: Ensure this ties to the logic of the work (many times the titles of a PBI can cause confusion between the PO and development team if improperly stated). Description: Explains the customers need and the functionality required. Acceptance Criteria: Explains the conditions under which the PO will accept the work item as complete (note that there are additional criteria which will be discussed in presenting a \"Definition of Done\"). These criteria should be testable (I.e. should be specific enough to be able to qualify as complete or not complete). A typical PBI descriptions contains the following elements: \"As a...\" - Defines the perspective of the user who needs the functionality. \"I need...\" - Defines the functionality needed. \"So that...\" - Defines the why of the functionality (enables the developers' ingenuity as they may have a better solution which still meets the \"why\" of the user's request though the how may be something different). Example: As a maintenance scheduler I need to be able to review the Daily Schedule So that I can review the events scheduled for the day and the status of their completion Additional information can be included within the description. One recommendation is to include the requirement number(s) from the original contract in this section to facilitate the creation of the Requirement's Traceability Matrix necessary for design reviews and deliveries. While the PO is the primary owner of the backlog, the initial creation of the backlog is most effective when completed in a collaborative manner. While the term Sprint 0 is not an official Scrum term, it does provide a context for doing the preparation work necessary to initiate an agile methodology (without the PBI's it is not possible to do the work). Whatever it is called, there is a preparation session which is necessary to prime the pump of the agile framework by building the initial PB. Here is a recommended approach to doing this which ends with the government System Functional Review (SFR) which established the PB as the Functional Baseline: Preparation Phases (Phases are provided here as timeline will have to adjust based on size and complexity of requirements as well as whether preliminary planning has been done...i.e. feature driven planning): Phase 1: (Government and Development Team collaboration preparation) Government provides initial Requirements Traceability Matrix (RTM). Government provides initial feature backlog linked to RTM to development team (with minimum of description filled out and rough draft of acceptance criteria). Development team reviews feature descriptions and provides feature backlog feedback to government at end of the phase with recommended revised acceptance criteria and questions of clarification for review prior to physical collaboration. Phase 2 (Physical Collaboration - government and development team) Collaboratively refine features with end-state of initial Title, Description, and Acceptance Criteria agreed on by government and development team Conduct breakout sessions as necessary to provide additional details to development team necessary to make initial estimates of feature sizes Development team makes initial estimates of feature sizes (T-Shirt sizing). Development team prepares for SFR.","title":"5.1.1 Establish the Product Backlog"},{"location":"agileframe/#512-system-functional-review-sfr","text":"The SFR is a multi-disciplined system-level technical review ensures that the functional baseline is established and has a reasonable expectation of satisfying the requirements of draft capability development document (CDD) within the current budget and schedule. It completes the process of defining the system-level technical requirements that are documented in the system performance specification. According to the BPD, a successful completion of SFR provides a sound technical basis for proceeding into preliminary design. If the above collaboration is done correctly, this review becomes a \"value language\" confirmation brief ensuring that the PB correctly reflects the requirements of the contract and are consistent with cost (program budget), risk and other system constraints. Audience: Functional Review Board (FRB) members Objectives: Reconfirm that Capability Package Requirements are linked to PB Features derived during the preparation event above (Sprint 0) Initial identification of system-level document changes which will need to be addressed during design reviews (see CDRL section (link within main document)) Establish Functional Baseline. Add Feature list to the Program Product Backlog to be considered in Release Planning for development based on PMO priorities and value determination. Determine based on Release Planning when each Feature should be considered within either the Preliminary Design Review or the Critical Design Review. Outputs: Required inputs for a Configuration Control Board (CCB). Approved PB and Functional Baseline based on Configuration Control Directive (CCD) from CCB. Updated system release roadmap with new features. Updated Requirement Traceability Matrix with links from requirements to PB. Identified system-level CDRLs to update for design reviews based on PB contents. Identified features for design reviews (Preliminary Design Review [PDR] and Critical Design Review [CDR])","title":"5.1.2 System Functional Review (SFR)"},{"location":"agileframe/#513-release-management","text":"Release management is about determining what is expected to be in each release and what comes next for the development team as well as identifying PBI dependencies early enough so that they can be rectified prior to becoming impediments during development. The PB includes those features approved in the functional baseline by the PMO during the System Functional Release (SFR) conducted at the end of Sprint 0. The features are prioritized within the PB by the PMO and considered by a Release Planning committee for inclusion in upcoming releases. Those features which are selected for inclusion in upcoming releases are updated on the Release Roadmap based on their tentative estimate of completion. The Release Roadmap is a high-level view of the PB features, with a loose time frame for when the team will refine and develop those features. Proposed agenda for Release Planning meeting: Objective Overview Review Current Release Roadmap (Starting point) Review Current Status of Development Streams (Establish current status which may have changed since last release planning session) Determine dates for: Next Release (Feature focus for the CDR) Next Release (+1) (Feature focus for the PDR) Review Product Backlog Feature Priority Lists (Confirm priorities) Confirm Features for Next Release Identify Features for Next Release (+1) Update Release Roadmap (Use information from session to update the release roadmap) Discuss any release process improvement steps (Continuous Improvement)","title":"5.1.3 Release Management"},{"location":"agileframe/#514-system-engineering-design-reviews","text":"Link to DAU System Engineering Overview (Add to links appendix - will not be contained in base document): https://www.dau.mil/guidebooks/Shared%20Documents%20HTML/Chapter%203%20Systems%20Engineering.aspx#toc83 A secondary advantage of implementing a pro-active release management system is the ability to establish what features will be reviewed at what level for Agile-based design review gates (here we are talking about Preliminary and Critical Design Reviews but not in the level of detail that was expected within traditional development engineering management plans as Agile is about doing architectural design \"just in time\" and iteratively \"just enough\"). The Design Reviews can be synchronized with the above release management process to fulfill their primary objectives based on two different perspectives: Preliminary Design Review (PDR): Focus : The features included in the release after the upcoming release (long-term perspective). Objectives : Provide sufficient confidence in the preliminary design's integration with necessary system components to serve as the starting point for Agile incremental design during development. Provide technical confidence that the capability need can be satisfied within cost goals based on the schedule included in the release roadmap. Confirms that high-level design decisions are consistent with the user's performance, schedule needs, and the validated Capability Development Document (CDD). Establishes the allocated baseline (based on features), which is placed under formal configuration control. Agile Revisions : Identify dependencies on external requirements or other features to ensure proper sequencing and identify issues for resolution prior to them becoming impediments (identifies feature risks early). Determine required infrastructure requirements so they can be acquired prior to development. Agenda Template : Review Release Roadmap Review Features for the release after the immediate upcoming release. Review Feature description Review Feature acceptance criteria Review Features system-level design integration (focused on necessary design integration of feature capability) Review Feature dependencies (I.e. external interface requirements, additional software requirements) - these will become an initial set of assumptions for risk management (the goal is to mitigate the risks so that the assumption will not impede our development efforts later) Review Feature CDRL requirements (what CDRLs will need to be updated based on the completion of the feature development) Review Feature size estimates (establishes metric for estimating duration) Revise Release Roadmap as necessary based on updated information Critical Design Review (CDR): Focus : The features included in the next (upcoming) release (short-term perspective). Objectives : Establishes the initial product baseline (based on features), which is placed under formal configuration control. Confirms the high-level system design is expected to meet system performance requirements Confirms the system is on track to achieve affordability and should-cost goals as evidenced by the design documentation Establishes requirements and system interfaces for enabling system elements such as support equipment, training system, maintenance and data systems. Agile Revisions : Confirms that dependencies identified in the PDR have a resolution strategy that will be completed in time to enable development of the dependent features (will not become impediments during development). Confirms that features are ready for further refinement into work items for the development team. Agenda Template : Review Release Roadmap Review Features for the upcoming release. Review Feature description Review Feature acceptance criteria Review Features system-level design integration and system requirements: UI change requirements (mock-ups if available) Middle tier requirements (services) Data structure requirements Review Feature dependencies to ensure they have been resolved or a feasible resolution strategy is in place prior to beginning work on the feature (these address the initial assumptions determined during the PDR) Review Feature CDRL requirements - update as required Review Feature size estimates (establishes metric for estimating duration) - update based on additional information Revise Release Roadmap as necessary based on updated information","title":"5.1.4 System Engineering Design Reviews"},{"location":"agileframe/#515-refine-the-product-backlog-features-initial-pbis","text":"In general, the refinement process establishes the \"development language\" between the Development Team and the Government through their designated PO. Feature refinement should be conducted about 2 months prior to working on the first PBI of the feature. The objectives of feature refinement are to: Break down feature to individual PBIs (user stories, cards, work items, etc) with, at a minimum, a description and acceptance criteria; Provide an initial estimate of the complexity of the work item (sizing estimates can be associated with established costs and used for EV metrics); Gain the formal approval of the PO to include the refined work items into the Product Backlog. Feature refinement in preparation for development work conducted between the government PO and Development Team: Normally takes 1-2 hours per feature if done correctly (the more discussion, the better clarity of understanding between the government and the development team increasing the likelihood that the development product will meet government expectations). Refine initial title, description, acceptance criteria at the feature - level based on knowledge acquired since design review - this is the focus of the PMO input - provides the WHAT of the feature. Identify the logic of the work. Identify the initial user stories. Ensure that the stories have a description and acceptance criteria which provide a common reference for the team to produce an initial sizing estimate (provides metric for EV - can be updated later). Identify infrastructure requirements (i.e. suites) necessary for development. Three questions to guide Feature Refinement Discussion and what they are attempting to elicit: What do you want to see at the end? (Acceptance Criteria) What are the high-level steps to get there? (Logic) What are the sub-steps for each of the high-level steps? (PBIs)","title":"5.1.5 Refine the Product Backlog (Features -&gt; Initial PBIs)"},{"location":"agileframe/#516-prioritize-the-product-backlog","text":"Agile is about providing value to the government customer as quickly as possible. Less valuable work is ranked lower in the priority scale. Thus, after refinement of the features into PBIs within the PB, it is necessary to rank them based on the value they provide. There are multiple considerations in evaluating the value of each individual PBI including: What provides most value functionally to the government user? What impact does the PBI have on other items within the backlog (i.e. are their dependencies between work items)? Cost / impact assessment of each PBI. Learning - Are their efficiencies to be gained for the remaining work items by completing a specific story earlier (i.e. taking a smaller item which is easier to do to establish the templates and processes for doing more complex work items of a similar nature later but at a more rapid pace)? Are their negative impacts to other PBIs by not completing one earlier? Regulatory deadlines (not considered agile, but is a consideration for prioritization) It is the responsibility of the PO, as the representative of the government to continuously reprioritize the PB to provide the most value to the government based on the current environment (especially as new PBIs are added or removed from the PB). Prioritization considerations to avoid (as they are normally short-term perspectives which may reduce long-term value): - Highest Ranking Person in the Room - Flavor of the Day","title":"5.1.6 Prioritize the Product Backlog"},{"location":"agileframe/#517-constraints-analysis","text":"The above sections focus on refining requirements into work items, but there are several; other areas of regulatory constraints within the governmental regulatory environment that should be addressed prior to determining as well as implementing an Agile development methodology. The following is a non-exhaustive list of items that may impact the execution of development based on scope (these provide a basis for discussion between the Government and Development Team to determine which will be enforced and to what level): External testing requirements such as DT&E and QT&E (capacity may be required to prepare and support testing - need to delineate what falls to the development team and what will be done by the government) Additional BPD Design, Test, Functional Readiness Reviews (all come with a preparation and documentation overhead - will need to discuss expectations and agile modifications based on iterative versus up front design) Development environment limitations (discussion around whether the contractor will control the environment or whether it is an externally administered environment - I.e. the Capabilities Integration Environment (CIE)) Access requirements (security requirements may dictate which personnel resources can be allocated to a team) Re-usability and compatibility requirements (establishes the boundaries of initiative for the developers - \"can be innovative within these limits\") Cybersecurity requirements (the pre-requisite cybersecurity requirements may limit development options)","title":"5.1.7 Constraints Analysis"},{"location":"agileframe/#518-change-management","text":"While there is an inherent flexibility within Agile methodologies to accept change specifically through backlog grooming and PBI refinement, there is still the challenge of differentiating between contractual scope additions versus simple requirement refinement. The differentiation of what will be left within the authority of the PO to approve through backlog refinement and what requirement changes need formal approval in the form of a Configuration Change Directive (CCD) should be established prior to the beginning of development to ensure proper steps are taken within the established constraints to adapt to change. When a formal configuration change is necessary, the government Configuration Control Board (CCB) with input from the Functional Review Board (FRB) should: Review the list of any new user requests, deficiency reports and change requests received in the project space; Assess the impact of a change; Assign a priority to the change; Assign a business value to the change; and Implement a CCD to insert an approved work item for the change into the Product Backlog (upon completion of any contractual modification requirements to accept in the new work)","title":"5.1.8 Change Management"},{"location":"agileframe/#52-part-ii-establish-the-methodology","text":"","title":"5.2 Part II - Establish the Methodology:"},{"location":"agileframe/#521-definition-of-ready-and-done","text":"Prior to implementing a specific development methodology, the definitions for the development entry and exit criteria need to be defined. Thus, prior to beginning development through any of the methodologies, two definitions should be agreed upon between the PO and development team in reference to individual PBIs: Deifinition of Ready Purpose Describes materials and topics that must be included and addressed in well written backlog items (I.e. Scrum / XP User Stories, Kanban cards) Used to evaluate whether or not a PBI has been appropriately elaborated and is ready for development Helps ensure that backlog items are complete and understandable before being scheduled for an iteration. EXAMPLE DEFINITION OF READY CHECKLIST Description clearly states the who (\"As a\"), what (\"I want to\"), and why (\"so that\") are for the backlog item PO has approved the backlog item Acceptance criteria is clearly defined Scenarios and expected outcomes are clearly defined Artifacts defining user interface requirements are included (if applicable) Business rules are referenced or included (if applicable) Dependencies are identified Development Team has reviewed and confirmed they understand the backlog item Backlog item is appropriately estimated (I.e. sized for scrum) \"Definition of Done\" for the PBI is understood Definition of Done - Purpose - The \"Definition of Done\" describes criterion that must be met by each committed User Story or Defect to development before the PO can validate that the PBI is accepted. - Definition is used to help ensure that the developed product is consistent with the associated backlog item, is high quality, and is ready for production testing. - \"Done\" establishes quality norms and assures the PO that major defects are not likely to be identified during production testing EXAMPLE DEFINITION OF DONE CHECKLIST Unit testing and module testing are complete All acceptance criteria defined within the PBI are met Test results have been reviewed with the PO The PO concurs with the adequacy of the testing The backlog item has passed acceptance testing by the PO The development product has been demonstrated and accepted by the PO","title":"5.2.1 Definition of \"Ready\" and \"done\""},{"location":"agileframe/#522-scrum","text":"This section provides a template for the scrum process. As it is a template, government and development teams are encouraged to adapt the process based on team structure, culture and regulatory constraints to make it more efficient. To that effect, this section is prescriptive versus directive in nature. Also note that we have transitioned from the general agile term of PBI to user stories as they relate to Scrum. Scrum Methodology: Scrum Events The five key Scrum events are: Product Backlog Refinement - Based on the project release roadmap and PO priorities, there are two components to refinement: Feature Refinement - Breaking down features and setting priorities (this process was explained in Section 5.1.5 above under feature refinement) User Story Refinement - Grooming user stories so that they are ready to be accepted into a sprint Sprint Planning: Determining the goals and related stories for a sprint Sprint Execution - Developing and testing stories so they are done Sprint Review - Providing feedback to improve the product Sprint Retrospective - Determining how to improve the process Product Backlog Refinement Feature Refinement Session (2 Sprints early) : Coordinated by the Scrum Master with the Solution Architect and Customer System Engineer (and POs if available) to: Ensure that the feature acceptance criteria are still correct for upcoming features originally determined during Sprint 0 activities. Ensure that all dependencies (possible future impediments) have a resolution strategy prior to development. Ensure that design tenets are understood within the context of the feature work. User Story Refinement Collaboration (Between Feature and User Story Refinement Sessions) : Team designated Business Analyst works collaboratively with the PO to refine story priority and the acceptance criteria necessary to accomplish the associated feature functionality. Team testers can also add necessary positive test cases based on draft acceptance criteria to the story (facilitates test driven development). Refinement Session - User Stories (1 Sprint early) : Business Analyst discusses each story with the development team to ensure understanding of delivery requirements. The PO is included in this refinement session, but it is the development team that is being introduced to the stories. The PO is there to confirm through listening to the team's conversations with the Business Analyst that his/her intent is understood within the context of the story. This refinement session is done to ensure that each story meets the established Definition of Ready prior to being accepted into the sprint for development. Meeting the Definition of Ready : The acronym INVEST provides a high-level cross-check of whether a story is Ready for development. It is normally part of the last check done on a story during either Refinement Day activities or as part of Sprint Planning for priority stories that have been updated since Refinement Day. While it is presented here under the Scrum section, it can be used with modification (I.e. Kanban work flow estimates versus sprint estimates) within the other agile frameworks as a final check for each PBI). The acronym represents the following criteria: (I) Independent - Story contains no dependencies on unavailable external resources or preparatory user story completion (N) Negotiable - Three perspectives must be represented in refinement of the user story to meet the PO's intent - Business Analyst, Developer, Tester - if they have not been involved in refinement, a key perspective on the complexity of the story is missing from the preparatory analysis. (V) Valuable - Established by the prioritization given by the POs. (E) Estimable - Estimates provide a gage for the acceptance of future work levels into a sprint as well as for forecasting future work timelines - they gain value over time as the team evolves. (S) Small - User stories should be small enough to complete within a sprint while large enough to provide observable value to the PO. (T) Testable - Positive manual test cases will be added to appropriate user stories prior to being taken into the sprint to account for successful achievement of acceptance criteria. As necessary, these tests will be augmented during the sprint with additional negative manual and automated tests. Based on the high-level of interaction between the team and the PO in this option, each story should have as a minimum 3 touch points with the PO to confirm that PO's intent is maintained throughout refinement. At a minimum, these touch points are the verification of the acceptance criteria in the documentation, the answering of team questions during team refinement of stories, and the final acceptance of the story as ready for development At this point the story is considered \"ready\" for acceptance into a sprint during sprint planning. Sprint Planning Sprint Planning (Appendix A) Planning centers around the presence of the PO within the Planning meeting. During this meeting, the PO will review the current list of Ready user stories to ensure that they are still valuable (relevant), as well as reprioritize the Backlog based on existing value as necessary (this facilitates the team's acceptance of work into the sprint as they simply take user stories from the top of the list within their established sprint capacity. Sprint Execution Sprint Execution (Appendix A) While developers and testers work to complete and validate the work required for each user story, Business Analysts and, if necessary, POs are available for clarification of requirements to ensure that the work produced meets the intent of the customer. If refinement has been done correctly, sprint execution consists of developers completing their work, testers executing the tests developed prior to the sprint (with the addition of any necessary negative test cases), and BAs and testers reviewing the completed work with the PO in order to receive early feedback or formal closure of the story as meeting the teams \"Definition of Done\". The user story acceptance criteria is the foundation for this definition. The story points assigned to sprint user stories not designated as \"Done\" are not counted when measuring sprint completion velocity. Sprint Review Sprint Review (Appendix A) Story acceptance as Done is completed by the PO only (the same person who approves the acceptance criteria must be the person who approves work against the approved acceptance criteria). If the PO believes that the work does not satisfy the customer requirements, they have to make a decision: if the work does not meet the acceptance criteria, they can reject the work as not done and provide additional clarification; if the work meets the acceptance criteria, but the PO wants additional revisions, additional user stories are created and prioritized within the existing backlog. The key to the review is that it is interactive between the PO and Development Team - the focus being to improve the product. Sprint Retrospective Sprint Retrospective (Appenidx A) This is where the POs and the Development Team focus on improving the Agile process. Critical to understanding the concept of the retrospective is that learning always occurs during these events (Successful sprints can indicate items to sustain; failed sprints provide opportunities to learn where the team can improve). One note to remember in the retrospective is that the process should facilitate the development efforts - not impede them (hence why this document is presented as a template - it needs to be adjusted to fit the needs of the agile environment in which it exists which is a combination of the people, processes, tools, relationships and levels of trust involved in the project).","title":"5.2.2 Scrum"},{"location":"agileframe/#53-kanban","text":"Considerations for establishing a Kanban process: Establish the Kanban Board - Visualize the workflow to organize, optimize, and track the work item development progress (allows for transparency of status) Limit Work in Progress (WIP) - Kanban is about flow. The more items in progress, the more risk of nothing getting done. It is better to have a limited WIP that emphasizes the Lean pull system. That is, when a resource has completed work on a work item and moved it to the next column, s/he pulls the next work item from the column before it. A starting rule for each policy column is limit WIP to amount of column resources + 1 (i.e. for a development column - the WIP would be the number of developers on the team plus one - that allows for a developer who hits an impediment to stop work on that work item and pull another one from those completed in the previous column while the Agile Lead works to remove the impediment. This allows for the flow to continue). Key points in Kanban: Minimize Lead Time (the time a work item is in any specific stage / column) Minimize Cycle Time (the time for the entire work flow cycle to complete for a single work item) - since establishing a cycle time allows for estimation of work items, but not all work items are the same complexity - one methodology for differentiating the work item complexity is to provide multiple swim lanes for tracking work progress - One for work items which have an expected cycle time of 1 week One for 2-week items One for 1-month items Optional - an expedited swim lane (similar to the express lane at a supermarket) for high-priority work items that should take precedence over all others (this will be mentioned again as a strategy for using Kanban to manage Operations and Maintenance work items) Manage flow - The Agile Lead works with the development team to ensure that workflow issues are identified and resolved (i.e. if there is a backup in the workflow, an additional resource may be needed to relieve pressure on the bottleneck column) Make process policies explicit - Each column on the Kanban Board should have an explicit policy which indicates when the work in that column is complete (policy = that column's \"Definition of Done\") Improve the process - In collaboration between the development team and PMO - there should be a focus on continuous improvement of the process (re-define the policies, determine whether columns should be added, determine whether columns should be deleted) While Scrum focuses on a cross-functional team, Kanban focuses more on aligning the resources with the policies. A simple representation of a Kanban Board (this also provides a quick start-up template for HIA products) is: Templated steps / policies for an initial Development Team Kanban board: Backlog : PMO PO prioritizes the Work Items in the Backlog Identify : Development Team Business Analyst in coordination with the PMO PO will identify / refine the description and acceptance criteria for the work item PO Approved : PMO PO will approve the description and acceptance criteria (Confirms that the work meets functional requirements - think mini-PDR) Design : Development Team's Senior Developer will complete draft solution design Cyber Assessment : Developer's cybersecurity representative (ICW PMO Cybersecurity) will complete security assessment of solution PMO Engineer Approved : PMO Engineer will approve design of solution (Confirm that solution meets technical requirements - think mini-CDR) - work item now \"ready\" for development. Develop / Implement : Developers implement the solution Test : Development team testers and cybersecurity will conduct testing and update documentation as needed Pending Acceptance : Development team will demonstrate completed work item functionality to PMO Done : PMO will accept work item as complete (meets the Definition of Done) An Operations and Maintenance team provides an example of a project built for Kanban. The backlog would contain the ticket items requiring resolution with the priority of work established by a government Product Owner. This approach allows refinement of last minute prioritized work items identified by system users through the workflow's column policies identified above (the process of making the work item \"ready\" is captured within the workflow). Also, if the Kanban board swim lanes are setup as described above, the board will contain an expedite lane for flowing through critical fixes requiring short duration resolution. (Note: Another example would be the cybersecurity team handling the implementation of required TCNO/IAVAs.)","title":"5.3 Kanban"},{"location":"agileframe/#531-kanban-cadences","text":"While Scrum Events focus on execution of the time-boxed process, the key Kanban events (cadences - named for their formal rhythm) focus on organizing, optimizing, and tracking the development process. The cadences indicated below are interdependent as represented in the figure above - some meetings inform others, while feedback from other meetings drive changes to the process. Strategy Review (Release Management) - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview: A Strategy Review session is held to periodically review the current status of the development effort as well as revise the Release Roadmap to ensure that it correctly reflects current value and priorities. The question answered during this meeting is: Are we doing the right thing? The Release Management process described before represents an example of the events and timelines reflected within this cadence. Example cadence : Quarterly or based on release period - should look at current and next release and how the PB reflects that plan / strategy. Stand-ups - Focused on the Kanban board and managing workflow Play: Link to Play below in the Appendix Overview: While the focus of the Scrum stand-up meeting is on the individual work and how it relates to the team's ability to meet sprint (time-boxed) goals (What did I do yesterday? What will I do today? What is impeding my work?), the focus of the stand-up for Kanban is managing the flow of work items among the Development Team resources to make it as efficient as possible (managing workflow to minimize the time any piece of work is in any stage of development). While the stand-up can be done daily, it is not a requirement of Kanban as the trigger for work to be pulled into the next column is automatic (completion of the column policy for that work item and next column resource availability). The cadence recommendation of a week is a goal - the team will most likely start out in a daily cadence until the workflow is better understood. Example cadence : Initially daily. Can evolve to once a week to coordinate the efforts of the team and resolve work-in-progress bottlenecks. Replenishment Meeting - Similar to the Sprint planning sessions - these meetings ensure that the work items which will be brought into the development stream are prioritized and \"ready\" to be worked on. Play: Link to Play below in the Appendix Overview: The replenishment meeting ensures that there is \"ready\" work prioritized in the backlog to be brought into the workflow. The battle rhythm for when these meetings are necessary are dependent on the workflow (the recommendation is to start by having this monthly and then determine whether it should happen more or less often). It should take the work that the government has identified (refined down to the work item level) and ensure that the work is \"ready\" to enter into the workflow. It is the last step in determining which prioritized work is up next and ensuring that the work still adds value to the government. Example cadence : Once every month (or as necessary to ensure that there is a \"ready\" backlog of work to feed the development stream). Delivery Planning Meeting - This meeting coordinates product hand-off based on current work delivered between multiple entities: Developers, Product Owners, Testers, Trainers, Users (can be facilitated with establishment of Continuous Delivery process and structure). Play: Link to Play below in the Appendix Overview: The Delivery Planning meeting is a discussion between multiple entities of the results of the actual workflow (while the Strategy Meeting establishes a plan, the Delivery Planning Meeting looks at the reality). An assessment is made of the Kanban board to determine what work in progress will be completed by a specific time to pass it on to the next entity in the chain. It is about establishing expectation management so that external agencies can plan for events which are dependent on the contents of the delivery. These include: PMO ensuring that reviewers are ready to conduct the Physical Configuration Audits and appropriate document reviews, The LDTO agency coordinating testing events based on specific RALOT. Users providing timely feedback; Functionals coordinating training events so that released functionality into production can be used. Example cadence : Once every month to update downfield receivers of the product (highly dependent on the release timeline - i.e. a release once a quarter might only require this meeting once a quarter). Operations Review - How do we improve the process from the organization's point of view Play: Link to Play below in the Appendix Overview: This review is similar to a Scrum retrospective (how to improve the process) but looks at the release management system from an organizational point of view. It looks at improving the entire system as it relates to incorporating the Kanban team workflow methodology into the government review and release system. While the Delivery Planning Meeting focuses on coordinating the efficient release of the product, this meeting is about improving the release process based on lessons learned. It looks back on each release from the perspective of all stakeholders to continuously improve the system for the next release. Example cadence : Once every 3 months (or by exception) following the Release Delivery Service Delivery Review - Review of the release product from the user's point of view. Play: Link to Play below in the Appendix Overview: This review is conducted to determine if the items being produced are actually meeting the user's needs. The Service Delivery Review focuses on what was produced from the end-user's point of view and whether it fulfills their functional needs as well as quality standards. It provides a direct feedback chain from the end user to allow the PMO PO insight into requirement and prioritization changes, as well as the development team to understand better how their product is being viewed by the users of the system (including their satisfaction and concerns). Example cadence: Once every 3 months (or by exception) following the Release Delivery. Frequency will vary based on user first look events or actual releases to the field - need to provide end-user enough time to review delivered product. Risk Review - Done as necessary to review all identified risks to determine the status of risk mitigation measures and issue / impediment resolution. Play: Link to Play below in the Appendix Overview: The Risk Review discusses the probability and impact of planning assumptions being wrong and what steps the PMO and development team are taking to minimize those risks. It focuses on what steps are being taken to minimize the probability of false assumptions impacting delivery cost or schedule. This review focuses on reduces uncertainties in the system in order to establish predictability and reliability in the system which enables trust to form within the different organizational entities including the PMO, development team, external testers, users, etc. Example cadence : Once every 2 weeks (or combined with PMO Risk Management Meeting)","title":"5.3.1 Kanban Cadences"},{"location":"agileframe/#54-extreme-programming-xp","text":"XP's primary contribution to the software development world is an interdependent collection of engineering practices that teams can use to be more effective and produce higher quality code. Many teams adopting agile start by using a different framework and when they identify the need for more?disciplined engineering practices they adopt several if not all of the engineering practices espoused by XP. The recommended methodology here is to begin with the Scrum Framework Events listed before and then adopt as necessary the XP engineering practices which enhance the process. General differences between Scrum and XP: 1) Scrum uses sprints which are normally 2-4 weeks long. XP uses iterations which are normally 1-2 weeks long. 2) Sprint backlogs are normally sacred while XP embraces more flexibility if a higher priority work item comes in (understanding that it will replace an equivalent work item already accepted into the iteration). 3) If the team identifies some stories that they are unable to estimate because they don't understand all of the technical considerations involved, they can introduce a spike to do some focused research on that particular story or a common aspect of multiple stories. Spikes are short, time-boxed time frames set aside for the purposes of doing research on a particular aspect of the project. Spikes can occur before regular iterations start or alongside ongoing iterations. 4) XP focuses on practice excellence. The method prescribes a small number of absolutely essential practices and encourages teams to perform those practices as good as they possibly can, almost to the extreme. This is where the name comes from. Not because the practices themselves are necessarily radical rather that teams continuously focus so intently on continuously improving their ability to perform those few practices. With that in mind, the initial recommended XP methodology will focus on the following initial events with selected engineering practices being employed during execution of these events: 1) Product Backlog Refinement 2) Iteration Planning 3) Iteration 4) Iteration Review 5) Iteration Retrospective The core of XP is the interconnected set of software development practices listed below. While it is possible to do these practices in isolation, many teams have found some practices reinforce the others and should be done in conjunction to fully eliminate the risks you often face in software development. The XP Practices have changed a bit since they were initially introduced. The original twelve practices are listed below. If you would like more information about how these practices were originally described, you can visit?http://ronjeffries.com/xprog/what-is-extreme-programming/. Planning Games Small Releases Metaphor Simple Design Testing Refactoring Pair Programming Collective Ownership Continuous Integration 40-hour week On-site Customer Coding Standard Below are the descriptions of the practices as described in the second edition of Extreme Programming Explained - Embrace?Change.?These descriptions include refinements based on experiences of many who practice extreme programming and reflect a more practical set of practices. Sit Together . Since communication is one of the five values of XP, and most people agree that face to face conversation is the best form of communication, have the team sit together in the same space without barriers to communication, such as cubicle walls (note that there are always exception to this rule for remote workers - but these exceptions should be minimized). Whole Team . A cross functional group of people with the necessary roles for a product form a single team. This means people with a need as well as all the people who play some part in satisfying that need all work together on a daily basis to accomplish a specific outcome. Informative Workspace . Set up the team space to facilitate face to face communication. Allow people to have some privacy when they need it, and make the work of the team transparent to each other and to interested parties outside the team. Utilize Information Radiators to actively communicate up-to-date information. Energized Work . Teams are most effective at software development when they are focused and free from distractions.?Energized work means taking steps to make sure the team is able physically and mentally to get into a focused state. This means do not overwork the team (surges should be minimized). Pair Programming . Pair Programming means all production software is developed by two people sitting at the same machine. The idea behind this practice is that two brains and four eyes are better than one brain and two eyes. This practice effectively provides a continuous code review and quicker response to nagging problems that may impede one person doing it on their own.?Teams that have used pair programming have found that it improves quality and does not actually take twice as long because they are able to work through problems quicker, and they stay more focused on the task at hand, thereby creating less code to accomplish the same thing. Stories . Describe what the product should do in terms meaningful to customers and users. These?stories?are intended to be short descriptions of things users want to be able to do with the product that can be used for planning and serve as reminders for more detailed conversations when the team gets around to realizing that particular story. Weekly Cycle . The Weekly Cycle is synonymous to?an?iteration. In the case of XP, the team meets on the first day of the week to reflect on progress to date, the customer picks the stories they would like delivered in that week, and the team determines how they will approach those stories. The goal by the end of the week is to have running tested features that realize the selected stories. The intent behind the time boxed delivery period is to produce something to show to the PMO for feedback. Quarterly Cycle . The Quarterly Cycle is synonymous to a release. The purpose is to keep the detailed work of each weekly cycle in context of the overall project. The Customer lays out the overall plan for the team in terms of features desired within a particular quarter, which provides the team with a view of the forest while they are in the trees, and it also helps the customer to work with other stakeholders who may need some idea of when features will be available. Remember when planning a quarterly cycle the information about any particular story is at a relatively high level, the order of story delivery within a quarterly cycle can change and the stories included in the quarterly cycle may change. Revisiting the plan following each iteration provides an opportunity to keep everyone informed as soon as those changes become apparent to keep surprises to a minimum. Slack . The idea behind slack in XP terms is to add some low priority tasks or stories in your weekly and quarterly cycles that can be dropped if the team gets behind on more important tasks or stories. Put another way, account for the inherent variability in estimates to make sure you leave yourself a good chance of meeting your forecasts. Ten-Minute Build . The goal with the Ten-Minute Build is to automatically build the whole system and run all of the tests in ten minutes. The founders of XP suggested a 10 minute time frame because if a team has a build that takes longer than that, it is less likely to be run on a frequent basis, thus introducing longer time between errors. This practice encourages the team to automate the build and test process to run on a regular basis. This practice supports the practice of Continuous Integration and is supported by the practice of Test First Development. Continuous Integration . Continuous Integration?is a practice where code changes are immediately tested when they are added to a larger code base. The benefit of this practice is the development team can catch and fix integration issues sooner. Most teams dread the code integration step because of the inherent discovery of conflicts and issues that result. Most teams take the approach \"If it hurts, avoid it as long as possible\". Practitioners of XP suggest \"if it hurts, do it more often\". The reasoning behind that approach is that if the development team experiences problems every time they integrate code, the more frequently they integrate, the smaller the changes and the easier to determine the source of the problem. This practice requires is highly dependent on Ten Minute Build and Test First Development. - Test-First Programming. Instead of following the normal path of: Develop code -> write tests -> run tests the practice of Test-First Programming follows the path of?Test Driven Development (TDD): Write failing automated test -> run failing test -> develop code to make test pass -> run test -> repeat As with Continuous Integration, Test-First Programming reduces the feedback cycle for developers to identify and resolve issues, thereby decreasing the number of bugs that get introduced into production. Incremental Design . The practice of?Incremental Design?suggests that the team does a little bit of work up front to understand the proper breadth-wise perspective of the system design, and then dives into the details of a particular aspect of that design when it delivers specific features. This approach reduces the cost of changes and allows the team to make design decisions when necessary based on the most current information available. The practice of Refactoring was originally listed among the 12 core, but was incorporated into the practice of Incremental Design. Refactoring is an excellent practice to use to keep the design simple, and one of the most recommended uses of refactoring is to remove duplication of processes. The biggest impact on instituting this practice is determining the scope of the governments formal design reviews (PDR/CDR) as this practice provides the inputs to these activities during the actual development iteration.","title":"5.4 Extreme Programming (XP)"},{"location":"agileframe/#541-xp-events","text":"Using the Scrum framework for the baseline of events to start the XP process, the following events are summarized here to avoid redundancy with the Scrum section above. Only key differences will be presented here along with a recommendation of which engineering processes could be incorporated into these events (note that many of the engineering practices can be employed in multiple events - below is only one recommendation): a. Product Backlog Refinement - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : Backlog refinement focuses on providing a prioritized set of \"ready\" (see Definition of Ready above) user stories for the development team to accept into the next iteration as well as a \"ready\" reserve set of user stories which are available to the team during the iteration in case additional capacity becomes available. XP differences : Introduction of Spike to do focused research on stories which require more clarity. Applicable Engineering Practices : Quarterly Cycle, Stories, Incremental Design b. **Iteration Planning - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : During this meeting, the Product Owner will review the current list of Ready user stories to ensure that they are still valuable (relevant), as well as reprioritize the Backlog based on existing value as necessary (this facilitates the team's acceptance of work into the iteration as they simply take user stories from the top of the list within their established iteration capacity. XP differences : XP uses iterations which are normally 1-2 weeks long. Applicable Engineering Practices : Weekly Cycle, Slack c. Iteration - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : Iteration execution consists of developers completing their work, testers executing the tests developed prior to the iteration (with the addition of any necessary negative test cases), and BAs and testers reviewing the completed work with the Product Owner in order to receive early feedback or formal closure of the story as meeting the teams \"Definition of Done\". XP differences : Focus of XP is on Test-First Programming and Pair Programming in the execution stage (tests fail at the beginning because the code is not developed - working in pairs, the developers develop the code to pass the tests, when development testing is done - the team testers do a final verification of the initial tests along with any additional negative tests they have written. This process can also be facilitated by implementing a Continuous Integration system to maximize the use of automated testing. Applicable Engineering Practices : Sit Together, Informative Workspace, Energized Work, Pair Programming, Test-First Programming, Continuous Integration, Ten-Minute Build d. Iteration Review - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : The iteration review is a presentation to Stakeholders of the completed increment based on acceptance by the Product Owner. The key to the review is that it is interactive between the Stakeholders and Development Team - the focus being to provide a feedback mechanism to improve the product. XP differences : None. Applicable Engineering Practices : None e. Iteration Retrospective - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : This is where the Product Owners and the Development Team focus on improving the Agile process. Critical to understanding the concept of the retrospective is that learning always occurs during these events whether the iteration was successful or not. XP differences : Focus will not only be on events - it will include the team's selected engineering practices. Applicable Engineering Practices : Whole Team","title":"5.4.1 XP Events"},{"location":"agileframe/#55-part-iii-establish-the-team","text":"The following section provides a general overview of the roles required for effective execution of the different agile methodologies presented in this playbook. Note that these are \"roles\", and, based on the team size, can be implemented by one or more people. Also, the same person can have multiple roles (i.e. a BA and a Scrum Master) though this might not lend to optimal effectiveness.","title":"5.5 Part III - Establish the Team"},{"location":"agileframe/#551-scrum-roles-and-responsibilities","text":"The relationship between development team members and the PMO is critical as understanding between the two will breed the trust necessary for successful execution of an agile process. Product Owners : These are representatives of the PMO with the necessary functional knowledge of the system to be able to collaborate effectively with the development team. One of the key aspects within the government environment is that often, the Product Owner is a collection of individuals representing different perspective (Program Management, Functionality, Engineering, and Cybersecurity are examples of these perspectives). Whether there are 1 or more people representing this position, individual or group must have the ability to make decisions on the requirements to effectively collaborate with the business analyst and the development team. Note that a government Project Manager may fulfill the role of the Product Owner, but they must have the functional knowledge to allow for effective clarification of requirements to the development team. POs are responsible for establishing: a. Requirements / Acceptance Criteria b. Priorities c. Clarification on Requirements for the Development Team d. Approval of \"Done\" for stories. Development Teams : Formed around the specific projects, their purpose is to create quality code that produces value for the government. They have the following components (the size and composition of each team will vary based on the development requirements within a release): a. Scrum Master - They are the Process Advisor to the PO. b. Business Analyst(s) - Responsible for gathering the requirements and translating them into documentation which not only reflects the intent of the PO but are also understandable and executable by the developers within the team. c. Developers (General) - Responsible for - focus is functionality to the users. e. Tester / Quality Assurance (QA) - Responsible for implementation of quality within the team.","title":"5.5.1 Scrum Roles and Responsibilities"},{"location":"agileframe/#552-kanban-roles-and-responsibilities","text":"For Scrum, the Product Owner, Scrum Master and Development team roles must be assigned. In Kanban, the workflow dictates the role requirements. The refinement work can be done prior to entering the workflow (thus requiring a government Product Owner and a team Business Analyst) or the work can be part of the workflow (i.e. a column for refinement). Depending on the policies of the Kanban board workflow, resources will be allocated which can work on the work items within the column. Optimally, there would be specialists as necessary for the different workflow aspects and a couple of generalists could work to ease bottlenecks within the workflow. Additionally, while an Agile Lead familiar with Kanban would maintain the discipline within the system and would also enable faster optimization of the system, a Project Manager could also serve as the team lead for managing the workflow. However, to provide a checklist to begin with in implementing a Kanban team (and to provide flexibility to adapt to or adopt other methodologies), the recommended roles for a Kanban Team startup would be: Agile Lead / Coach - Similar to Scrum Master above but experienced in the execution of Kanban. Product Owner - Same as Product Owner in Scrum. Development Team Members - Determined as necessary to enable the workflow (i.e. business analyst, developers, testers, cybersecurity personnel as necessary based on the different policies of the Kanban Board). Key here is the team is not required to be cross-functional so can be formed around specialists and generalists as necessary.","title":"5.5.2 Kanban Roles and Responsibilities"},{"location":"agileframe/#553-xp-roles-and-responsibilities","text":"Although Extreme Programming specifies engineering practices for the development team to follow, it does not really establish specific roles for the people on the team.?Depending on the reference material on roles in XP, there is either no guidance, or there is a description of how roles typically found in more traditional projects behave on Extreme Programming projects. Here are four most common roles associated with Extreme Programming:? ? The Customer - The Customer role in XP is almost exactly the same as the Product Owner Role in Scrum (same responsibilities stated above). ? ? The Developer - Because XP does not have much need for role definition, everyone on the team (with the exception of the customer and a couple of secondary roles listed below) is labeled a developer. ? The Coach - This role is similar to the Scrum Master role for Scrum. The fundamental difference is that this Agile lead should have experience with XP Practices.? The main value of the coach is that they have gone through it before and can help the team maintain practice discipline and avoid process mistakes.?","title":"5.5.3 XP Roles and Responsibilities"},{"location":"agileframe/#554-roles-and-responsibilities-plays","text":"Since the core roles are essentially the same between the different agile frameworks, there will only be one play for each of the roles common to all frameworks. We will not be discussing developers and testers / quality assurance here as these are roles common to IT development environment. However, when drafting job requirements, the following should be added in the preferred qualifications: Experience in agile development methodologies (Scrum, Kanban, and/or XP) The following agile related roles have plays associated with them: Agile Lead (includes Coach and Scrum Master) Product Owner Business Analyst The contents of each role play is the following: Recommended skill sets Recommended qualifications Recommended certifications General Responsibility Overview Link: Appendix B: Key Personnel Plays","title":"5.5.4 Roles and Responsibilities Plays"},{"location":"agiletrans/","text":"2 Agile Transformation 2.1 Waterfall background (where is the reader coming from) It is challenging to use agile methods in the DoD and AF because the workforce has been trained in and has practiced the traditional waterfall methodology for 50+ years. In many ways, the waterfall methodology was implemented as a way to carefully control risk along the development path and accommodate the often stove piped organizations that supported development efforts. This means that the processes, procedures, acquisition rules, contracting approaches, testing requirements, and so on are all based on the traditional waterfall method. 2.2 Agile Introduction 2.2.1 4 Agile Values The Agile Manifesto describes the overarching beliefs of Agile software development as follows (http://agilemanifesto.org): That is, while there is value in the items on the right, we value the items on the left more. 2.2.2 Benefits of Agile over Traditional Methods Higher quality product (incremental development, continuous integration and automated testing tools allow developers to fix issues quicker when they are fresh in their mind and have fewer secondary effects on code built on top of a bug). Ability to change dynamically to customer/user wants, needs, and/or requirements (value adaptation based on increased transparency, formal feedback events, and high degree of customer collaboration). Ability to balance workloads based on cross-functional teams (while team members often have areas of expertise, agile emphasizes cross-functionality which allows flexibility to surge resources as necessary). Decreases and eventually eliminates the \"throw over the wall\" approach, thereby allowing development, operations, and security to work together iteratively to swarm on a particular issue or bug, should one come up, during releases to lower environments (this decreases the risk of failures in production if all environments are aligned and mirrored-appropriately). Shorter implementation time to usable product (provides quicker return on investment, decreased risk of project failure, faster end-user feedback into the development cycle and increased customer satisfaction). Ability to balance technical debt and new functionality, thereby decreasing technical debt over time. 2.2.3 High-Level Differences 2.2.4 Friction Points with Waterfall Lack of Ownership - The traditional regulatory environment is based on establishment of fixed requirements, letting the external development team develop the product and then inspecting and validating the product to see if it meets the requirements (tennis match of throwing things back and forth between customers and developers with the assumption that both parties understand the requirements in the same way). -- Recommended Mitigation : The key to any agile methodology is collaboration. A dedicated PO (PO) (can be a government employee or contractor on the business level or on the technical team) with decision making authority or immediate reach-back for those decisions is necessary to support the velocity of agile methodologies in being able to provide value in an ever-changing environment. Lack of Collaboration - Traditional design reviews (Preliminary Design Review / Critical Design Review) currently focus on developers presenting their design results at a fixed point of time after exhaustive analysis. -- Recommended Mitigation : What is needed is mini-collaborative design reviews that are done in such a way that integration elements are identified early enough while allowing additional design elements to be refined as close to the work being done as necessary (based on changes happening constantly - re-work will be generated on designs which are done too early and no longer apply to the current situation). Traditional CDRL formats - The traditional government waterfall process intentionally incurs oversight costs as a risk mitigation strategy to ensure that progress was made by contracted agents. In terms of documentations, the government includes in their Contract Deliverable Requirements List (CDRL) a list of documents which are formatted and generated based on a waterfall development framework. The reviewers of these documents are familiar with what the old contents were, and when reviewing delivered documents expect the same waterfall content which are focused on large immovable designs versus less-detailed more agile enabling designs. -- Recommended Mitigation : Contracts need to be modified up front so that only the necessary CDRLs that provide actual value are included and document deliveries are based on an agile timeline (smaller iterative updates versus large updates for significant milestones). (NOTE: While adequate documentation is necessary - the definition of \"adequate\" needs to be modified to provide timely value as determined by the document owners). See recommended CDRL considerations attachment in Section 3.5. Team and team member performance - Agile is based on team dynamics which take time to coalesce (normally that means there is a start-up period where less physical value is produced in the short-term while establishing the foundation to more rapidly produce value in the long-term). This means that by the straight-line value metrics of traditional project management, the project will appear behind at first. The normal strategy here is to provide more \"waterfall\" elements into the equation to try and \"catch-up\" the project which actually impedes / restricts the capabilities of the team to add value. Then in the end if the blame is placed on the agile system versus on the waterfall regulatory restrictions that were placed on it in order to \"catch it up\". -- Recommended Mitigation : Establish trust and transparency (see Communication Management section) as the team learns together how to implement an agile-based process which is understood and accepted by the government and development team. Include more formalized process to \"ramp up\" new government and contractor team members and their understanding of the system and environments. Uncontrolled change/scope creep versus managed change to provide increased value. Waterfall methodologies focus on a formal change management process in order to manage scope creep. Agile is inherently flexible in nature - it welcomes change based on the need to provide value to an ever-changing reality versus provide the value determined during a snapshot in time planning event which could have taken place months or years previously. However, the issue with agile is this inherent flexibility leads to general requirements which allow for the customer to revise in an ad hoc manner which facilitates uncontrolled scope creep (which is okay unless there is a hard deadline in providing the product based on the initial general requirements). -- Recommended Mitigation : Establish a flexible change management system (this will be based on the amount of trust established by the team). It should be flexible enough to rapidly adapt to the changing reality of what the definition of value is within existing requirements while implementing formal steps to manage scope creep (an example is adding a new requirement / feature versus revising an existing one). Earned value reporting - EV is an attempt to monitor the progress of a project by linking contract to cost to contents - these fixed linkages established at the beginning of the contract are normally difficult to change as the work adapts to reality (I.e. we already reported these features and their value to our superiors and it is too hard to revise - so we would rather use the logic we reported instead of revising that logic to fit reality). This often causes a disconnect between the development team and the contracting team as the metrics based on past logic which EV is measuring does not reflect the evolved reality existing in the agile development environment. -- Recommended Mitigation : Link earned value costs to features (can be based on high-level estimates done during feature driven planning. Conduct final feature refinement (work / story breakdown) as close to the actual development effort as possible (as an example for Scrum - conduct feature refinement 2 sprints out to better reflect reality. Base the EV metrics on the percentage of the features decomposed work items / user stories completed. External testing requirements - external test organizations (including those involved with Development Testing and Evaluation (DT&E), Quality Testing and Evaluation (QT&E), etc) often require advanced information for their test events (examples include the release contents, developers test plan, test cases, conditions, etc. Sometimes up to 270 days prior to a test event). Also, these test events are scheduled at fixed milestones versus being iterative in nature. These two items constrain the flexibility of agile to both react to evolving requirements as well as receive timely feedback from testers to incorporate in current development efforts (waiting until the contents of 6 sprints are released before receiving any feedback on their results - see Extreme Programming (XP) Test Driven Development section for possible practices). -- Recommended Mitigation : External testers need to be integrated into the government / development team to derive the evolution of the requirements in preparation for their external testing. An additional step is to establish an iterative external testing cycle which coincides with the agile framework battle rhythm (rather than conduct annual large testing events). Pure agile theorists - unable to allow for regulatory requirements because they are \"not agile\". One of the final and most difficult problems is not the transition from waterfall to agile of the government, but the inability of agile implementers to allow for the regulatory requirements mandated by the government. Just as a waterfall purist will blame agile for all difficulties encountered, the agile purist will blame all issues on the waterfall regulations imposed. -- Recommended Mitigation : What is necessary is to find the correct hybrid framework between the two extremes that allow the process to work in the most efficient manner possible and also allows buy-in from all team members (the transition from a strictly waterfall to an appropriate hybrid framework may also be iterative in nature).","title":"-2 Agile Transformation"},{"location":"agiletrans/#2-agile-transformation","text":"","title":"2 Agile Transformation"},{"location":"agiletrans/#21-waterfall-background-where-is-the-reader-coming-from","text":"It is challenging to use agile methods in the DoD and AF because the workforce has been trained in and has practiced the traditional waterfall methodology for 50+ years. In many ways, the waterfall methodology was implemented as a way to carefully control risk along the development path and accommodate the often stove piped organizations that supported development efforts. This means that the processes, procedures, acquisition rules, contracting approaches, testing requirements, and so on are all based on the traditional waterfall method.","title":"2.1 Waterfall background (where is the reader coming from)"},{"location":"agiletrans/#22-agile-introduction","text":"","title":"2.2 Agile Introduction"},{"location":"agiletrans/#221-4-agile-values","text":"The Agile Manifesto describes the overarching beliefs of Agile software development as follows (http://agilemanifesto.org): That is, while there is value in the items on the right, we value the items on the left more.","title":"2.2.1 4 Agile Values"},{"location":"agiletrans/#222-benefits-of-agile-over-traditional-methods","text":"Higher quality product (incremental development, continuous integration and automated testing tools allow developers to fix issues quicker when they are fresh in their mind and have fewer secondary effects on code built on top of a bug). Ability to change dynamically to customer/user wants, needs, and/or requirements (value adaptation based on increased transparency, formal feedback events, and high degree of customer collaboration). Ability to balance workloads based on cross-functional teams (while team members often have areas of expertise, agile emphasizes cross-functionality which allows flexibility to surge resources as necessary). Decreases and eventually eliminates the \"throw over the wall\" approach, thereby allowing development, operations, and security to work together iteratively to swarm on a particular issue or bug, should one come up, during releases to lower environments (this decreases the risk of failures in production if all environments are aligned and mirrored-appropriately). Shorter implementation time to usable product (provides quicker return on investment, decreased risk of project failure, faster end-user feedback into the development cycle and increased customer satisfaction). Ability to balance technical debt and new functionality, thereby decreasing technical debt over time.","title":"2.2.2 Benefits of Agile over Traditional Methods"},{"location":"agiletrans/#223-high-level-differences","text":"","title":"2.2.3 High-Level Differences"},{"location":"agiletrans/#224-friction-points-with-waterfall","text":"Lack of Ownership - The traditional regulatory environment is based on establishment of fixed requirements, letting the external development team develop the product and then inspecting and validating the product to see if it meets the requirements (tennis match of throwing things back and forth between customers and developers with the assumption that both parties understand the requirements in the same way). -- Recommended Mitigation : The key to any agile methodology is collaboration. A dedicated PO (PO) (can be a government employee or contractor on the business level or on the technical team) with decision making authority or immediate reach-back for those decisions is necessary to support the velocity of agile methodologies in being able to provide value in an ever-changing environment. Lack of Collaboration - Traditional design reviews (Preliminary Design Review / Critical Design Review) currently focus on developers presenting their design results at a fixed point of time after exhaustive analysis. -- Recommended Mitigation : What is needed is mini-collaborative design reviews that are done in such a way that integration elements are identified early enough while allowing additional design elements to be refined as close to the work being done as necessary (based on changes happening constantly - re-work will be generated on designs which are done too early and no longer apply to the current situation). Traditional CDRL formats - The traditional government waterfall process intentionally incurs oversight costs as a risk mitigation strategy to ensure that progress was made by contracted agents. In terms of documentations, the government includes in their Contract Deliverable Requirements List (CDRL) a list of documents which are formatted and generated based on a waterfall development framework. The reviewers of these documents are familiar with what the old contents were, and when reviewing delivered documents expect the same waterfall content which are focused on large immovable designs versus less-detailed more agile enabling designs. -- Recommended Mitigation : Contracts need to be modified up front so that only the necessary CDRLs that provide actual value are included and document deliveries are based on an agile timeline (smaller iterative updates versus large updates for significant milestones). (NOTE: While adequate documentation is necessary - the definition of \"adequate\" needs to be modified to provide timely value as determined by the document owners). See recommended CDRL considerations attachment in Section 3.5. Team and team member performance - Agile is based on team dynamics which take time to coalesce (normally that means there is a start-up period where less physical value is produced in the short-term while establishing the foundation to more rapidly produce value in the long-term). This means that by the straight-line value metrics of traditional project management, the project will appear behind at first. The normal strategy here is to provide more \"waterfall\" elements into the equation to try and \"catch-up\" the project which actually impedes / restricts the capabilities of the team to add value. Then in the end if the blame is placed on the agile system versus on the waterfall regulatory restrictions that were placed on it in order to \"catch it up\". -- Recommended Mitigation : Establish trust and transparency (see Communication Management section) as the team learns together how to implement an agile-based process which is understood and accepted by the government and development team. Include more formalized process to \"ramp up\" new government and contractor team members and their understanding of the system and environments. Uncontrolled change/scope creep versus managed change to provide increased value. Waterfall methodologies focus on a formal change management process in order to manage scope creep. Agile is inherently flexible in nature - it welcomes change based on the need to provide value to an ever-changing reality versus provide the value determined during a snapshot in time planning event which could have taken place months or years previously. However, the issue with agile is this inherent flexibility leads to general requirements which allow for the customer to revise in an ad hoc manner which facilitates uncontrolled scope creep (which is okay unless there is a hard deadline in providing the product based on the initial general requirements). -- Recommended Mitigation : Establish a flexible change management system (this will be based on the amount of trust established by the team). It should be flexible enough to rapidly adapt to the changing reality of what the definition of value is within existing requirements while implementing formal steps to manage scope creep (an example is adding a new requirement / feature versus revising an existing one). Earned value reporting - EV is an attempt to monitor the progress of a project by linking contract to cost to contents - these fixed linkages established at the beginning of the contract are normally difficult to change as the work adapts to reality (I.e. we already reported these features and their value to our superiors and it is too hard to revise - so we would rather use the logic we reported instead of revising that logic to fit reality). This often causes a disconnect between the development team and the contracting team as the metrics based on past logic which EV is measuring does not reflect the evolved reality existing in the agile development environment. -- Recommended Mitigation : Link earned value costs to features (can be based on high-level estimates done during feature driven planning. Conduct final feature refinement (work / story breakdown) as close to the actual development effort as possible (as an example for Scrum - conduct feature refinement 2 sprints out to better reflect reality. Base the EV metrics on the percentage of the features decomposed work items / user stories completed. External testing requirements - external test organizations (including those involved with Development Testing and Evaluation (DT&E), Quality Testing and Evaluation (QT&E), etc) often require advanced information for their test events (examples include the release contents, developers test plan, test cases, conditions, etc. Sometimes up to 270 days prior to a test event). Also, these test events are scheduled at fixed milestones versus being iterative in nature. These two items constrain the flexibility of agile to both react to evolving requirements as well as receive timely feedback from testers to incorporate in current development efforts (waiting until the contents of 6 sprints are released before receiving any feedback on their results - see Extreme Programming (XP) Test Driven Development section for possible practices). -- Recommended Mitigation : External testers need to be integrated into the government / development team to derive the evolution of the requirements in preparation for their external testing. An additional step is to establish an iterative external testing cycle which coincides with the agile framework battle rhythm (rather than conduct annual large testing events). Pure agile theorists - unable to allow for regulatory requirements because they are \"not agile\". One of the final and most difficult problems is not the transition from waterfall to agile of the government, but the inability of agile implementers to allow for the regulatory requirements mandated by the government. Just as a waterfall purist will blame agile for all difficulties encountered, the agile purist will blame all issues on the waterfall regulations imposed. -- Recommended Mitigation : What is necessary is to find the correct hybrid framework between the two extremes that allow the process to work in the most efficient manner possible and also allows buy-in from all team members (the transition from a strictly waterfall to an appropriate hybrid framework may also be iterative in nature).","title":"2.2.4 Friction Points with Waterfall"},{"location":"applyag/","text":"3 Applying Agile Methods and Mindset Within The Air Force At the inception of a software-based project, the detailed software requirements may be unknown or unknowable, and even if the requirements are known, they usually experience significant changes as the development progresses. To address these evolving requirements issues, agile or iterative development promotes enhanced collaboration between program managers, requirements analysts, testers, the end-user community, and, of course, the software developers. This approach develops software iteratively in short cycles (called \"sprints,\" \"spirals,\" or \"spins\"), and involves frequent testing, user feedback, rapid deliveries, and adaptation to changing requirements. While traditional Waterfall software development was approached via rigorous preplanning to fully specify requirements before building an entire computer program application, Agile software development breaks the project down to provide iterative improvements which also adapt to the evolving environment. This allows government Program Managers to incorporate changed or new requirements in accordance with user needs, thus promoting modular IT contracting. This section focuses on how to incorporate Agile practices into the Air Force acquisition organizations, contracting and the PMO organizations to pro-actively enable an Agile development framework rather than have the framework adapt to a pre-defined waterfall based contracting approach. As this section is predominantly focused on the mindset shift, it will focus on presenting considerations versus prescribing a specific methodology. 3.1 Agile Contracting Before jumping into the Agile development, PMOs should take time to consider how Agile can benefit their program, what the issues will be, and if perhaps a hybrid approach (combination of Waterfall and Agile) is the best approach. Some of the concepts that need to be considered when embarking on the use of Agile are discussed below. The discussion assumes the government will be contracting with a firm to actually do the development. Since the contractor will be creating the Agile organization structure, it is important the government understands the contractors' Agile organization and how the government interacts within that structure. The better the understanding, the less likely there will be inadvertent roadblocks or obstacles created to impede the progress of the Agile team(s). If the government is doing the development internally, some of the actions may differ and would be accomplished by the government. The following establish some of the key variables which must be considered in the context of enabling an Agile development framework. Acquisition life cycle Team environment End-user access Training and coaching Oversight including milestone reviews, documentation, evaluation (metrics) Rewards and incentives Culture These concepts were actual issues that programs deal with during their use of Agile methods. The concepts discussed here overlap and are intertwined. In many cases, the concepts are mutually reinforcing. 3.1.1 Acquisition Life Cycle The acquisition life cycle consists of multiple phases: Materiel Solution Analysis, Technology Development, Engineering and Manufacturing Development, Production & Deployment and Operations & Support. Each of these phases presents unique challenges and opportunities. Some phases lend themselves to the use of Agile better than others. The PMO should determine how to best employ Agile in their program depending on their specific situation. The following paragraphs propose questions to ask and identify issues to consider in building an Agile program. If the PMO is doing a Request for Proposal (RFP), no matter which phase, ensure that the RFP contains language that allows the use of Agile. In many instances, the traditional RFP language makes it difficult, if not impossible, to propose an Agile-based solution. One consideration is the types of reviews and documents required. If the PMO wants to employ Agile, be prepared to allow for Agile style document development, i.e., incremental development of documents and data for reviews that result from the individual iterations and/or releases. This might not seem much different from what the traditional waterfall methods provide but consider the level of detail may be sparser using Agile in the earlier versions of the documents. Even final documents might not contain the amount of detail provided in traditional documents. The key here is not the volume, but the content. A necessary and sufficient criterion is that all important information required for operation and maintenance of the system are supplied. 3.1.2 Team Environment Organization structure needs to be established to support an Agile implementation. The context of a program and its inherent organizational structure are related. Many systems contain software and could be considered software intensive but the software is only a small part of the overall system and certainly not the end item being procured. For information systems this is usually not an issue. Due to the size and complexity of most Air force programs, multiple agile iteration teams will be needed. The number is dependent upon the program and in some instances the locations of the contractor team. The larger the number of teams, the more complicated the communications and the greater the need for more users to be involved. In an ideal situation, each agile iteration team would have access to their own dedicated Product Owner. However, that is not practical in the DoD environment so alternatives need to be employed. PMO can consider the use of Product Owner proxies, rotating personnel every x weeks (x usually is two-four weeks), or perhaps a separate - team of subject matter experts (SMEs) accessible by the agile iteration teams as needed. The structure of the overall program team-especially the contractor team-is dependent upon which Agile method is chosen. Agile Scrum, Kanban and XP are just three examples of management practices within Agile methods. Typically, the contractor determines the flavor of Agile. However, the government PMO team needs to be responsive and supportive of that method. Otherwise, using Agile will have less than optimal results. The Agile team also must exhibit behavior reflecting the approach. Seven Extreme Programming (XP) engineering practices have been observed to scale up to enterprise-level Agile development projects and will serve as a foundation for the discussion of Agile contracting. The Define/Build/Test Component. Three basic workflows are combined in the component team: define, build, and test, operating cooperatively within a pre-defined period, known as a time box. The juxtaposition of these skill sets into one team tends to run counter to some conventional methods employed in DoD programs, where these players are often separated by intent. Two-Level Planning. Two-level planning is portrayed as providing both guidance of how software is to be inserted into the operational environment as well as allowing some flexibility to accommodate what is learned during development: The top level of the planning cycle is termed release level planning. This cycle of planning defines series of releases that broadly define capability to be contained in each release. This could be done at the feature set level. The second level of the planning cycle is termed iteration or flow level planning, where work is made ready for development within either a time-boxed iteration or rhythmic workflow approach. Mastering the flow / Iteration. The ability of a team to reliably execute a process flow (Kanban) or sequence of iterations (Scrum / XP) may well be the key behavior that distinguishes a team capable of exploiting Agile techniques in a large organization. If this capability is not present, the likelihood of success is minimal at best. The development iteration or workflow consists of the following key activities: creation of complete, tested, working code implementing a set of features and integration of the developed code into the working baseline. The result is potentially releasable to the user. Producing Smaller and More Frequent Releases. One goal of an agile development framework is the desire for more frequent feedback from the customer and/or stakeholders to avoid large-scale course corrections. The shorter duration of iterations or workflow lead time will help to maintain more or less continuous feedback from the customer. Concurrent Testing. Concurrent testing practices are based upon thorough testing of code both during development and during integration. The goal is that all code is tested. One primary methodology for this is the application of a Test-Driven-Development Approach where the unit tests for software are created prior to the actual code development. DevOps Continuous Integration (CI)/Continuous Delivery (CD) Pipelines. DevOps CI/CD pipelines may well be the most useful and controversial practice advocated in the Agile community. The DevOps CI/CD model diverges from the usual V-shaped model advocated by traditional systems engineering practice employed in DoD programs. In the V-shaped model, requirements synthesis, allocation, and development are carried out in a top-down fashion. This is followed by a bottom-up sequence of integration and verification activities, leading to a product ready for production. DevOps CI/CD pipeline processes are contingent upon the ability to concurrently execute two crucial activities: (1) collect incremental changes from multiple developers on a regular basis, ideally on a daily basis on code check-in, and (2) perform the nightly build discipline, where all changes are brought together in an incremental software baseline, which is in turn compiled and tested with the available automated unit, security, functional and regression test tools. Regular Reflection and Adaptation. Reflection and adaptation (called the Retrospective in Scrum) is the Agile version of continuous process improvement that is highlighted in other quality practices such as CMMI-DEV processes. In keeping with the bottom-up discipline of Agile approaches, this introspection is driven down to the individual team level. 3.1.3 Contracting Consideration Checklist Procure the repeatable process for the delivery of functional products Contractual Requirements should be the scope, period of performance, and price. The technical execution of the project should be at the discretion of the Product Owner Enhancement and fixes should be owned by the same team Contract Types: Fixed Price per iteration is good for the procurement of the process for an entire team but the current DoD acquisition process does not support short-term contract changes Time and Materials is ideal for the procurement of time of required skill sets but the risk is entirely on the government A preferred type which enables agile development is a \"Rent the Factory\" type contract: Establish contract to resource (\"rent\") a team for a specified time period from a contractor Control change through PMO management of the Product Backlog Implement within contract incremental options for extension to decrease government risk (off-ramp for lack of performance) Provide reward incentives for excellence in performance 3.2 Agile Organization, Roles, and Responsibilities - Stakeholder Level One addition to the typical traditional Air Force PMO organization is an Agile Coach. As described in the previous training and coaching section, the Agile Coach is someone who can provide real-time answers for the immediate Agile issue. Another addition to the typical PMO staff is an end-user representative, the PO, who is empowered to work with the contractors' agile development team and make decisions that are binding for the development. Given the nature of government contracting, care must be taken to ensure that the PO user representative has the legal authority to direct the contractor. We can envision a situation where constructive change could become an issue. Another addition to the PMO is a DevSecOps Lead who works with the with contractor engineering support teams that may be divided into two segments: Continuous Integration Team and Continuous Delivery/Deployment Team to implement configuration management, version control, automated build, automated security testing, automated functional testing and regression testing. The government needs skilled Agile personnel to review the documentation and understand how the Agile software development approach works. Many traditional PMO teams do not have software representatives experienced with modern software development approaches. That could be more problematic in an Agile environment, where any shortfalls quickly become more visible. Another challenge is keeping high-performing Agile teams together long enough for them to achieve peak performance. This is a challenge because developers can change at the end of a contractual period of performance. The continuity of an Agile team enhances the tacit knowledge of the program and this improves overall performance. One recommendation might be to look at is putting key Agile technical leads into the PMO under a separate contract vehicle or hire them to work for the government PMO organization itself. 3.2.1 Stakeholder Consideration Checklist: Empower the Product Owner to make technical decisions Provide regular feedback to stakeholders demonstrating progress Maintain a short feedback loop with users Align with external organizations Testing Configuration 3.3 Project Management This section describes how project management practices need to be adjusted in support of Agile projects by first identifying common Agile practices and then describing how these management practices work in terms of scope, schedule and cost baselines for the project work. 3.3.1Planning An integrated project management plan (PMP) is developed for the Agile project to define the basis of all project work and how the work will be performed. It describes how the project will be executed, monitored, control and closed. From the Agile perspective, the performance measurement baseline is an integrated scope, schedule, and cost baseline for the software release project work maintained in the Product Backlog against which project execution metrics are used to measure and manage performance. The PMP describes the series of phases (themes, initiatives, and epics) the project passes through from initiation to closure. The PMP also describes the Agile development management approach; i.e. Agile iteration-based (Scrum/XP), flow-based (Kanban) or a hybrid model. Figure xx describes the hierarchal structure of the Agile project work effort. Notice that this view of the project work hierarchy is similar to the WBS in predictive (Waterfall) projects. One additional consideration is that the above hierarchy promotes a multi-team or \"scaled\" agile approach. While methodologies exist to support the scaling of agile (i.e. Scaled Agile Framework (SAFe), Scrum of Scrums, Disciplined Agile Delivery (DAD), etc), these will not be presented in this playbook but are a topic for further elaboration as an organization's agile process matures. 3.3.2 Scope In Agile projects the requirements are defined by the organization's stakeholders and Product Owner with support from the Agile Team in the form of Epics, Features, user stories or PBIs that are maintained in the Product Backlog. Therefore, the project scope for an Agile project begins with the organization's governance process which commonly consists of a Configuration Control Board (CCB) that produces a high-level product backlog for a release consisting of a list of approved requirements defined in the form of epics, features and sometimes high-level PBIs. The product owner then works with the Agile Inception Team to prioritize these items (epics, features, PBIs if applicable). The PMO can initially use the MoSCoW (Must Have, Should Have, Could Have, Won't Have this time) method to prioritize requirements. The initial Product Backlog items are divided into 'must haves', 'should haves', 'could haves' & 'won't have': The items with highest business value, and lowest effort involved, are often the items that qualify the 'must haves' criteria. The items that are of higher priority, and will need some effort to be delivered, are deemed, as 'should haves'. All the backlog items that might be desirable in terms of scope, but are of lower business value are segmented, as 'could haves'. The items that are agreed upon to be moved to later releases, are filtered out as 'won't haves' for now. 3.3.3Forecasting Schedule and Cost Once the user stories and PBIs in the product backlog are defined, prioritized and the MVP determined, Agile estimating techniques can be applied to estimate the effort for each feature, sum up the effort for all the features in a project as well as determine which features would be part of which release. The PMO and development team are then able to forecast a schedule and cost for the release project. There are several gross-level estimation techniques used by teams using agile approaches such as Scrum, Kanban, and eXtreme Programming which include T-shirt Sizes (for Features), and Affinity Mapping. T-Shirt Sizes . This estimation technique can be applied when providing a quick and rough estimation to a project feature. Here, the features are estimated in T-shirt sizes, ranging from XS to XL, which would be later converted to numbers, as per requirements. In this type of estimation, the estimators assign a size to each of the features. Points are assigned to the each of the T-Shirt Sizes using the Fibonacci-like format: 0, 1, 2, 3, 5, 8, 13, 20, 40, 100. These points are summed up and based on a rough estimate of how many feature points can get done within a time period by a normal agile team (note, the more detailed the refinement, the better the estimates). 3.3.4 Cost The cost for the release is estimated by using the team's iteration average cost and multiplying it by the number of iterations estimated to complete the backlog. For example, the following formula to determine budgeted cost can demonstrate this estimation for the above example: (Team monthly cost (example: $15,000.00 per month) multiplied by the number of months/iterations (example: 5 months) = $75,000.00 + other expenses = forecast budgeted cost. The above example is utterly simplistic and does not take into account the following factors: The Inception phase at the beginning that is required to develop the product backlog, estimate the size in story points and develop an architecture vision; Changes in scope during the Construction iterations caused by adding new features driven by urgent business needs or Cybersecurity issues; A Transition phase at the end of development for Government Acceptance Tests and Security Tests required to achieve customer acceptance and Authority to Operate (AtO) for deployment; Many other variables such as Cloud Migration and implementation of a DevSecOps reference model. 3.3.5 Project Management Consideration Checklist The project manager removes ensures funding, organizes stakeholder interactions and keeps the team from being distracted Work in increments. Buy, build, and fail small. Make proceed and pivot decisions regularly. Learn from mistakes but don't punish the people Leverage the efficiency of commercial contracting methods. If and when possible, use services and tools sold by private sector vendors. Use living roadmaps not fixed Integrated master schedules 3.4 Agile CDRLs and delivery 3.4.1 Overview One of the four agile manifesto values is that \"working products are valued over comprehensive documentation\". Many times this is viewed by the agile purist as a justification for not doing documentation. However, even in agile, there is a value for doing documentation. Documentation exists to support the development teams work in creating the product and supporting the product after release. Prior to looking deeper at agile content and delivery recommendations for CDRLs, here are some general considerations to keep in mind when determining the format, content, and delivery schedule for CDRLs within an agile framework - \"Just in time\" - a. Document late (based on design completion) - Consolidate deliverable design documentation as late as possible (though can be iteratively updated) - better to have the stable concepts versus speculative ideas which are constantly changing as part of the agile framework and would require constant document revisions and submissions. b. Document continuously (based on iteration) - iteratively update development related documentation (i.e. user guides) in parallel with development efforts to not lose critical ideas. A key concept here is the idea of a living document, which is discussed below. \"Just sufficient\" - Sufficiency is defined by the document owner (provide the necessary useful documentation elements). Additional thoughts: a. Provide the fewest CDRLs possible with the least amount of overlap (I.e. considering combining the Interface Requirements Specification (IRS) with the Interface Design Description IDD). b. Better communication means less documentation (collaboration is key to agile - often a conversation between engineers can eliminate the need for a staffing document). c. Working software does not eliminate the need for documentation - the software delivered still needs to be improved, operated and maintained in the future - documentation's value is transferring product knowledge gained in development to users, operators and maintainers or to new development personnel when contracts change. \"NOT Just Because\" - Treat documentation as any other requirement that needs to be justified by the government document owner (since resources will need to be allocated to produce it). Documentation work efforts can then be prioritized within the product backlog based on the value it provides. 3.4.2 Agile CDRL Content Considerations Agile is built to be fast and flexible, and the contents of the CDRLs must be able to keep up with this development framework. CDRLs should not be an after-thought - they must be incorporated into the process. In other words, we don't want big CDRLs that are out of date by the time they are published. We need documents which can be frequently updated based on the ongoing iterative development efforts. Updates for CDRLs should be provided incrementally by the team when it is fresh in their mind, versus producing documentation at the end of the release when much of the valuable information has already been forgotten. In the military, this is a mindset switch. 3.4.3 Agile CDRL Delivery Consideration In order to maintain the current value of CDRLs, these documents should be flexible enough to keep up with an iterative update approach (versus long periods of time between updates). In that case, the best methodology is to establish a system to enable CDRLs as \"living documents\". This can best be enabled by re-thinking the methodology of delivery for CDRLs. By considering alternate digital delivery methodologies, CDRLs can be more quickly updated and maintain their relevance throughout the agile development process. One final note to this section is while Sharepoint or a shared drive may fulfill the CDRL requirements above, a further shift from the traditional mindset is to provide appropriate dashboards or reports within an existing system to provide the CDRL information requirements. An example of this is the Test Descriptions / Scripts. Executable test cases are normally already stored in a digital format within the test management software. 3.4.4 CDRL Recommended Modifications Attachment The following link is to an attachment which provides a more detailed list of CDRLs normally associated with a software development project. The list contains the associated DID, waterfall description, agile recommended modifications, and normal delivery timelines (i.e. are the documents delivered one time for the project, at specified design reviews, with a delivery, or on an as needed basis. 3.5 Measuring Agile Delivery, KPIs, and Metrics - Status Reporting In Agile, the system always runs, thus Agile metrics are empirical and business value-based measurements instead of predictive measurements such as the performance measurement baseline and earned value that are used in traditional Waterfall. Agile metrics measure what the Agile Team delivers, not what the team predicts it will deliver. Project teams use this data for improved schedule and cost forecasts as well as for surfacing problems and issues that the Agile Team can diagnose and address. The metrics described below address Team Metrics, Program Metrics and Portfolio Metrics. These metrics were derived from the Project Management Institute, Inc. Agile Practice Guide, SAFe Metrics, DAD, and Atlassian web sites. 3.5.1 Team Iteration Metrics The Agile team metrics discussed below focus on the delivery of software. Whether the project team is a Scrum or Kanban team, each of these agile metrics will help the team better understand their development process, making releasing software easier. Scrum Metrics Sprint burndown. Scrum teams organize development into time-boxed sprint iterations. At the outset of the sprint, the team forecasts how many story points they can finish during a sprint. A sprint burndown report (Figure xx) then tracks the completion of work during the sprint. The x-axis represents time, and the y-axis refers to the amount of story points left to complete. The goal is to have all the forecasted work completed by the end of the sprint. A team that consistently meets its forecast is a compelling advertisement for Agile in their organization, however, it may be too good to be true if the team is inflating the numbers by declaring an item complete before it really is. In the long run cheating hampers learning and improvement. There are several anti-patterns to watch for in team performance: 1. The team finishes early sprint after sprint because they aren't committing to enough work in the sprint backlog. 2. The team misses their forecast sprint after sprint because they're committing to too much work. 3. The burndown line makes steep drops rather than a more gradual burndown because the work hasn't been broken down into granular user stories or PBIs. 4. The product owner adds PBIs or changes the scope mid-sprint. Velocity Velocity is the average amount of work a Scrum team completes during a sprint, measured in story points and we used it in the example from the prior section to forecast a release schedule. The product owner can use velocity to predict how quickly a team can work through the product backlog, since the velocity chart report tracks the forecasted and completed work over several iteration-the more iterations, the more accurate the forecast. Each team's velocity is unique. If team A has a velocity of 25 story points and team B has a velocity of 50 story points, it doesn't mean that team B has higher throughput. Because each team's story point estimation technique is unique, their sprint velocity will be as well. Organizations should resist the temptation to compare velocity across teams. Instead, Program Management should measure the level of effort and output of work based on each team's unique interpretation of story points. Kanban Metrics Team Kanban Board Flow-based Agile Teams using Kanban methods and Kanban Boards need to use different measurements like work in progress, lead time for delivery of a feature to customer, cycle time for completion of a task on the Kanban Board, and response time - the amount of time the item waits until work begins. Figure xx shows an example of an Agile team's initial Kanban board, which captures their current workflow states: analyze, review, build, and integrate and test. After defining the initial process and Work in Process (WIP) limits and executing for a while, the Kanban team's bottlenecks should surface. If this is the case, the Kanban Team refines the workflow process step where the bottleneck occurred or reduces some WIP limits until it becomes evident that a workflow state is 'starving' or is too full. In this manner the Kanban Team continually adjusts the process workflows to optimize their flow. For example, changing WIP limits and merging, splitting, or redefining workflow states. Cumulative Flow Diagram The cumulative flow diagram is a key resource for Kanban teams, helping them ensure the flow of work across the team is consistent. With number of issues on the Y axis, time on the X axis, and colors to indicate the various workflow states, it visually points out shortages and bottlenecks and works in conjunction with Work in Process (WIP) limits. The cumulative flow diagram should look smooth(ish) from left to right. Bubbles or gaps in any one color indicate shortages and bottlenecks, so when the Agile Team sees one, they should look for ways to smooth out color bands across the chart. Anti-patterns to look for are: - Blocking issues create large backups in some parts of the process and starvation in others. - Unchecked backlog growth over time. This results from product owners not closing issues that are obsolete or simply too low in priority to ever be pulled in. Measuring cycle time is an efficient and flexible way to improve a team's processes because the results of changes are discernable almost immediately, allowing them to make any further adjustments right away. The end goal is to have a consistent and short cycle time, regardless of the type of work (new feature, technical debt, etc. Here are two areas to watch out for: - Increasing cycle time - Increasing cycle time is a negative trend which should be addressed in the Operations Review. One exception: if the team's definition of done has expanded, cycle time will probably expand too. - Erratic cycle time - The goal is to have consistent cycle time for work items which have similar story point values. Filter the control chart for each story point value to check for consistency. If cycle time is erratic on small and large story point values, spend time in the Operations Review discussing how to improve estimates.","title":"-3 Applying Agile Methods and Mindset Within The Air Force"},{"location":"applyag/#3-applying-agile-methods-and-mindset-within-the-air-force","text":"At the inception of a software-based project, the detailed software requirements may be unknown or unknowable, and even if the requirements are known, they usually experience significant changes as the development progresses. To address these evolving requirements issues, agile or iterative development promotes enhanced collaboration between program managers, requirements analysts, testers, the end-user community, and, of course, the software developers. This approach develops software iteratively in short cycles (called \"sprints,\" \"spirals,\" or \"spins\"), and involves frequent testing, user feedback, rapid deliveries, and adaptation to changing requirements. While traditional Waterfall software development was approached via rigorous preplanning to fully specify requirements before building an entire computer program application, Agile software development breaks the project down to provide iterative improvements which also adapt to the evolving environment. This allows government Program Managers to incorporate changed or new requirements in accordance with user needs, thus promoting modular IT contracting. This section focuses on how to incorporate Agile practices into the Air Force acquisition organizations, contracting and the PMO organizations to pro-actively enable an Agile development framework rather than have the framework adapt to a pre-defined waterfall based contracting approach. As this section is predominantly focused on the mindset shift, it will focus on presenting considerations versus prescribing a specific methodology.","title":"3 Applying Agile Methods and Mindset Within The Air Force"},{"location":"applyag/#31-agile-contracting","text":"Before jumping into the Agile development, PMOs should take time to consider how Agile can benefit their program, what the issues will be, and if perhaps a hybrid approach (combination of Waterfall and Agile) is the best approach. Some of the concepts that need to be considered when embarking on the use of Agile are discussed below. The discussion assumes the government will be contracting with a firm to actually do the development. Since the contractor will be creating the Agile organization structure, it is important the government understands the contractors' Agile organization and how the government interacts within that structure. The better the understanding, the less likely there will be inadvertent roadblocks or obstacles created to impede the progress of the Agile team(s). If the government is doing the development internally, some of the actions may differ and would be accomplished by the government. The following establish some of the key variables which must be considered in the context of enabling an Agile development framework. Acquisition life cycle Team environment End-user access Training and coaching Oversight including milestone reviews, documentation, evaluation (metrics) Rewards and incentives Culture These concepts were actual issues that programs deal with during their use of Agile methods. The concepts discussed here overlap and are intertwined. In many cases, the concepts are mutually reinforcing.","title":"3.1 Agile Contracting"},{"location":"applyag/#311-acquisition-life-cycle","text":"The acquisition life cycle consists of multiple phases: Materiel Solution Analysis, Technology Development, Engineering and Manufacturing Development, Production & Deployment and Operations & Support. Each of these phases presents unique challenges and opportunities. Some phases lend themselves to the use of Agile better than others. The PMO should determine how to best employ Agile in their program depending on their specific situation. The following paragraphs propose questions to ask and identify issues to consider in building an Agile program. If the PMO is doing a Request for Proposal (RFP), no matter which phase, ensure that the RFP contains language that allows the use of Agile. In many instances, the traditional RFP language makes it difficult, if not impossible, to propose an Agile-based solution. One consideration is the types of reviews and documents required. If the PMO wants to employ Agile, be prepared to allow for Agile style document development, i.e., incremental development of documents and data for reviews that result from the individual iterations and/or releases. This might not seem much different from what the traditional waterfall methods provide but consider the level of detail may be sparser using Agile in the earlier versions of the documents. Even final documents might not contain the amount of detail provided in traditional documents. The key here is not the volume, but the content. A necessary and sufficient criterion is that all important information required for operation and maintenance of the system are supplied.","title":"3.1.1 Acquisition Life Cycle"},{"location":"applyag/#312-team-environment","text":"Organization structure needs to be established to support an Agile implementation. The context of a program and its inherent organizational structure are related. Many systems contain software and could be considered software intensive but the software is only a small part of the overall system and certainly not the end item being procured. For information systems this is usually not an issue. Due to the size and complexity of most Air force programs, multiple agile iteration teams will be needed. The number is dependent upon the program and in some instances the locations of the contractor team. The larger the number of teams, the more complicated the communications and the greater the need for more users to be involved. In an ideal situation, each agile iteration team would have access to their own dedicated Product Owner. However, that is not practical in the DoD environment so alternatives need to be employed. PMO can consider the use of Product Owner proxies, rotating personnel every x weeks (x usually is two-four weeks), or perhaps a separate - team of subject matter experts (SMEs) accessible by the agile iteration teams as needed. The structure of the overall program team-especially the contractor team-is dependent upon which Agile method is chosen. Agile Scrum, Kanban and XP are just three examples of management practices within Agile methods. Typically, the contractor determines the flavor of Agile. However, the government PMO team needs to be responsive and supportive of that method. Otherwise, using Agile will have less than optimal results. The Agile team also must exhibit behavior reflecting the approach. Seven Extreme Programming (XP) engineering practices have been observed to scale up to enterprise-level Agile development projects and will serve as a foundation for the discussion of Agile contracting. The Define/Build/Test Component. Three basic workflows are combined in the component team: define, build, and test, operating cooperatively within a pre-defined period, known as a time box. The juxtaposition of these skill sets into one team tends to run counter to some conventional methods employed in DoD programs, where these players are often separated by intent. Two-Level Planning. Two-level planning is portrayed as providing both guidance of how software is to be inserted into the operational environment as well as allowing some flexibility to accommodate what is learned during development: The top level of the planning cycle is termed release level planning. This cycle of planning defines series of releases that broadly define capability to be contained in each release. This could be done at the feature set level. The second level of the planning cycle is termed iteration or flow level planning, where work is made ready for development within either a time-boxed iteration or rhythmic workflow approach. Mastering the flow / Iteration. The ability of a team to reliably execute a process flow (Kanban) or sequence of iterations (Scrum / XP) may well be the key behavior that distinguishes a team capable of exploiting Agile techniques in a large organization. If this capability is not present, the likelihood of success is minimal at best. The development iteration or workflow consists of the following key activities: creation of complete, tested, working code implementing a set of features and integration of the developed code into the working baseline. The result is potentially releasable to the user. Producing Smaller and More Frequent Releases. One goal of an agile development framework is the desire for more frequent feedback from the customer and/or stakeholders to avoid large-scale course corrections. The shorter duration of iterations or workflow lead time will help to maintain more or less continuous feedback from the customer. Concurrent Testing. Concurrent testing practices are based upon thorough testing of code both during development and during integration. The goal is that all code is tested. One primary methodology for this is the application of a Test-Driven-Development Approach where the unit tests for software are created prior to the actual code development. DevOps Continuous Integration (CI)/Continuous Delivery (CD) Pipelines. DevOps CI/CD pipelines may well be the most useful and controversial practice advocated in the Agile community. The DevOps CI/CD model diverges from the usual V-shaped model advocated by traditional systems engineering practice employed in DoD programs. In the V-shaped model, requirements synthesis, allocation, and development are carried out in a top-down fashion. This is followed by a bottom-up sequence of integration and verification activities, leading to a product ready for production. DevOps CI/CD pipeline processes are contingent upon the ability to concurrently execute two crucial activities: (1) collect incremental changes from multiple developers on a regular basis, ideally on a daily basis on code check-in, and (2) perform the nightly build discipline, where all changes are brought together in an incremental software baseline, which is in turn compiled and tested with the available automated unit, security, functional and regression test tools. Regular Reflection and Adaptation. Reflection and adaptation (called the Retrospective in Scrum) is the Agile version of continuous process improvement that is highlighted in other quality practices such as CMMI-DEV processes. In keeping with the bottom-up discipline of Agile approaches, this introspection is driven down to the individual team level.","title":"3.1.2 Team Environment"},{"location":"applyag/#313-contracting-consideration-checklist","text":"Procure the repeatable process for the delivery of functional products Contractual Requirements should be the scope, period of performance, and price. The technical execution of the project should be at the discretion of the Product Owner Enhancement and fixes should be owned by the same team Contract Types: Fixed Price per iteration is good for the procurement of the process for an entire team but the current DoD acquisition process does not support short-term contract changes Time and Materials is ideal for the procurement of time of required skill sets but the risk is entirely on the government A preferred type which enables agile development is a \"Rent the Factory\" type contract: Establish contract to resource (\"rent\") a team for a specified time period from a contractor Control change through PMO management of the Product Backlog Implement within contract incremental options for extension to decrease government risk (off-ramp for lack of performance) Provide reward incentives for excellence in performance","title":"3.1.3 Contracting Consideration Checklist"},{"location":"applyag/#32-agile-organization-roles-and-responsibilities-stakeholder-level","text":"One addition to the typical traditional Air Force PMO organization is an Agile Coach. As described in the previous training and coaching section, the Agile Coach is someone who can provide real-time answers for the immediate Agile issue. Another addition to the typical PMO staff is an end-user representative, the PO, who is empowered to work with the contractors' agile development team and make decisions that are binding for the development. Given the nature of government contracting, care must be taken to ensure that the PO user representative has the legal authority to direct the contractor. We can envision a situation where constructive change could become an issue. Another addition to the PMO is a DevSecOps Lead who works with the with contractor engineering support teams that may be divided into two segments: Continuous Integration Team and Continuous Delivery/Deployment Team to implement configuration management, version control, automated build, automated security testing, automated functional testing and regression testing. The government needs skilled Agile personnel to review the documentation and understand how the Agile software development approach works. Many traditional PMO teams do not have software representatives experienced with modern software development approaches. That could be more problematic in an Agile environment, where any shortfalls quickly become more visible. Another challenge is keeping high-performing Agile teams together long enough for them to achieve peak performance. This is a challenge because developers can change at the end of a contractual period of performance. The continuity of an Agile team enhances the tacit knowledge of the program and this improves overall performance. One recommendation might be to look at is putting key Agile technical leads into the PMO under a separate contract vehicle or hire them to work for the government PMO organization itself.","title":"3.2 Agile Organization, Roles, and Responsibilities - Stakeholder Level"},{"location":"applyag/#321-stakeholder-consideration-checklist","text":"Empower the Product Owner to make technical decisions Provide regular feedback to stakeholders demonstrating progress Maintain a short feedback loop with users Align with external organizations Testing Configuration","title":"3.2.1 Stakeholder Consideration Checklist:"},{"location":"applyag/#33-project-management","text":"This section describes how project management practices need to be adjusted in support of Agile projects by first identifying common Agile practices and then describing how these management practices work in terms of scope, schedule and cost baselines for the project work.","title":"3.3 Project Management"},{"location":"applyag/#331planning","text":"An integrated project management plan (PMP) is developed for the Agile project to define the basis of all project work and how the work will be performed. It describes how the project will be executed, monitored, control and closed. From the Agile perspective, the performance measurement baseline is an integrated scope, schedule, and cost baseline for the software release project work maintained in the Product Backlog against which project execution metrics are used to measure and manage performance. The PMP describes the series of phases (themes, initiatives, and epics) the project passes through from initiation to closure. The PMP also describes the Agile development management approach; i.e. Agile iteration-based (Scrum/XP), flow-based (Kanban) or a hybrid model. Figure xx describes the hierarchal structure of the Agile project work effort. Notice that this view of the project work hierarchy is similar to the WBS in predictive (Waterfall) projects. One additional consideration is that the above hierarchy promotes a multi-team or \"scaled\" agile approach. While methodologies exist to support the scaling of agile (i.e. Scaled Agile Framework (SAFe), Scrum of Scrums, Disciplined Agile Delivery (DAD), etc), these will not be presented in this playbook but are a topic for further elaboration as an organization's agile process matures.","title":"3.3.1Planning"},{"location":"applyag/#332-scope","text":"In Agile projects the requirements are defined by the organization's stakeholders and Product Owner with support from the Agile Team in the form of Epics, Features, user stories or PBIs that are maintained in the Product Backlog. Therefore, the project scope for an Agile project begins with the organization's governance process which commonly consists of a Configuration Control Board (CCB) that produces a high-level product backlog for a release consisting of a list of approved requirements defined in the form of epics, features and sometimes high-level PBIs. The product owner then works with the Agile Inception Team to prioritize these items (epics, features, PBIs if applicable). The PMO can initially use the MoSCoW (Must Have, Should Have, Could Have, Won't Have this time) method to prioritize requirements. The initial Product Backlog items are divided into 'must haves', 'should haves', 'could haves' & 'won't have': The items with highest business value, and lowest effort involved, are often the items that qualify the 'must haves' criteria. The items that are of higher priority, and will need some effort to be delivered, are deemed, as 'should haves'. All the backlog items that might be desirable in terms of scope, but are of lower business value are segmented, as 'could haves'. The items that are agreed upon to be moved to later releases, are filtered out as 'won't haves' for now.","title":"3.3.2 Scope"},{"location":"applyag/#333forecasting-schedule-and-cost","text":"Once the user stories and PBIs in the product backlog are defined, prioritized and the MVP determined, Agile estimating techniques can be applied to estimate the effort for each feature, sum up the effort for all the features in a project as well as determine which features would be part of which release. The PMO and development team are then able to forecast a schedule and cost for the release project. There are several gross-level estimation techniques used by teams using agile approaches such as Scrum, Kanban, and eXtreme Programming which include T-shirt Sizes (for Features), and Affinity Mapping. T-Shirt Sizes . This estimation technique can be applied when providing a quick and rough estimation to a project feature. Here, the features are estimated in T-shirt sizes, ranging from XS to XL, which would be later converted to numbers, as per requirements. In this type of estimation, the estimators assign a size to each of the features. Points are assigned to the each of the T-Shirt Sizes using the Fibonacci-like format: 0, 1, 2, 3, 5, 8, 13, 20, 40, 100. These points are summed up and based on a rough estimate of how many feature points can get done within a time period by a normal agile team (note, the more detailed the refinement, the better the estimates).","title":"3.3.3Forecasting Schedule and Cost"},{"location":"applyag/#334-cost","text":"The cost for the release is estimated by using the team's iteration average cost and multiplying it by the number of iterations estimated to complete the backlog. For example, the following formula to determine budgeted cost can demonstrate this estimation for the above example: (Team monthly cost (example: $15,000.00 per month) multiplied by the number of months/iterations (example: 5 months) = $75,000.00 + other expenses = forecast budgeted cost. The above example is utterly simplistic and does not take into account the following factors: The Inception phase at the beginning that is required to develop the product backlog, estimate the size in story points and develop an architecture vision; Changes in scope during the Construction iterations caused by adding new features driven by urgent business needs or Cybersecurity issues; A Transition phase at the end of development for Government Acceptance Tests and Security Tests required to achieve customer acceptance and Authority to Operate (AtO) for deployment; Many other variables such as Cloud Migration and implementation of a DevSecOps reference model.","title":"3.3.4 Cost"},{"location":"applyag/#335-project-management-consideration-checklist","text":"The project manager removes ensures funding, organizes stakeholder interactions and keeps the team from being distracted Work in increments. Buy, build, and fail small. Make proceed and pivot decisions regularly. Learn from mistakes but don't punish the people Leverage the efficiency of commercial contracting methods. If and when possible, use services and tools sold by private sector vendors. Use living roadmaps not fixed Integrated master schedules","title":"3.3.5 Project Management Consideration Checklist"},{"location":"applyag/#34-agile-cdrls-and-delivery","text":"","title":"3.4 Agile CDRLs and delivery"},{"location":"applyag/#341-overview","text":"One of the four agile manifesto values is that \"working products are valued over comprehensive documentation\". Many times this is viewed by the agile purist as a justification for not doing documentation. However, even in agile, there is a value for doing documentation. Documentation exists to support the development teams work in creating the product and supporting the product after release. Prior to looking deeper at agile content and delivery recommendations for CDRLs, here are some general considerations to keep in mind when determining the format, content, and delivery schedule for CDRLs within an agile framework - \"Just in time\" - a. Document late (based on design completion) - Consolidate deliverable design documentation as late as possible (though can be iteratively updated) - better to have the stable concepts versus speculative ideas which are constantly changing as part of the agile framework and would require constant document revisions and submissions. b. Document continuously (based on iteration) - iteratively update development related documentation (i.e. user guides) in parallel with development efforts to not lose critical ideas. A key concept here is the idea of a living document, which is discussed below. \"Just sufficient\" - Sufficiency is defined by the document owner (provide the necessary useful documentation elements). Additional thoughts: a. Provide the fewest CDRLs possible with the least amount of overlap (I.e. considering combining the Interface Requirements Specification (IRS) with the Interface Design Description IDD). b. Better communication means less documentation (collaboration is key to agile - often a conversation between engineers can eliminate the need for a staffing document). c. Working software does not eliminate the need for documentation - the software delivered still needs to be improved, operated and maintained in the future - documentation's value is transferring product knowledge gained in development to users, operators and maintainers or to new development personnel when contracts change. \"NOT Just Because\" - Treat documentation as any other requirement that needs to be justified by the government document owner (since resources will need to be allocated to produce it). Documentation work efforts can then be prioritized within the product backlog based on the value it provides.","title":"3.4.1 Overview"},{"location":"applyag/#342-agile-cdrl-content-considerations","text":"Agile is built to be fast and flexible, and the contents of the CDRLs must be able to keep up with this development framework. CDRLs should not be an after-thought - they must be incorporated into the process. In other words, we don't want big CDRLs that are out of date by the time they are published. We need documents which can be frequently updated based on the ongoing iterative development efforts. Updates for CDRLs should be provided incrementally by the team when it is fresh in their mind, versus producing documentation at the end of the release when much of the valuable information has already been forgotten. In the military, this is a mindset switch.","title":"3.4.2 Agile CDRL Content Considerations"},{"location":"applyag/#343-agile-cdrl-delivery-consideration","text":"In order to maintain the current value of CDRLs, these documents should be flexible enough to keep up with an iterative update approach (versus long periods of time between updates). In that case, the best methodology is to establish a system to enable CDRLs as \"living documents\". This can best be enabled by re-thinking the methodology of delivery for CDRLs. By considering alternate digital delivery methodologies, CDRLs can be more quickly updated and maintain their relevance throughout the agile development process. One final note to this section is while Sharepoint or a shared drive may fulfill the CDRL requirements above, a further shift from the traditional mindset is to provide appropriate dashboards or reports within an existing system to provide the CDRL information requirements. An example of this is the Test Descriptions / Scripts. Executable test cases are normally already stored in a digital format within the test management software.","title":"3.4.3 Agile CDRL Delivery Consideration"},{"location":"applyag/#344-cdrl-recommended-modifications-attachment","text":"The following link is to an attachment which provides a more detailed list of CDRLs normally associated with a software development project. The list contains the associated DID, waterfall description, agile recommended modifications, and normal delivery timelines (i.e. are the documents delivered one time for the project, at specified design reviews, with a delivery, or on an as needed basis.","title":"3.4.4 CDRL Recommended Modifications Attachment"},{"location":"applyag/#35-measuring-agile-delivery-kpis-and-metrics-status-reporting","text":"In Agile, the system always runs, thus Agile metrics are empirical and business value-based measurements instead of predictive measurements such as the performance measurement baseline and earned value that are used in traditional Waterfall. Agile metrics measure what the Agile Team delivers, not what the team predicts it will deliver. Project teams use this data for improved schedule and cost forecasts as well as for surfacing problems and issues that the Agile Team can diagnose and address. The metrics described below address Team Metrics, Program Metrics and Portfolio Metrics. These metrics were derived from the Project Management Institute, Inc. Agile Practice Guide, SAFe Metrics, DAD, and Atlassian web sites.","title":"3.5 Measuring Agile Delivery, KPIs, and Metrics - Status Reporting"},{"location":"applyag/#351-team-iteration-metrics","text":"The Agile team metrics discussed below focus on the delivery of software. Whether the project team is a Scrum or Kanban team, each of these agile metrics will help the team better understand their development process, making releasing software easier. Scrum Metrics Sprint burndown. Scrum teams organize development into time-boxed sprint iterations. At the outset of the sprint, the team forecasts how many story points they can finish during a sprint. A sprint burndown report (Figure xx) then tracks the completion of work during the sprint. The x-axis represents time, and the y-axis refers to the amount of story points left to complete. The goal is to have all the forecasted work completed by the end of the sprint. A team that consistently meets its forecast is a compelling advertisement for Agile in their organization, however, it may be too good to be true if the team is inflating the numbers by declaring an item complete before it really is. In the long run cheating hampers learning and improvement. There are several anti-patterns to watch for in team performance: 1. The team finishes early sprint after sprint because they aren't committing to enough work in the sprint backlog. 2. The team misses their forecast sprint after sprint because they're committing to too much work. 3. The burndown line makes steep drops rather than a more gradual burndown because the work hasn't been broken down into granular user stories or PBIs. 4. The product owner adds PBIs or changes the scope mid-sprint. Velocity Velocity is the average amount of work a Scrum team completes during a sprint, measured in story points and we used it in the example from the prior section to forecast a release schedule. The product owner can use velocity to predict how quickly a team can work through the product backlog, since the velocity chart report tracks the forecasted and completed work over several iteration-the more iterations, the more accurate the forecast. Each team's velocity is unique. If team A has a velocity of 25 story points and team B has a velocity of 50 story points, it doesn't mean that team B has higher throughput. Because each team's story point estimation technique is unique, their sprint velocity will be as well. Organizations should resist the temptation to compare velocity across teams. Instead, Program Management should measure the level of effort and output of work based on each team's unique interpretation of story points. Kanban Metrics Team Kanban Board Flow-based Agile Teams using Kanban methods and Kanban Boards need to use different measurements like work in progress, lead time for delivery of a feature to customer, cycle time for completion of a task on the Kanban Board, and response time - the amount of time the item waits until work begins. Figure xx shows an example of an Agile team's initial Kanban board, which captures their current workflow states: analyze, review, build, and integrate and test. After defining the initial process and Work in Process (WIP) limits and executing for a while, the Kanban team's bottlenecks should surface. If this is the case, the Kanban Team refines the workflow process step where the bottleneck occurred or reduces some WIP limits until it becomes evident that a workflow state is 'starving' or is too full. In this manner the Kanban Team continually adjusts the process workflows to optimize their flow. For example, changing WIP limits and merging, splitting, or redefining workflow states. Cumulative Flow Diagram The cumulative flow diagram is a key resource for Kanban teams, helping them ensure the flow of work across the team is consistent. With number of issues on the Y axis, time on the X axis, and colors to indicate the various workflow states, it visually points out shortages and bottlenecks and works in conjunction with Work in Process (WIP) limits. The cumulative flow diagram should look smooth(ish) from left to right. Bubbles or gaps in any one color indicate shortages and bottlenecks, so when the Agile Team sees one, they should look for ways to smooth out color bands across the chart. Anti-patterns to look for are: - Blocking issues create large backups in some parts of the process and starvation in others. - Unchecked backlog growth over time. This results from product owners not closing issues that are obsolete or simply too low in priority to ever be pulled in. Measuring cycle time is an efficient and flexible way to improve a team's processes because the results of changes are discernable almost immediately, allowing them to make any further adjustments right away. The end goal is to have a consistent and short cycle time, regardless of the type of work (new feature, technical debt, etc. Here are two areas to watch out for: - Increasing cycle time - Increasing cycle time is a negative trend which should be addressed in the Operations Review. One exception: if the team's definition of done has expanded, cycle time will probably expand too. - Erratic cycle time - The goal is to have consistent cycle time for work items which have similar story point values. Filter the control chart for each story point value to check for consistency. If cycle time is erratic on small and large story point values, spend time in the Operations Review discussing how to improve estimates.","title":"3.5.1 Team Iteration Metrics"},{"location":"autotest/","text":"1 Overview 1.1 Purpose AF/A4 published the \"US Air Force Enterprise Logistics Flight Plan v2.0\" (ELFP) in April 2016 and the subordinate document \"Enterprise Logistics Technology Annex\" (ELTA) in June of 2016. This plan and annex describe the desired \"synthesized logistics information\" future state of US Air Force (AF) Enterprise Logistics in 2035. BES believes that in order to maintain a path and schedule to achieve those long-term goals, a series of enabling initiatives are needed to accelerate current progress in order to achieve the necessary near-term milestones. It is the intent of the Business and Enterprise System (BES) to include the resulting Playbooks into the BES Process Directory (BPD) to ensure all members of Air Force Program Executive Office (AFPEO) BES, at all operating locations, have quick easy access to standard processes and templates for Defense Business System programs. DoD continues to recognize the need to apply automated software testing processes and procedures in a more consistent and repeatable manner. The Director of Operational Test & Evaluation (DOT&E) annual reports dating back to 2013 and earlier, show a concerted effort to improve the adoption rate of automation across the DoD. The National Defense Authorization Act (NDAA) for FY 2018 commissioned an Automated Testing Technologies study. There continues to be strong interest across the Services to investigate ways in which automation adoption and momentum can be increased. Defense Acquisition Policy, DoD Instruction 5000.02 and 5000.75 through AFMAN 63-144 contain language that encourages the use of automated testing. With the move towards more flexible and agile approaches to software development comes a greater urgency to implement test automation. This playbook addresses the desire to adopt test automation with practical, experienced-based methods and best practices. Automation Playbook Benefits Defines a common understanding of automation processes and terminology Establishes automation best practices to facilitate adoption by AF community Explains the various roles needed to start and maintain test automation Defines an overall architecture of automation applicable across projects and programs Helps programs understand how to migrate from manual to automated testing 1.2 Audience This playbook is intended for those individuals responsible for the management and engineering of test automation. It provides managers with the knowledge that will help them support programs looking to implement automation and it will provide engineers with the information they will need to successfully plan the implementation of a test automation solution. The approach is holistic in that it broadly defines many factors, not just technical ones, that are necessary to understand and apply when moving towards automation. 1.3Benfits of Automated Testing An investment in automation can reap many rewards to the test team and overall project. There are primary and secondary benefits to using automated tools. Primary Benefits The primary benefits to using automation for testing can be summarized as follows: Faster test execution More reliable/repeatable test execution Increased quality from greater test coverage due to additional tests Facilitates testing of more complex scenarios Less error-prone than manual testing More consistent than manual testing Provides for unattended 24/7 test execution Ability to create additional test conditions from single script Reusability of tests within and across test events Ability to test more in the same or shorter time schedule Testing across a variety of software/hardware platforms Allows for the possibility of testing that which could not be tested manually Allows for increased frequency of testing Allows more effective use of testing resources (i.e. more test design, less manual execution) Secondary Benefits Secondary benefits for using automated test tools consist of support activities for testing, rather than the testing itself. These include: User account creation in advance of testing Database seeding with required test data Creation/management of test datasets Test environment configuration setup Pre-test initialization activities Post-test clean-up activities Automated data analysis of concluded test events Project and Program Benefits The use of automation brings benefits beyond testing to the project and program. These include: Improved software quality Earlier defect detection Fewer defects sent to next testing phase Greater efficiency in accomplishing testing Greater relevance of timely test results Reduced risk of deployment Improved test reporting Facilitated identification of defect root causes Reduced test execution cost Shortened test execution period Improved consistency of test executions Better adapted to iterative development where more frequent testing is required Improved feedback related to application quality Improved system reliability through repeatability and consistency of tests","title":"CDRL 0002 Automated Testing - 1 Overview"},{"location":"autotest/#1-overview","text":"","title":"1 Overview"},{"location":"autotest/#11-purpose","text":"AF/A4 published the \"US Air Force Enterprise Logistics Flight Plan v2.0\" (ELFP) in April 2016 and the subordinate document \"Enterprise Logistics Technology Annex\" (ELTA) in June of 2016. This plan and annex describe the desired \"synthesized logistics information\" future state of US Air Force (AF) Enterprise Logistics in 2035. BES believes that in order to maintain a path and schedule to achieve those long-term goals, a series of enabling initiatives are needed to accelerate current progress in order to achieve the necessary near-term milestones. It is the intent of the Business and Enterprise System (BES) to include the resulting Playbooks into the BES Process Directory (BPD) to ensure all members of Air Force Program Executive Office (AFPEO) BES, at all operating locations, have quick easy access to standard processes and templates for Defense Business System programs. DoD continues to recognize the need to apply automated software testing processes and procedures in a more consistent and repeatable manner. The Director of Operational Test & Evaluation (DOT&E) annual reports dating back to 2013 and earlier, show a concerted effort to improve the adoption rate of automation across the DoD. The National Defense Authorization Act (NDAA) for FY 2018 commissioned an Automated Testing Technologies study. There continues to be strong interest across the Services to investigate ways in which automation adoption and momentum can be increased. Defense Acquisition Policy, DoD Instruction 5000.02 and 5000.75 through AFMAN 63-144 contain language that encourages the use of automated testing. With the move towards more flexible and agile approaches to software development comes a greater urgency to implement test automation. This playbook addresses the desire to adopt test automation with practical, experienced-based methods and best practices. Automation Playbook Benefits Defines a common understanding of automation processes and terminology Establishes automation best practices to facilitate adoption by AF community Explains the various roles needed to start and maintain test automation Defines an overall architecture of automation applicable across projects and programs Helps programs understand how to migrate from manual to automated testing","title":"1.1 Purpose"},{"location":"autotest/#12-audience","text":"This playbook is intended for those individuals responsible for the management and engineering of test automation. It provides managers with the knowledge that will help them support programs looking to implement automation and it will provide engineers with the information they will need to successfully plan the implementation of a test automation solution. The approach is holistic in that it broadly defines many factors, not just technical ones, that are necessary to understand and apply when moving towards automation.","title":"1.2 Audience"},{"location":"autotest/#13benfits-of-automated-testing","text":"An investment in automation can reap many rewards to the test team and overall project. There are primary and secondary benefits to using automated tools. Primary Benefits The primary benefits to using automation for testing can be summarized as follows: Faster test execution More reliable/repeatable test execution Increased quality from greater test coverage due to additional tests Facilitates testing of more complex scenarios Less error-prone than manual testing More consistent than manual testing Provides for unattended 24/7 test execution Ability to create additional test conditions from single script Reusability of tests within and across test events Ability to test more in the same or shorter time schedule Testing across a variety of software/hardware platforms Allows for the possibility of testing that which could not be tested manually Allows for increased frequency of testing Allows more effective use of testing resources (i.e. more test design, less manual execution) Secondary Benefits Secondary benefits for using automated test tools consist of support activities for testing, rather than the testing itself. These include: User account creation in advance of testing Database seeding with required test data Creation/management of test datasets Test environment configuration setup Pre-test initialization activities Post-test clean-up activities Automated data analysis of concluded test events Project and Program Benefits The use of automation brings benefits beyond testing to the project and program. These include: Improved software quality Earlier defect detection Fewer defects sent to next testing phase Greater efficiency in accomplishing testing Greater relevance of timely test results Reduced risk of deployment Improved test reporting Facilitated identification of defect root causes Reduced test execution cost Shortened test execution period Improved consistency of test executions Better adapted to iterative development where more frequent testing is required Improved feedback related to application quality Improved system reliability through repeatability and consistency of tests","title":"1.3Benfits of Automated Testing"},{"location":"autotestappend_a/","text":"APPENDIX A: RESOURCES THE FOLLOWING RESOURCES PROVIDE CERTIFICATION AND ACCREDITED TRAINING FOR SOFTWARE TESTING AND TEST AUTOMATION TOPICS: International Software Testing Qualification Board URL: https://www.istqb.org/ As of December 2017, ISTQB has administered over 785,000 exams and issued more than 570,000 certifications in over 120 countries world-wide. The scheme relies on a Body of Knowledge (Syllabi and Glossary) and exam rules that are applied consistently all over the world, with exams and supporting material being available in many languages. American Software Testing Qualification Board URL: https://www.astqb.org/ The mission of ASTQB is to promote professionalism in Software Testing in the United States. We do this by providing and administering quality exams for the ISTQB, ASTQB and IQBBA certifications, by supporting and facilitating software training providers in delivering high quality courses, by actively engaging in the ISTQB working groups, and by supporting efforts to develop and encourage people who are already in or are entering the software testing profession. ASQ URL: https://asq.org/cert/software-quality-engineer With individual and organizational members around the world, ASQ has the reputation and reach to bring together the diverse quality champions who are transforming the world's corporations, organizations and communities to meet tomorrow's critical challenges. The Certified Software Quality Engineer understands software quality development and implementation, software inspection, testing, verification and validation, and implements software development and maintenance processes and methods. QAI Global URL: http://www.qaiusa.com/software-certifications/software-testing-certifications/ As the IT industry becomes more competitive, the ability for management to distinguish professional and skilled individuals in the field becomes mandatory. QAI Global Institute is the global program administrator for the International Software Certification Board (ISCB). Software Certifications has become recognized worldwide as the standard for information technology quality professionals - having certified over 50,000 professionals. ISCB test centers are located in 135 countries across 6 continents. Software certifications cover five major domains and provide eleven professional certifications. These internationally-recognized, examination-based and vendor-independent programs provide full career paths for professionals at all levels. THE FOLLOWING RESOURCES PROVIDE REPORTING ON AUTOMATED TEST TOOL TOPICS: Magic Quadrant for Software Testing Tools URL: https://www.gartner.com/home The need to support faster time to market with higher quality is driving the demand for effective functional test automation tools. We evaluate vendors in this space to help application leaders who are modernizing software development select test automation tools that best match their needs. (note: may require subscription for access to reports) Carnegie Melon University Software Engineering Institute - The Importance of Automated Testing in Open Systems Architecture Initiatives URL: https://insights.sei.cmu.edu/sei_blog/2014/03/the-importance-of-automated-testing-in-open-systems-architecture-initiatives.html The Better Buying Power 2.0 initiative is a concerted effort by the United States Department of Defense to achieve greater efficiencies in the development, sustainment, and re-competition of major defense acquisition programs through cost control, elimination of unproductive processes and bureaucracy, and promotion of open competition. Carnegie Melon University Software Engineering Institute - Five Keys to Effective Agile Test Automation for Government Programs URL: https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=503507 In this discussion-focused webinar, Bob Binder and SuZ Miller will discuss 5 key questions that government organizations contemplating embarking on adopting automated test techniques and tools in an Agile environment are likely to have. THE FOLLOWING RESOURCES PROVIDE FOR COLLABORATIVE DISCUSSIONS AROUND TEST TOOL TOPICS: SW Test Academy URL: https://www.swtestacademy.com/ SW Test Academy (STA) is focused on mainly technical testing topics. In this site, you can find comprehensive descriptions and examples of test automation, performance testing, mobile testing, web service testing, API testing, DevOps, continuous integration, and similar topics. QA Testing Tools URL: http://qatestingtools.com/ QA Testing Tools is an innovative platform and is the only website that gives you an Opportunity to read technical reviews on every software-testing tool, simultaneously giving you in-depth technical information, and comparison tables that direct you towards the most suitable group of tools to fulfill your requirements. Automate the Planet URL: https://www.automatetheplanet.com/resources/ Learn how to write automated tests through working real-world examples. Stack Overflow URL: https://stackoverflow.com/ Each month, over 50 million developers come to Stack Overflow to learn, share their knowledge, and build their careers. Software Testing and Quality Assurance Forums URL: http://www.sqaforums.com/forums/ The online community for software testing and quality assurance professionals. Open Source Testing URL: http://www.opensourcetesting.org/ The open source testing site aims to boost the profile of open source testing tools within the testing industry, principally by providing users with an easy to use gateway to information on the wide range of open source testing tools available. Test Automation Group on LinkedIn URL: https://www.linkedin.com/groups/86204 LinkedIn is the world's largest professional network with more than 562 million users in more than 200 countries and territories worldwide. The Test Automation LinkedIn group is for people that are interested in QA test automation. The following issues can be found in the group discussions: Automation frameworks, Selenium, QTP, Web automation, Automation ROI, TestComplete, XUnit, JUnit, NUnit, JSystem, Automation strategics, Mobile testing (Android, iPhone, Blackberry), Load, agile, jobs and more! (Note: There are several additional groups in LinkedIn that cover test automation topics and specific tools)","title":"-Appendix A Resources"},{"location":"autotestappend_a/#appendix-a-resources","text":"THE FOLLOWING RESOURCES PROVIDE CERTIFICATION AND ACCREDITED TRAINING FOR SOFTWARE TESTING AND TEST AUTOMATION TOPICS: International Software Testing Qualification Board URL: https://www.istqb.org/ As of December 2017, ISTQB has administered over 785,000 exams and issued more than 570,000 certifications in over 120 countries world-wide. The scheme relies on a Body of Knowledge (Syllabi and Glossary) and exam rules that are applied consistently all over the world, with exams and supporting material being available in many languages. American Software Testing Qualification Board URL: https://www.astqb.org/ The mission of ASTQB is to promote professionalism in Software Testing in the United States. We do this by providing and administering quality exams for the ISTQB, ASTQB and IQBBA certifications, by supporting and facilitating software training providers in delivering high quality courses, by actively engaging in the ISTQB working groups, and by supporting efforts to develop and encourage people who are already in or are entering the software testing profession. ASQ URL: https://asq.org/cert/software-quality-engineer With individual and organizational members around the world, ASQ has the reputation and reach to bring together the diverse quality champions who are transforming the world's corporations, organizations and communities to meet tomorrow's critical challenges. The Certified Software Quality Engineer understands software quality development and implementation, software inspection, testing, verification and validation, and implements software development and maintenance processes and methods. QAI Global URL: http://www.qaiusa.com/software-certifications/software-testing-certifications/ As the IT industry becomes more competitive, the ability for management to distinguish professional and skilled individuals in the field becomes mandatory. QAI Global Institute is the global program administrator for the International Software Certification Board (ISCB). Software Certifications has become recognized worldwide as the standard for information technology quality professionals - having certified over 50,000 professionals. ISCB test centers are located in 135 countries across 6 continents. Software certifications cover five major domains and provide eleven professional certifications. These internationally-recognized, examination-based and vendor-independent programs provide full career paths for professionals at all levels. THE FOLLOWING RESOURCES PROVIDE REPORTING ON AUTOMATED TEST TOOL TOPICS: Magic Quadrant for Software Testing Tools URL: https://www.gartner.com/home The need to support faster time to market with higher quality is driving the demand for effective functional test automation tools. We evaluate vendors in this space to help application leaders who are modernizing software development select test automation tools that best match their needs. (note: may require subscription for access to reports) Carnegie Melon University Software Engineering Institute - The Importance of Automated Testing in Open Systems Architecture Initiatives URL: https://insights.sei.cmu.edu/sei_blog/2014/03/the-importance-of-automated-testing-in-open-systems-architecture-initiatives.html The Better Buying Power 2.0 initiative is a concerted effort by the United States Department of Defense to achieve greater efficiencies in the development, sustainment, and re-competition of major defense acquisition programs through cost control, elimination of unproductive processes and bureaucracy, and promotion of open competition. Carnegie Melon University Software Engineering Institute - Five Keys to Effective Agile Test Automation for Government Programs URL: https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=503507 In this discussion-focused webinar, Bob Binder and SuZ Miller will discuss 5 key questions that government organizations contemplating embarking on adopting automated test techniques and tools in an Agile environment are likely to have. THE FOLLOWING RESOURCES PROVIDE FOR COLLABORATIVE DISCUSSIONS AROUND TEST TOOL TOPICS: SW Test Academy URL: https://www.swtestacademy.com/ SW Test Academy (STA) is focused on mainly technical testing topics. In this site, you can find comprehensive descriptions and examples of test automation, performance testing, mobile testing, web service testing, API testing, DevOps, continuous integration, and similar topics. QA Testing Tools URL: http://qatestingtools.com/ QA Testing Tools is an innovative platform and is the only website that gives you an Opportunity to read technical reviews on every software-testing tool, simultaneously giving you in-depth technical information, and comparison tables that direct you towards the most suitable group of tools to fulfill your requirements. Automate the Planet URL: https://www.automatetheplanet.com/resources/ Learn how to write automated tests through working real-world examples. Stack Overflow URL: https://stackoverflow.com/ Each month, over 50 million developers come to Stack Overflow to learn, share their knowledge, and build their careers. Software Testing and Quality Assurance Forums URL: http://www.sqaforums.com/forums/ The online community for software testing and quality assurance professionals. Open Source Testing URL: http://www.opensourcetesting.org/ The open source testing site aims to boost the profile of open source testing tools within the testing industry, principally by providing users with an easy to use gateway to information on the wide range of open source testing tools available. Test Automation Group on LinkedIn URL: https://www.linkedin.com/groups/86204 LinkedIn is the world's largest professional network with more than 562 million users in more than 200 countries and territories worldwide. The Test Automation LinkedIn group is for people that are interested in QA test automation. The following issues can be found in the group discussions: Automation frameworks, Selenium, QTP, Web automation, Automation ROI, TestComplete, XUnit, JUnit, NUnit, JSystem, Automation strategics, Mobile testing (Android, iPhone, Blackberry), Load, agile, jobs and more! (Note: There are several additional groups in LinkedIn that cover test automation topics and specific tools)","title":"APPENDIX A: RESOURCES"},{"location":"autotestappend_b/","text":"APPENDIX B: TEST TOOLS","title":"-Appendix B Test Tools"},{"location":"autotestappend_b/#appendix-b-test-tools","text":"","title":"APPENDIX B: TEST TOOLS"},{"location":"developauto/","text":"5 Developing A Test Automation Solution 5.1 Approaches to Test Automation Many approaches are available for how automated tools are used. Some involve using the tool functionality as-is with little to no modification or customization. This is what is usually done by test groups with less technically skilled testers. Where more technically skilled testers exist, more customizable solutions are created. While these ultimately offer greater flexibility, they also require greater up-front investment and continued skilled resources to maintain and expand the automated solution. The following represent common approaches to automation. Capture/replay Capture/replay or record/replay is a technique which directly uses the test automation tool's built-in feature to allow a user session to be \"captured\" as the tester interacts with the SUT. Often this is what a vendor might demonstrate as capability with a controlled sample application. After the user keystrokes are captured, they are faithfully replayed by the test tool. While this makes for a very effective demonstration of ultimate simplicity, the application of capture/replay in real-world complex applications often doesn't fair as well. In most cases, using capture/replay in moderate to complex system will likely not yield a successful playback of the initially recorded script. This is due to the fact that most tools are not sophisticated enough to understand all the nuances and the multiple parameters and values that are changing dynamically as a user navigates across an application. For tests that are recorded, the maintenance becomes quite difficult to achieve once the number of tests is in the double digits or higher. Each captured script is unique to the others and needs to be individually maintained. This becomes very time consuming and inefficient. Data driven As the name implies, the data driven test technique aims to use data, external to the tool, to drive the test execution. The remaining techniques below are all variations or adaptations of driving tests with data. In its simplest form, data driven tests replace the specific (static) values that were used for one iteration of a test with a set of data that can be used for many iterations of the same test. With the proper application of specification-based test techniques test datasets can be constructed that minimize the number of tests require for the maximum coverage. Keyword driven The keyword-driven approach at its core also uses data to drive tests but the keyword descriptor is a meaningful action for the system to perform. For example, to test a system with parts that need to be placed in inventory, taken out of inventory, or repaired, a keyword driven script may use the keywords \"check in\", \"check out\", and \"repair\" for any given part number. The way a keyword driven script works is that test cases are developed using keywords and the corresponding values for each keyword. Then, the test automation solution parses those keywords and values and lets pre-defined scripts navigate in a pre-determined manner, through the application in order to execute the desired functionality. Process driven The process driven approach also uses data to drive tests, but the data it defines is more detailed than that of the keyword approach. This approach requires the definition of the following in order to create a test script: control, instruction, data. The \"control\" identifies the application window/object which needs to be interrogated. The \"instruction\" may include: input, verification, navigation, timing, etc. The \"data\" would be the relevant data for the given instruction in the context of the window/object. The process driven approach therefore is the most abstracted model as its very design is built around externalizing all data. The benefit this approach brings is highly reusable function libraries. Model driven A model driven approach for testing is derived from a model based design methodology. With modeling, the desired behavior of a SUT can be represented. From this behavior design test cases can be derived and then specific data can be created to make these tests executable. Model based techniques can facilitate creating tests which can then be used under automated control. 5.2 Overall Architectural Considerations The International Software Testing Qualifications Board (ISTQB) has published a body of knowledge (BoK) and glossary that comprise the syllabus for the Advanced Level Test Automation Engineer certification. This BoK contains a general test automation architecture (GTAA) from which a purpose-built test automation architecture (TAA) can be derived. The Generic Test Automation Architecture GTAA (Source: ISTQB Advanced Level Test Automation Engineer Syllabus) The defined layers of the GTAA include: test generation, test definition, test execution, and test adaptation. This generic architecture can be thought of as a catalog of functionality from which we select those capabilities required for a given project or program. Test Generation Layer Test cases, whether for manual or automated testing must be developed. The development process might include a test case generator for systems that are fully integrated with requirements and design. Some advanced development environments will included system modeling from which high-level test cases can be generated. Use of advanced model based testing techniques and tools can provide an efficient method of preparing an initial set of test cases for automation. Test Definition Layer The Test Definition Layer provides support to help define high- and low-level test cases and test procedures. Data needed for test procedures would be generated at this layer. This can include the data needed for keywords used access libraries when a keyword-driven approach is used. Test libraries can include purpose built test sequences and reusable components for our automation solution. Test Execution Layer The execution layer represents that portion of our automated solution that \"runs\" the tests. We often think of the various test tools available as test execution tools. At their core is the capability to execute a test. This applies equally to COTS, GOTS, and OSS test tool solutions, as discussed in Section 6.3.1. Within the execution layer we have two additional components: test reporting and test logging. The test reporting function provides us with information regarding the SUT so that we can ascertain if the SUT if performing as our tests expect it to. This will alert us to changes in the SUT, although it will not necessarily identify the root cause, which may be SUT, data, or environment related. The test logs will help us identify where some of the errors may be originating from, when providing information at a very granular level. The logs can also provide us with information regarding the operation of our Test Automation Framework components and libraries. Errors in any of these should be written out to the log files so that corrective action can be taken. Test Adaptation Layer Test tool solutions must be compatible to work with the intended environment. This can include a graphical user interface (e.g., browser, mobile app, desktop app, etc.) or a messaging interface. Most systems that we are testing will often contain several interfaces which need to be exercised for thoroughness in testing. By analyzing the SUT architecture we gain an understanding of what interfaces exist, and what are requirements are for testing them. Test tools often provide support for multiple interfaces. However, additional interface testing requirements can be met through other test tools or by developing such capability. Test Automation & Framework The 4 layers discussed above all encompass a test automation solution. This ranges from the test script definitions through the automated execution via a given interface. We can think of the Test Automation Framework as those purpose built testware components that allow us to define data inputs, execute tests, and produce reports. The idea behind a framework is that it should be built with reusable components and not contain embedded application data. This will allow for greatest reuse, and ease of maintenance. Configuration Management There are many artifacts that comprise our test automation solution. In order to keep them manageable, we need to understand what version of what component we are using. This applies equally to test data, test automation functions and libraries, test reporting, and test environments (which may include OS patches, security updates, and updated DLLs). By having this understanding and control we are able to revert to a known working environment quickly. Test Management Test management is there to support the development and maintenance of the test automation solution. This includes providing the proper funding, staffing, tools, and environments in order to be successful. Project Management Test automation is a project, often requiring significant development tasks. As such it needs to be planned, and executed with tasks, milestones, people, and resources. Different development lifecycle methodologies will affect the approach and timing of developing an automated solution, as discussed in Chapter 7. 5.2.1 Understanding the system architecture Enterprise systems are built with a multitude of servers and other specialized components with which they interface. A first step in understanding what needs to be tested and what can potentially be automated is to understand the overall system architecture, or enterprise system architecture (ESA). Most systems receive an input (e.g., a user request for data) and route that request through a multitude of servers, that may include application servers, web servers, database servers, and report servers. A good test strategy is to isolate the various requests as they go from one server to another and identify a way to test each individual component in addition to creating an end-to-end test. Automated tools exist that support multiple protocols and that can act as stubs to simulate behavior between systems or components. 5.2.2 Defining interface components Software and systems communicate with one another often via a protocol across an interface. Understanding what interfaces the SUT communicates with will help determine the testing requirement for that interface. Once the interface has been identified, the communications protocol needs to be established in order to simulate data over that interface. Common interfaces and protocols include: Graphical User Interface Text User Interface Applications Programming Interface (API) Services (including web services) Database (ODBC, etc.) A test automation engineer will need to gather the specification for the interface in order that testing can simulate, or emulate the given interface. 5.2.3 Creating a purpose-built architecture The GTAA, as described above, allows for flexibility in the creation of a purpose built Test Automation Architecture (TAA). By understanding the various components that make up our SUT, we can devise a TAA that fits our needs. For example, if we use model based testing, we may already have a solution for the test generation layer in creating our high-level test cases. Otherwise, we need to define the manual process by which these are created. Once we start building our test cases and corresponding data we need to define a repository where these will be stored. This repository should be under configuration control in order to support the versioning of data by release. For test execution, we may already have a robust test tool with pre-made logging and reporting functions. Otherwise, we'll need to develop these to cover those specific needs. A broad overview of interfaces, as described in section 6.2.2 will guide us as to what tools we'll need to support interface testing. 5.3 Evaluation and Selection of Test Tools 5.3.1 Tool choices Testing tools are available from a number of sources. These include: commercial vendors, open source, and custom built. Each source brings with it possible benefits and limitations. For example, a commercial test tool may be rich in features and offer good support, but require a significant initial and ongoing financial investment for license, maintenance, and support. An open source solution may provide the functionality needed at a given moment, but may lack the ability to expand to meet future needs. Custom solutions, which are either contracted out or developed by the government, will be purpose-built and target key areas needing automation. However, the burden of maintaining, correcting, and expanding such a solution will fall to the government to direct either through internal resources or through additional contractor funding. Appendix B: Test Tools provides a listing of tools frequently found to perform well. Each project will have its own needs and will need to evaluate the fit to a given requirement. Open Source Software (OSS) The OSS category of software products has many options available for the various functional and non-functional requirements for test automation (as described in Chapter 4 Scope of test automation). OSS test tools are updated by a community of contributors, and keeping abreast of the changes ensures that the tool is functioning optimally. Many OSS test tools are have been developed to support a specific environment. For example, there are tools to support browser based testing and tools for mobile application testing. A few tools may provide support for multiple environments, but that is more commonly seen in commercial test tools. (see Defense Acquisition Policy, DoD Instruction 5000.75) OSS test tools have expanded rapidly over the years and can meet many enterprise testing needs. The U.S. Government supports the use of OSS solutions to meet information technology needs, as addressed in the Federal Source Code Policy (https://sourcecode.cio.gov/). Commercial off the Shelf (COTS) Commercial software testing tools developed and supported by vendors have been around for many years. These tools tend to be fully featured and often support multiple environments (e.g., browsers, terminals, API, mobile, etc.). Ideally, there would be one tool that offers the tester capabilities to test all software. In reality, even those tools that cover a wide range of environments may support some better than others. So when evaluating a tool across multiple environments, a separate evaluation will need to be made to see how well each environment is supported. By using a paid license and maintenance model, COTS tools provide for research and development to fund a steady stream of improvements, feature additions, and defect patching. A well-funded organization has the resources to do this and keep the tools current. COTS vendors often have multiple test tools in their catalog to support various testing activity (e.g. functional testing, load testing, test management). These tools are often integrated with one another, such as keeping all test artifacts under configuration version control through a test management module. Government of-the-shelf (GOTS) The GOTS solution is one that has been engineered to meet a specific need. Most often this level of effort is only undertaken when an exhaustive evaluation of existing COTS and OSS solutions shows no matches. The GOTS solution Government directed and can be developed internally with technical staff or via subcontract to an external entity. These solutions vary in scope and features and it is often difficult to know what may be available as no centralized catalog exists that lists them. See Appendix B: Test Tools for commonly used test tools by government and industry. 5.3.2 Test tool evaluation In order to properly evaluate test automation tools we need to make sure we identify the correct quality criteria that will help identify the proper fit for the project or program. Environment Each environment presents unique opportunities and challenges for the implementation of test automation tools. Environments may include multiple platforms to support workstations, laptops, and mobile tables and phones. Each of these platforms often will run a different operating system and the software implementation, whether native or through a browser (which are also native to the device), will dictate the functionality to that platform. When evaluating test automation tools, we need to be aware of our complete needs for the environment, including all platforms and devices that ultimately might be required. Be aware that tools perform differently across devices so a prioritization of which device testing should be automated first will help ensure that those most critical platforms can be implemented successfully. Test Types Types and test levels are discussed in sections 4.2 and 4.3 and need to be considering when evaluating test automation tools. This includes defining the types of tests that are candidates for automation across test levels. Tools are purpose built, and each combination of test type/test level may require re-evaluation of a given test tool selected for a prior test type/test level pairing. Technology Test tools exist to support a variety of older and current technologies (e.g. terminals, text UIs, graphical UIs, browsers, etc.) Some tools support many technologies within one product suite. These are most often COTS tools (see section 6.3.1) where a vendor's tools evolve technology to meet continuing technology needs. However, a bundled solution suite may not necessarily contain \"best-in-class\" for each technology that is supported. An a-la-carte approach to find a specific tool for each technology stack that needs testing may ultimately provide a more satisfactory solution, even where integration amongst disparate tools may be required. Language Most tools provide customization through programmability. In this way they are similar to Interactive Development Environments (IDEs) used by developers to design and build applications. With automation we are building an application too, one that when run automates the execution of testing. Common languages used by test tools include Java, Python, C#, C++, VBScript, etc., with some tools supporting multiple languages. Languages offer a range of features and flexibility and a range of effort to learn them. Identifying tools with the right balance of features and flexibility as they pertain to the knowledge and skills in the organization are key to using a specified tool effectively. Reporting When we look at the reporting features of tools we want evaluate both test reporting and logging functionality. The logging function which is often undervalued or not evaluated and is important in determining if errors occurred specific to the automation of tests. The logging function is an audit trail that can be examined to know exactly what occurred doing execution, irrespective of any application criteria being examined. Often the logging function can be customizable to provide specific information that may be required for analysis. For reporting, features should include meaningful displays of data and analysis from which decisions can be made. These may require additional test runs in order to show varying parameters or cumulative data. Most tools have a basic level of on-demand reporting but the better tools will allow for the customization of reporting. Dashboards also provide reporting, most often in a graphical or table based approach where application health is readily displayed. Where other reporting systems exist, exporting data from the tool may be necessary in order to integrate the results with project-level metrics required by project management for planning and reporting purposes. Ease of Use Test tools should be intuitive and easy to understand and use. They should be well documented so that common answers to questions and techniques can be quickly identified. For example, one may need to know what function is used to verify the content on a calendar widget or how to export a test report in csv format. Although some test tools may claim that there is no need to use scripting for test development, most medium to complex test requirements will likely require a level of scripting that makes the test perform to meet standards and requirements. Therefore, \"ease of use\" is a relative term. For those automators with strong programming skills it will be easy to develop scripts with a test tool. However, for the traditional manual tester, the requirement to write program code may be difficult and counterproductive to their domain-specific skills. Sourcing As defined in section 6.3.1 tools are available form a variety of sources. These can include COTS, GOTS, and OSS. Each category may have very good solutions. The solution needs to include the sourcing of the tool, the training required for the tool, the ongoing maintenance requirements to the tool, and the support available for the tool (in the form of technical support, product updates, etc.) Support Support for test tools can be available from many sources. Vendors often have dedicated support sites that allow logging of issues. These are normally restricted to customers paying for support, either directly or indirectly via maintenance contracts. Additionally forums exist on the internet that focus specifically on particular testing tools. These forums may not be a reliable place for answers as contributions are made by volunteers who may or may not have the knowledge to answer correctly. Investment Test tools from commercial entities have a variety of licensing fees structures. These can include individual, per seat, and floating, among others. Understanding the intended usage by your team will help define which license structure that is best for you. Additionally, commercial entities have support and maintenance fees. Support allows for technical questions or issues to be submitted and responded to, while maintenance usually provides for minor and major updates to the testing tool. Working with the most recent version of a tool will often help resolve technical issues (e.g., compatibility, defects, etc.). Training should also be included as a cost, both for vendor and open source solutions. EVALUATION TABLES Evaluation of the above criteria can be reduced to a table indicating the relative importance for each category, with a total weight at 100%. As tools are evaluated, completing the table helps to compare one tool against another tool objectively. (sample values for illustration) Categories should be rated for how closely they meet the intended requirements. A score allows the category to show a strength or weakness in meeting the category requirement. Use of a scoring system ranging from 0 to 5 where each value indicates alignment to the requirement score. Finally, no tool evaluation is complete without trying the tool out in the intended environment where there is a need to automate. This will ensure that the tool really does work as intended. 5.3.3 Proof-of-concept A proof of concept (PoC) is the first step in understanding if a given test tool meets the anticipated needs of the project. With a PoC we want to ensure that the technology that we've provisionally selected actually works in the intended environment. This is key as any prior demonstration of the tool, however capable, needs to now show the same capability in the intended environment. 5.3.4 Test tool prototypes A prototype takes the PoC further along by automating a narrowly defined set of functionality from a system where automation is to take hold. The prototype helps the project team understand the complexities of implementation in a timeframe that will not drain excessive resources or funding. The prototype shows the way ahead and serves to realign expectations of how long it actually takes to get test built for automation. The lessons learned from the prototype will bring greater efficiency to the next phase of automation. 5.3.5 Tool training and support Success in automation requires an overall understanding of testing, technology, and the features and functions of specific testing tools in meeting the stated requirements. Appendix B: Test Tools provides a listing of tools that have been shown to be successful across projects and programs. Appendix A: Resources provides additional resources that cover education and certification around testing best practices and test tools. Combining education and certification with hands-on test tool knowledge empowers the test team to develop purpose-built solutions that align with project and program needs.","title":"-5 Developing a Test Automation Solution"},{"location":"developauto/#5-developing-a-test-automation-solution","text":"","title":"5 Developing A Test Automation Solution"},{"location":"developauto/#51-approaches-to-test-automation","text":"Many approaches are available for how automated tools are used. Some involve using the tool functionality as-is with little to no modification or customization. This is what is usually done by test groups with less technically skilled testers. Where more technically skilled testers exist, more customizable solutions are created. While these ultimately offer greater flexibility, they also require greater up-front investment and continued skilled resources to maintain and expand the automated solution. The following represent common approaches to automation. Capture/replay Capture/replay or record/replay is a technique which directly uses the test automation tool's built-in feature to allow a user session to be \"captured\" as the tester interacts with the SUT. Often this is what a vendor might demonstrate as capability with a controlled sample application. After the user keystrokes are captured, they are faithfully replayed by the test tool. While this makes for a very effective demonstration of ultimate simplicity, the application of capture/replay in real-world complex applications often doesn't fair as well. In most cases, using capture/replay in moderate to complex system will likely not yield a successful playback of the initially recorded script. This is due to the fact that most tools are not sophisticated enough to understand all the nuances and the multiple parameters and values that are changing dynamically as a user navigates across an application. For tests that are recorded, the maintenance becomes quite difficult to achieve once the number of tests is in the double digits or higher. Each captured script is unique to the others and needs to be individually maintained. This becomes very time consuming and inefficient. Data driven As the name implies, the data driven test technique aims to use data, external to the tool, to drive the test execution. The remaining techniques below are all variations or adaptations of driving tests with data. In its simplest form, data driven tests replace the specific (static) values that were used for one iteration of a test with a set of data that can be used for many iterations of the same test. With the proper application of specification-based test techniques test datasets can be constructed that minimize the number of tests require for the maximum coverage. Keyword driven The keyword-driven approach at its core also uses data to drive tests but the keyword descriptor is a meaningful action for the system to perform. For example, to test a system with parts that need to be placed in inventory, taken out of inventory, or repaired, a keyword driven script may use the keywords \"check in\", \"check out\", and \"repair\" for any given part number. The way a keyword driven script works is that test cases are developed using keywords and the corresponding values for each keyword. Then, the test automation solution parses those keywords and values and lets pre-defined scripts navigate in a pre-determined manner, through the application in order to execute the desired functionality. Process driven The process driven approach also uses data to drive tests, but the data it defines is more detailed than that of the keyword approach. This approach requires the definition of the following in order to create a test script: control, instruction, data. The \"control\" identifies the application window/object which needs to be interrogated. The \"instruction\" may include: input, verification, navigation, timing, etc. The \"data\" would be the relevant data for the given instruction in the context of the window/object. The process driven approach therefore is the most abstracted model as its very design is built around externalizing all data. The benefit this approach brings is highly reusable function libraries. Model driven A model driven approach for testing is derived from a model based design methodology. With modeling, the desired behavior of a SUT can be represented. From this behavior design test cases can be derived and then specific data can be created to make these tests executable. Model based techniques can facilitate creating tests which can then be used under automated control.","title":"5.1 Approaches to Test Automation"},{"location":"developauto/#52-overall-architectural-considerations","text":"The International Software Testing Qualifications Board (ISTQB) has published a body of knowledge (BoK) and glossary that comprise the syllabus for the Advanced Level Test Automation Engineer certification. This BoK contains a general test automation architecture (GTAA) from which a purpose-built test automation architecture (TAA) can be derived. The Generic Test Automation Architecture GTAA (Source: ISTQB Advanced Level Test Automation Engineer Syllabus) The defined layers of the GTAA include: test generation, test definition, test execution, and test adaptation. This generic architecture can be thought of as a catalog of functionality from which we select those capabilities required for a given project or program. Test Generation Layer Test cases, whether for manual or automated testing must be developed. The development process might include a test case generator for systems that are fully integrated with requirements and design. Some advanced development environments will included system modeling from which high-level test cases can be generated. Use of advanced model based testing techniques and tools can provide an efficient method of preparing an initial set of test cases for automation. Test Definition Layer The Test Definition Layer provides support to help define high- and low-level test cases and test procedures. Data needed for test procedures would be generated at this layer. This can include the data needed for keywords used access libraries when a keyword-driven approach is used. Test libraries can include purpose built test sequences and reusable components for our automation solution. Test Execution Layer The execution layer represents that portion of our automated solution that \"runs\" the tests. We often think of the various test tools available as test execution tools. At their core is the capability to execute a test. This applies equally to COTS, GOTS, and OSS test tool solutions, as discussed in Section 6.3.1. Within the execution layer we have two additional components: test reporting and test logging. The test reporting function provides us with information regarding the SUT so that we can ascertain if the SUT if performing as our tests expect it to. This will alert us to changes in the SUT, although it will not necessarily identify the root cause, which may be SUT, data, or environment related. The test logs will help us identify where some of the errors may be originating from, when providing information at a very granular level. The logs can also provide us with information regarding the operation of our Test Automation Framework components and libraries. Errors in any of these should be written out to the log files so that corrective action can be taken. Test Adaptation Layer Test tool solutions must be compatible to work with the intended environment. This can include a graphical user interface (e.g., browser, mobile app, desktop app, etc.) or a messaging interface. Most systems that we are testing will often contain several interfaces which need to be exercised for thoroughness in testing. By analyzing the SUT architecture we gain an understanding of what interfaces exist, and what are requirements are for testing them. Test tools often provide support for multiple interfaces. However, additional interface testing requirements can be met through other test tools or by developing such capability. Test Automation & Framework The 4 layers discussed above all encompass a test automation solution. This ranges from the test script definitions through the automated execution via a given interface. We can think of the Test Automation Framework as those purpose built testware components that allow us to define data inputs, execute tests, and produce reports. The idea behind a framework is that it should be built with reusable components and not contain embedded application data. This will allow for greatest reuse, and ease of maintenance. Configuration Management There are many artifacts that comprise our test automation solution. In order to keep them manageable, we need to understand what version of what component we are using. This applies equally to test data, test automation functions and libraries, test reporting, and test environments (which may include OS patches, security updates, and updated DLLs). By having this understanding and control we are able to revert to a known working environment quickly. Test Management Test management is there to support the development and maintenance of the test automation solution. This includes providing the proper funding, staffing, tools, and environments in order to be successful. Project Management Test automation is a project, often requiring significant development tasks. As such it needs to be planned, and executed with tasks, milestones, people, and resources. Different development lifecycle methodologies will affect the approach and timing of developing an automated solution, as discussed in Chapter 7.","title":"5.2 Overall Architectural Considerations"},{"location":"developauto/#521-understanding-the-system-architecture","text":"Enterprise systems are built with a multitude of servers and other specialized components with which they interface. A first step in understanding what needs to be tested and what can potentially be automated is to understand the overall system architecture, or enterprise system architecture (ESA). Most systems receive an input (e.g., a user request for data) and route that request through a multitude of servers, that may include application servers, web servers, database servers, and report servers. A good test strategy is to isolate the various requests as they go from one server to another and identify a way to test each individual component in addition to creating an end-to-end test. Automated tools exist that support multiple protocols and that can act as stubs to simulate behavior between systems or components.","title":"5.2.1 Understanding the system architecture"},{"location":"developauto/#522-defining-interface-components","text":"Software and systems communicate with one another often via a protocol across an interface. Understanding what interfaces the SUT communicates with will help determine the testing requirement for that interface. Once the interface has been identified, the communications protocol needs to be established in order to simulate data over that interface. Common interfaces and protocols include: Graphical User Interface Text User Interface Applications Programming Interface (API) Services (including web services) Database (ODBC, etc.) A test automation engineer will need to gather the specification for the interface in order that testing can simulate, or emulate the given interface.","title":"5.2.2 Defining interface components"},{"location":"developauto/#523-creating-a-purpose-built-architecture","text":"The GTAA, as described above, allows for flexibility in the creation of a purpose built Test Automation Architecture (TAA). By understanding the various components that make up our SUT, we can devise a TAA that fits our needs. For example, if we use model based testing, we may already have a solution for the test generation layer in creating our high-level test cases. Otherwise, we need to define the manual process by which these are created. Once we start building our test cases and corresponding data we need to define a repository where these will be stored. This repository should be under configuration control in order to support the versioning of data by release. For test execution, we may already have a robust test tool with pre-made logging and reporting functions. Otherwise, we'll need to develop these to cover those specific needs. A broad overview of interfaces, as described in section 6.2.2 will guide us as to what tools we'll need to support interface testing.","title":"5.2.3 Creating a purpose-built architecture"},{"location":"developauto/#53-evaluation-and-selection-of-test-tools","text":"","title":"5.3 Evaluation and Selection of Test Tools"},{"location":"developauto/#531-tool-choices","text":"Testing tools are available from a number of sources. These include: commercial vendors, open source, and custom built. Each source brings with it possible benefits and limitations. For example, a commercial test tool may be rich in features and offer good support, but require a significant initial and ongoing financial investment for license, maintenance, and support. An open source solution may provide the functionality needed at a given moment, but may lack the ability to expand to meet future needs. Custom solutions, which are either contracted out or developed by the government, will be purpose-built and target key areas needing automation. However, the burden of maintaining, correcting, and expanding such a solution will fall to the government to direct either through internal resources or through additional contractor funding. Appendix B: Test Tools provides a listing of tools frequently found to perform well. Each project will have its own needs and will need to evaluate the fit to a given requirement. Open Source Software (OSS) The OSS category of software products has many options available for the various functional and non-functional requirements for test automation (as described in Chapter 4 Scope of test automation). OSS test tools are updated by a community of contributors, and keeping abreast of the changes ensures that the tool is functioning optimally. Many OSS test tools are have been developed to support a specific environment. For example, there are tools to support browser based testing and tools for mobile application testing. A few tools may provide support for multiple environments, but that is more commonly seen in commercial test tools. (see Defense Acquisition Policy, DoD Instruction 5000.75) OSS test tools have expanded rapidly over the years and can meet many enterprise testing needs. The U.S. Government supports the use of OSS solutions to meet information technology needs, as addressed in the Federal Source Code Policy (https://sourcecode.cio.gov/). Commercial off the Shelf (COTS) Commercial software testing tools developed and supported by vendors have been around for many years. These tools tend to be fully featured and often support multiple environments (e.g., browsers, terminals, API, mobile, etc.). Ideally, there would be one tool that offers the tester capabilities to test all software. In reality, even those tools that cover a wide range of environments may support some better than others. So when evaluating a tool across multiple environments, a separate evaluation will need to be made to see how well each environment is supported. By using a paid license and maintenance model, COTS tools provide for research and development to fund a steady stream of improvements, feature additions, and defect patching. A well-funded organization has the resources to do this and keep the tools current. COTS vendors often have multiple test tools in their catalog to support various testing activity (e.g. functional testing, load testing, test management). These tools are often integrated with one another, such as keeping all test artifacts under configuration version control through a test management module. Government of-the-shelf (GOTS) The GOTS solution is one that has been engineered to meet a specific need. Most often this level of effort is only undertaken when an exhaustive evaluation of existing COTS and OSS solutions shows no matches. The GOTS solution Government directed and can be developed internally with technical staff or via subcontract to an external entity. These solutions vary in scope and features and it is often difficult to know what may be available as no centralized catalog exists that lists them. See Appendix B: Test Tools for commonly used test tools by government and industry.","title":"5.3.1 Tool choices"},{"location":"developauto/#532-test-tool-evaluation","text":"In order to properly evaluate test automation tools we need to make sure we identify the correct quality criteria that will help identify the proper fit for the project or program. Environment Each environment presents unique opportunities and challenges for the implementation of test automation tools. Environments may include multiple platforms to support workstations, laptops, and mobile tables and phones. Each of these platforms often will run a different operating system and the software implementation, whether native or through a browser (which are also native to the device), will dictate the functionality to that platform. When evaluating test automation tools, we need to be aware of our complete needs for the environment, including all platforms and devices that ultimately might be required. Be aware that tools perform differently across devices so a prioritization of which device testing should be automated first will help ensure that those most critical platforms can be implemented successfully. Test Types Types and test levels are discussed in sections 4.2 and 4.3 and need to be considering when evaluating test automation tools. This includes defining the types of tests that are candidates for automation across test levels. Tools are purpose built, and each combination of test type/test level may require re-evaluation of a given test tool selected for a prior test type/test level pairing. Technology Test tools exist to support a variety of older and current technologies (e.g. terminals, text UIs, graphical UIs, browsers, etc.) Some tools support many technologies within one product suite. These are most often COTS tools (see section 6.3.1) where a vendor's tools evolve technology to meet continuing technology needs. However, a bundled solution suite may not necessarily contain \"best-in-class\" for each technology that is supported. An a-la-carte approach to find a specific tool for each technology stack that needs testing may ultimately provide a more satisfactory solution, even where integration amongst disparate tools may be required. Language Most tools provide customization through programmability. In this way they are similar to Interactive Development Environments (IDEs) used by developers to design and build applications. With automation we are building an application too, one that when run automates the execution of testing. Common languages used by test tools include Java, Python, C#, C++, VBScript, etc., with some tools supporting multiple languages. Languages offer a range of features and flexibility and a range of effort to learn them. Identifying tools with the right balance of features and flexibility as they pertain to the knowledge and skills in the organization are key to using a specified tool effectively. Reporting When we look at the reporting features of tools we want evaluate both test reporting and logging functionality. The logging function which is often undervalued or not evaluated and is important in determining if errors occurred specific to the automation of tests. The logging function is an audit trail that can be examined to know exactly what occurred doing execution, irrespective of any application criteria being examined. Often the logging function can be customizable to provide specific information that may be required for analysis. For reporting, features should include meaningful displays of data and analysis from which decisions can be made. These may require additional test runs in order to show varying parameters or cumulative data. Most tools have a basic level of on-demand reporting but the better tools will allow for the customization of reporting. Dashboards also provide reporting, most often in a graphical or table based approach where application health is readily displayed. Where other reporting systems exist, exporting data from the tool may be necessary in order to integrate the results with project-level metrics required by project management for planning and reporting purposes. Ease of Use Test tools should be intuitive and easy to understand and use. They should be well documented so that common answers to questions and techniques can be quickly identified. For example, one may need to know what function is used to verify the content on a calendar widget or how to export a test report in csv format. Although some test tools may claim that there is no need to use scripting for test development, most medium to complex test requirements will likely require a level of scripting that makes the test perform to meet standards and requirements. Therefore, \"ease of use\" is a relative term. For those automators with strong programming skills it will be easy to develop scripts with a test tool. However, for the traditional manual tester, the requirement to write program code may be difficult and counterproductive to their domain-specific skills. Sourcing As defined in section 6.3.1 tools are available form a variety of sources. These can include COTS, GOTS, and OSS. Each category may have very good solutions. The solution needs to include the sourcing of the tool, the training required for the tool, the ongoing maintenance requirements to the tool, and the support available for the tool (in the form of technical support, product updates, etc.) Support Support for test tools can be available from many sources. Vendors often have dedicated support sites that allow logging of issues. These are normally restricted to customers paying for support, either directly or indirectly via maintenance contracts. Additionally forums exist on the internet that focus specifically on particular testing tools. These forums may not be a reliable place for answers as contributions are made by volunteers who may or may not have the knowledge to answer correctly. Investment Test tools from commercial entities have a variety of licensing fees structures. These can include individual, per seat, and floating, among others. Understanding the intended usage by your team will help define which license structure that is best for you. Additionally, commercial entities have support and maintenance fees. Support allows for technical questions or issues to be submitted and responded to, while maintenance usually provides for minor and major updates to the testing tool. Working with the most recent version of a tool will often help resolve technical issues (e.g., compatibility, defects, etc.). Training should also be included as a cost, both for vendor and open source solutions. EVALUATION TABLES Evaluation of the above criteria can be reduced to a table indicating the relative importance for each category, with a total weight at 100%. As tools are evaluated, completing the table helps to compare one tool against another tool objectively. (sample values for illustration) Categories should be rated for how closely they meet the intended requirements. A score allows the category to show a strength or weakness in meeting the category requirement. Use of a scoring system ranging from 0 to 5 where each value indicates alignment to the requirement score. Finally, no tool evaluation is complete without trying the tool out in the intended environment where there is a need to automate. This will ensure that the tool really does work as intended.","title":"5.3.2 Test tool evaluation"},{"location":"developauto/#533-proof-of-concept","text":"A proof of concept (PoC) is the first step in understanding if a given test tool meets the anticipated needs of the project. With a PoC we want to ensure that the technology that we've provisionally selected actually works in the intended environment. This is key as any prior demonstration of the tool, however capable, needs to now show the same capability in the intended environment.","title":"5.3.3 Proof-of-concept"},{"location":"developauto/#534-test-tool-prototypes","text":"A prototype takes the PoC further along by automating a narrowly defined set of functionality from a system where automation is to take hold. The prototype helps the project team understand the complexities of implementation in a timeframe that will not drain excessive resources or funding. The prototype shows the way ahead and serves to realign expectations of how long it actually takes to get test built for automation. The lessons learned from the prototype will bring greater efficiency to the next phase of automation.","title":"5.3.4 Test tool prototypes"},{"location":"developauto/#535-tool-training-and-support","text":"Success in automation requires an overall understanding of testing, technology, and the features and functions of specific testing tools in meeting the stated requirements. Appendix B: Test Tools provides a listing of tools that have been shown to be successful across projects and programs. Appendix A: Resources provides additional resources that cover education and certification around testing best practices and test tools. Combining education and certification with hands-on test tool knowledge empowers the test team to develop purpose-built solutions that align with project and program needs.","title":"5.3.5 Tool training and support"},{"location":"developmentcycle/","text":"7 Test Automation Development Lifecycle 7.1 Phases in Automation Development The lifecycle phases for development of a test automation solution are not unlike software development for software applications. Although there are differences the process mostly follows the same steps, which include: Requirements Phase The requirements phase allows us to capture features and functionality necessary for our automation solution. Requirements for automation often fall within the following areas: Input/output (I/O) requirements External interface requirements User interface (UI) control requirements Navigation requirements Timing and synchronization requirements Development of logging and reporting functions Requirements for utilities Requirements for automation are all about enabling interaction with the SUT and providing clear reporting of those interactions. Design Phase The design phase for automation includes the evaluation of tools that as described in section 5.3, Evaluation and selection of test tools. Here, we differ from traditional software development in that we are finding test tools that offer compatibility and features needed to interact with our SUT. This is the primary consideration of our design as failing to ensure compatibility with the SUT prevents us from effectively automating. Technical Design Phase The technical design for automation is described in section 5.2, Overall architectural considerations. Here we define an approach that covers the various interfaces and reporting requirements of our SUT. We map out our framework components in this phase so that we know what development activity will be required. Development Phase The development phase in automation will include the possibility of programming functions and developing libraries that address our stated requirements. Additionally it may include some pre-built modules that provide out-of-the-box functionality from our selected test tool or it may include some third-party modules that integrate with the test tool add functionality. Test Phase Each feature and function that has been engineered into our automated solution must be tested in order that we can be certain it will reliably and accurately help us in testing the SUT. A test baseline must be used in order to validate functionality as testing the automation against the SUT may be unpredictable. Implementation Phase Once we have a high degree of confidence that our automation solution is working properly, we can start using it to test against the SUT. While we expect that automated testing will help us uncover defects, we always first need to verify that the defect is not attributable to the automation components. With automation, the features and functions we build are there to support our need for testing against software application programs. 7.2 Similarities to Application Development Development of a test automation solution should include best practices. These include, but are not limited to: Reviews Reviews are an important aspect of making sure that what is being developed stays true to what is expected. Implied in the review process is that someone other than the individual producing the work has an opportunity to evaluate and provide feedback. Reviews are often done incrementally thus allowing for changes or course-correction to take place if warranted. Reviews can start at a very high level, progress to a very detailed level, and also include quality characteristics. Reviews can cover: Architecture Code Peer Usability Documentation Documentation helps us understand the automated solution at various levels. From how it works, and what features it contains, to how to make changes or upgrades. Documentation helps make sure that the automated solution is used properly. In summary we can use documentation for: In-line code descriptions Function descriptions Library cataloging User instructions Maintenance instructions Standard variable naming The way in which variables are named in a software often helps us understand the context and use of that variable. Using capitalization and lower case letters in a mixed case format along with compound word choices can facilitate the automation engineer's understanding of what variables are used for. This also provides discipline for new variables to use the same naming standards and conventions. Abstraction Abstraction indicates a separation of all but the most relevant data in order to reduce complexity, duplication, and increase efficiency of the automation code base. With automation, an example of abstraction may revolve around the way we choose to interact with a given UI control. With abstraction, we would create a single function that is used to interact with that control regardless of where it appears within the SUT. The function itself will have the details pertaining to what methods and data the control needs. Cyclomatic complexity The cyclomatic complexity metric is a quantitative measure of code complexity. It allows us to measure the number of linearly independent paths through code. A high cyclomatic complexity value indicates that many (if not too many) paths exist which in turn indicates a high number of test cases necessary to fully validate code. Although most test automation code is not subject to cyclomatic complexity analysis, in the way that a software program might, it is good practice to keep the number of independent paths (often referred to as conditions) low so that we can ensure proper testing can be done while minimizing potential errors due to code complexity. 7.3 Similarities to Manual Testing Once we have our test automation solution up and running and we have data to drive some testing we can confirm that our automated solution properly tests the SUT and faithfully reproduces the testing that was otherwise done manually. We still need to analyze what is to be tested. From this we need to define the test types and develop test data. This is all still very similar to the work we do in manual testing. However, the definition of data for automated testing is more structured as we're now reliant on the test automation solution and specifically the testing framework to work with our defined data and provide us with results. As with manual testing we will have results to analyze, but with automation we would expect a good portion of this analysis, at least as it pertains to verifications and comparisons, to occur automatically via automation. From here we need to ensure the confidence in our acceptance in that automation has met or exceeded our previous manual testing efforts. This is an important milestone and one which can help bring the team together in supporting their contributions in the development of test automation.","title":"-7 Test Automation Development Lifecycle"},{"location":"developmentcycle/#7-test-automation-development-lifecycle","text":"","title":"7 Test Automation Development Lifecycle"},{"location":"developmentcycle/#71-phases-in-automation-development","text":"The lifecycle phases for development of a test automation solution are not unlike software development for software applications. Although there are differences the process mostly follows the same steps, which include: Requirements Phase The requirements phase allows us to capture features and functionality necessary for our automation solution. Requirements for automation often fall within the following areas: Input/output (I/O) requirements External interface requirements User interface (UI) control requirements Navigation requirements Timing and synchronization requirements Development of logging and reporting functions Requirements for utilities Requirements for automation are all about enabling interaction with the SUT and providing clear reporting of those interactions. Design Phase The design phase for automation includes the evaluation of tools that as described in section 5.3, Evaluation and selection of test tools. Here, we differ from traditional software development in that we are finding test tools that offer compatibility and features needed to interact with our SUT. This is the primary consideration of our design as failing to ensure compatibility with the SUT prevents us from effectively automating. Technical Design Phase The technical design for automation is described in section 5.2, Overall architectural considerations. Here we define an approach that covers the various interfaces and reporting requirements of our SUT. We map out our framework components in this phase so that we know what development activity will be required. Development Phase The development phase in automation will include the possibility of programming functions and developing libraries that address our stated requirements. Additionally it may include some pre-built modules that provide out-of-the-box functionality from our selected test tool or it may include some third-party modules that integrate with the test tool add functionality. Test Phase Each feature and function that has been engineered into our automated solution must be tested in order that we can be certain it will reliably and accurately help us in testing the SUT. A test baseline must be used in order to validate functionality as testing the automation against the SUT may be unpredictable. Implementation Phase Once we have a high degree of confidence that our automation solution is working properly, we can start using it to test against the SUT. While we expect that automated testing will help us uncover defects, we always first need to verify that the defect is not attributable to the automation components. With automation, the features and functions we build are there to support our need for testing against software application programs.","title":"7.1 Phases in Automation Development"},{"location":"developmentcycle/#72-similarities-to-application-development","text":"Development of a test automation solution should include best practices. These include, but are not limited to: Reviews Reviews are an important aspect of making sure that what is being developed stays true to what is expected. Implied in the review process is that someone other than the individual producing the work has an opportunity to evaluate and provide feedback. Reviews are often done incrementally thus allowing for changes or course-correction to take place if warranted. Reviews can start at a very high level, progress to a very detailed level, and also include quality characteristics. Reviews can cover: Architecture Code Peer Usability Documentation Documentation helps us understand the automated solution at various levels. From how it works, and what features it contains, to how to make changes or upgrades. Documentation helps make sure that the automated solution is used properly. In summary we can use documentation for: In-line code descriptions Function descriptions Library cataloging User instructions Maintenance instructions Standard variable naming The way in which variables are named in a software often helps us understand the context and use of that variable. Using capitalization and lower case letters in a mixed case format along with compound word choices can facilitate the automation engineer's understanding of what variables are used for. This also provides discipline for new variables to use the same naming standards and conventions. Abstraction Abstraction indicates a separation of all but the most relevant data in order to reduce complexity, duplication, and increase efficiency of the automation code base. With automation, an example of abstraction may revolve around the way we choose to interact with a given UI control. With abstraction, we would create a single function that is used to interact with that control regardless of where it appears within the SUT. The function itself will have the details pertaining to what methods and data the control needs. Cyclomatic complexity The cyclomatic complexity metric is a quantitative measure of code complexity. It allows us to measure the number of linearly independent paths through code. A high cyclomatic complexity value indicates that many (if not too many) paths exist which in turn indicates a high number of test cases necessary to fully validate code. Although most test automation code is not subject to cyclomatic complexity analysis, in the way that a software program might, it is good practice to keep the number of independent paths (often referred to as conditions) low so that we can ensure proper testing can be done while minimizing potential errors due to code complexity.","title":"7.2 Similarities to Application Development"},{"location":"developmentcycle/#73-similarities-to-manual-testing","text":"Once we have our test automation solution up and running and we have data to drive some testing we can confirm that our automated solution properly tests the SUT and faithfully reproduces the testing that was otherwise done manually. We still need to analyze what is to be tested. From this we need to define the test types and develop test data. This is all still very similar to the work we do in manual testing. However, the definition of data for automated testing is more structured as we're now reliant on the test automation solution and specifically the testing framework to work with our defined data and provide us with results. As with manual testing we will have results to analyze, but with automation we would expect a good portion of this analysis, at least as it pertains to verifications and comparisons, to occur automatically via automation. From here we need to ensure the confidence in our acceptance in that automation has met or exceeded our previous manual testing efforts. This is an important milestone and one which can help bring the team together in supporting their contributions in the development of test automation.","title":"7.3 Similarities to Manual Testing"},{"location":"effectmeasure/","text":"8 Effectiveness Measurements Automation impacts can be measured beyond speed of test execution. The following areas provide metrics on use and effectiveness of automation. Schedule Test Execution time improvement Test Setup time improvement Time to determine failures/defects Time to analyze data Effectiveness Failures found Number of test required/number of system errors Defects found/number of test procedures executed Test procedures executed without defects/ total test procedures Coverage Test Coverage - test procedures/test requirements Automation test coverage - automated test cases/total test cases New test capabilities Reusability improvement within project/program Cost Man-hour reduction Total project or program savings Additional training requirement Additional resources Additional Maintainability Each project or program should select those metrics that provide the most relevant feedback to assess quality and effectiveness of automation. Although test execution times are dramatically shortened through the use of automation, test preparation steps and maintenance for automation need to be measured as part of the overall effort and cost, as compared to manual testing.","title":"-8 Effectiveness Measurements"},{"location":"effectmeasure/#8-effectiveness-measurements","text":"Automation impacts can be measured beyond speed of test execution. The following areas provide metrics on use and effectiveness of automation. Schedule Test Execution time improvement Test Setup time improvement Time to determine failures/defects Time to analyze data Effectiveness Failures found Number of test required/number of system errors Defects found/number of test procedures executed Test procedures executed without defects/ total test procedures Coverage Test Coverage - test procedures/test requirements Automation test coverage - automated test cases/total test cases New test capabilities Reusability improvement within project/program Cost Man-hour reduction Total project or program savings Additional training requirement Additional resources Additional Maintainability Each project or program should select those metrics that provide the most relevant feedback to assess quality and effectiveness of automation. Although test execution times are dramatically shortened through the use of automation, test preparation steps and maintenance for automation need to be measured as part of the overall effort and cost, as compared to manual testing.","title":"8 Effectiveness Measurements"},{"location":"lookingahead/","text":"10 Looking Ahead 10.1 Building Momentum in Test Automation Test automation is not a static activity. Due to its nature, it is not as robust as a full-on software application solution. Automation may require review, evaluation, and tweaking from time to time in order that it continues to support testing activities. Therefore, there should always be someone assigned to monitor and maintain the test automation solution. Automation is additive. From the initial scripts that are automated, more scripts can be developed or reused. In this way, automation helps us build capability similar to stacking bricks to build a wall. Initial automation will be purpose-built, while later automation may be more about stringing prior automated scripts into larger and more complex business processes. Automation can also help with pre- and post-testing activities, as described in section 2.1. These activities often take time to carry out and the use of automation will help reduce the overall test execution activity time. Often other projects with similar technology will benefit from the automation developed. It is far easier to adapt an automation solution to another program than to build it again from scratch. 10.2 Keeping Abreast of the Technology Landscape Software development continues to evolve and the SUT will, over time, incorporate these technology updates. The test automation solution must anticipate and adapt to these changes so that automation continues to work reliably and efficiently. It is recommended to perform a tool evaluation on a regular basis (every 2 years is recommended). Many tools, whether COTS, GOTS, or OSS regularly have incremental updates made to them. Before automatically updating the tool to the latest version, it is best to find out what improvements were made to the tool and if those improvements have any impact to the project using those tools. If it makes sense to make the update, this should first be tested in a separate environment to ensure that the update functions properly and does not inadvertently affect any other software or components previously installed. Generally, the latest release will have the latest security updates and features so updates are recommended. When an automated framework is built with modularity in mind, specific components of the framework can be swapped out or swapped in without having to re-write the entire framework. For example, a project that originally used browsers has now migrated to native mobile applications as well. The requirement now is to also support native mobile apps. This could potentially be accomplished in one of several ways: Identify if current tool in use also supports mobile apps Identify add-in component that supports mobile apps Identify new tool that supports mobile apps Each condition allows for a different approach to adding mobile app support to the framework. 10.3 Continuous Improvement Activities The areas that most benefit from ongoing inspection, review, and improvement will include: Functional script design Functional tests can be analyzed for their overall construction. This will include identifying areas of duplication with other scripts and extracting common functionality which can be better served by calling a shared component. Scripts that are excessively long will likely benefit from modularization. Error trapping Often, things go wrong during testing. This may include predicted and unpredicted errors. When errors occur they can cause havoc with automated tests. One approach to address this is through the use of error trapping. With error trapping we can direct abnormal application behavior to a known state. This is helpful when we have a large number of tests queued up and we do not want to hold up the next testing activity. Timing When executing tests we want the behavior to mimic real user interactions with the system. In order to do this we may need to slow down the rate at which automation interacts with the SUT. Adding a static number of seconds to pause the automation is not recommended as these static pauses can add up and eventually slow down test execution. A better approach is to use dynamic wait statements. These are often implemented by observing the attribute of a control. For example, if we want to make sure that the screen is ready to accept our order, we could dynamically interrogate the order field to make sure it is ready to accept input. This way, whether it takes .05 seconds or 3 seconds, the automation will wait the appropriate time before continuing with the order. Code Base Treat your automation code as any other software development project. This includes frequent reviews, coding guidelines, documentation, and the use of developers to help with some of the more challenging components. Performance After using automation for some time we may find that we are no longer able to execute all of our testing on an overnight run. This will require examining just what scripts are being executed, and evaluating if some of these no longer provide the value they once did and need to be re-written or merged into other more efficient scripts. Performance can also be measured at the component level, where much of the I/O activity occurs. You may want to evaluate if you we using resources effectively. For example, are you writing temporary results to a disk? If so, you may consider writing them to RAM which is much quicker. Script performance will also be affected with timing, as described above. Audit and reporting Understanding of what occurred during test execution can be enhance by audit logs and reporting. Audit logs can be developed that provide a level of granularity that helps us understand everything about our test execution. For example, we could develop a logging function that lets us know what windows were present at any time that the test was running. This make give us clues to windows that were not closed, messaging pop-ups that were not accounted for, or starting conditions that did not meet our expectations. Reporting requirements will come from stakeholders and once they start receiving information they will likely want additional information from the SUT and similar information but expressed differently. Satisfying the needs of stakeholders is an important part of keeping an automation solution viable. 10.4 Making the Process Repeatable Ultimately, we want to learn from our efforts and know that we can do this again for the next project. Repeatability of the process requires a sound overall design, clear implementation, and useful documentation. As the automation evolves, all aspects of its design need to be understood and documented so that as testers are rotated into and out of a program there is a permanence to the automation that was built.","title":"-10 Looking Ahead"},{"location":"lookingahead/#10-looking-ahead","text":"","title":"10 Looking Ahead"},{"location":"lookingahead/#101-building-momentum-in-test-automation","text":"Test automation is not a static activity. Due to its nature, it is not as robust as a full-on software application solution. Automation may require review, evaluation, and tweaking from time to time in order that it continues to support testing activities. Therefore, there should always be someone assigned to monitor and maintain the test automation solution. Automation is additive. From the initial scripts that are automated, more scripts can be developed or reused. In this way, automation helps us build capability similar to stacking bricks to build a wall. Initial automation will be purpose-built, while later automation may be more about stringing prior automated scripts into larger and more complex business processes. Automation can also help with pre- and post-testing activities, as described in section 2.1. These activities often take time to carry out and the use of automation will help reduce the overall test execution activity time. Often other projects with similar technology will benefit from the automation developed. It is far easier to adapt an automation solution to another program than to build it again from scratch.","title":"10.1 Building Momentum in Test Automation"},{"location":"lookingahead/#102-keeping-abreast-of-the-technology-landscape","text":"Software development continues to evolve and the SUT will, over time, incorporate these technology updates. The test automation solution must anticipate and adapt to these changes so that automation continues to work reliably and efficiently. It is recommended to perform a tool evaluation on a regular basis (every 2 years is recommended). Many tools, whether COTS, GOTS, or OSS regularly have incremental updates made to them. Before automatically updating the tool to the latest version, it is best to find out what improvements were made to the tool and if those improvements have any impact to the project using those tools. If it makes sense to make the update, this should first be tested in a separate environment to ensure that the update functions properly and does not inadvertently affect any other software or components previously installed. Generally, the latest release will have the latest security updates and features so updates are recommended. When an automated framework is built with modularity in mind, specific components of the framework can be swapped out or swapped in without having to re-write the entire framework. For example, a project that originally used browsers has now migrated to native mobile applications as well. The requirement now is to also support native mobile apps. This could potentially be accomplished in one of several ways: Identify if current tool in use also supports mobile apps Identify add-in component that supports mobile apps Identify new tool that supports mobile apps Each condition allows for a different approach to adding mobile app support to the framework.","title":"10.2 Keeping Abreast of the Technology Landscape"},{"location":"lookingahead/#103-continuous-improvement-activities","text":"The areas that most benefit from ongoing inspection, review, and improvement will include: Functional script design Functional tests can be analyzed for their overall construction. This will include identifying areas of duplication with other scripts and extracting common functionality which can be better served by calling a shared component. Scripts that are excessively long will likely benefit from modularization. Error trapping Often, things go wrong during testing. This may include predicted and unpredicted errors. When errors occur they can cause havoc with automated tests. One approach to address this is through the use of error trapping. With error trapping we can direct abnormal application behavior to a known state. This is helpful when we have a large number of tests queued up and we do not want to hold up the next testing activity. Timing When executing tests we want the behavior to mimic real user interactions with the system. In order to do this we may need to slow down the rate at which automation interacts with the SUT. Adding a static number of seconds to pause the automation is not recommended as these static pauses can add up and eventually slow down test execution. A better approach is to use dynamic wait statements. These are often implemented by observing the attribute of a control. For example, if we want to make sure that the screen is ready to accept our order, we could dynamically interrogate the order field to make sure it is ready to accept input. This way, whether it takes .05 seconds or 3 seconds, the automation will wait the appropriate time before continuing with the order. Code Base Treat your automation code as any other software development project. This includes frequent reviews, coding guidelines, documentation, and the use of developers to help with some of the more challenging components. Performance After using automation for some time we may find that we are no longer able to execute all of our testing on an overnight run. This will require examining just what scripts are being executed, and evaluating if some of these no longer provide the value they once did and need to be re-written or merged into other more efficient scripts. Performance can also be measured at the component level, where much of the I/O activity occurs. You may want to evaluate if you we using resources effectively. For example, are you writing temporary results to a disk? If so, you may consider writing them to RAM which is much quicker. Script performance will also be affected with timing, as described above. Audit and reporting Understanding of what occurred during test execution can be enhance by audit logs and reporting. Audit logs can be developed that provide a level of granularity that helps us understand everything about our test execution. For example, we could develop a logging function that lets us know what windows were present at any time that the test was running. This make give us clues to windows that were not closed, messaging pop-ups that were not accounted for, or starting conditions that did not meet our expectations. Reporting requirements will come from stakeholders and once they start receiving information they will likely want additional information from the SUT and similar information but expressed differently. Satisfying the needs of stakeholders is an important part of keeping an automation solution viable.","title":"10.3 Continuous Improvement Activities"},{"location":"lookingahead/#104-making-the-process-repeatable","text":"Ultimately, we want to learn from our efforts and know that we can do this again for the next project. Repeatability of the process requires a sound overall design, clear implementation, and useful documentation. As the automation evolves, all aspects of its design need to be understood and documented so that as testers are rotated into and out of a program there is a permanence to the automation that was built.","title":"10.4 Making the Process Repeatable"},{"location":"migrating/","text":"6 Migrating From Manual to Automated Testing 6.1 What Makes Sense to Automate The process of test case selection and evaluation Complex tests Tests that are complex create the possibility of introducing errors into their execution. Testers, careful as they are, are susceptible to making execution errors while they are testing. Complexity is a quality best delegated to computers. However, the complexity of the test need to be designed and tested before or during the codification to automation. Complex tests may require testing results from calculations and those calculation results are best defined outside of the automated test. Otherwise, the automated test adds additional risk of coding and logic error if it not only has to validate results but also needs to calculate them as well. Long tests Often, business scenarios can take 100 - 300 steps in order to be fully exercised. This creates very long tests which are very time-consuming to execute. The longer the manual execution time, the more time there is to introduce an operator error. Additionally, manually executing a long test introduces risk as it is difficult to document the intervening steps making it difficult to state unequivocally if the test was 100% successful. Automation allows us to codify any number of steps necessary to execute a test, all with the possibility of providing an audit trail of that activity. Repeatable tests Tests that are run on a regular basis in order to validate application functionality are excellent candidates for automation as reuse provides high return on investment. Often, projects will use a risk-based approach to testing that balances the risks with the high-value system functions. In this way the most critical functionality is tested first, while less critical functionality is tested time permitting. Often the first tests that will be automated are the smoke tests, or high-level tests which are run for every release, major and minor, and whose function is to establish the most basic operational requirement. This often will include the ability to access a system with valid credentials, navigate across major application functions, and verify that there is database connectivity. Dependency tests Although we develop tests as stand-alone entities, often tests and test data are dependent on one another. For example, when entering an airplane part into an order, we would expect to find that same airplane part in a shipping manifest for that newly created order. If we have one test that creates the order, we need to somehow pass the new order number to the next test so that it can verify the shipping manifest. When testing this manually, we look at the system generated order number, write it down, and then use that order number on the following test. This can get cumbersome and inefficient. With automation, we merely provide an instruction to capture and store the value that we can access at a later time. Scenario based tests Business scenarios often involve multiple dependencies, as described above. However, for the scenario we have a larger scale process that we need to confirm is working. Automation provides an opportunity to link the various functions that are tested across a business scenario. Automation can help synchronize the starting of tests, passing of data from one test to another, aggregating test results, and monitoring systems while the automated test is running. For example, when testing month-end or quarter-end close activities there are a number of processes that need to be done, in a specific order, and dependent upon each other, in order that the final month-end reporting is correct and accurate. 6.2 When Should Automation Occur Ideally, automation of tests should occur once we have a strong understanding of what needs to be tested, when have defined test cases, and when we've developed test data for those test cases. When we've achieved the above, we've likely already tested once or more times manually in order to ensure that our test definitions are correct. This initial manual test activity can save much time for the test automation engineer as that individual will now have valid test conditions and data to work with in developing automated tests. For traditional development methodologies, this would occur after the initial software release and automation would be most helpful in building out a regression test bed. For Agile projects, the automation will likely be scheduled as a 1- or 2-sprint offset as individual sprints themselves do not often allow adequate time to build out the test automation architecture and framework. Automation in Agile becomes a work-in-progress where each successive sprint sees additional functionality and maturation added to the test automation solution. 6.3 Criteria for Creating New Automated Tests There may be an opportunity to immediately automate tests, rather than prepare for automation with manual tests. In order for this to happen, the test automation framework has to be sufficiently developed to allow its use for new tests. The timing of this is dependent on the complexity of the application and components that need to be tested. For example, if an application has 200 different controls spread across 5 functional areas, the framework must be capable of identifying and interacting with each of those controls, so that no test is off limits for development. If the 200 controls can be mapped out against the 5 functional areas, then as long as the functional areas under test have been verified and are compatible automation can proceed. Often automation teams do not do their due diligence to find all controls and ensure that the test automation solution indeed is compatible with every one of them. It only takes 1 error in trying to interact with a control for testing to fail, thus undermining the automated approach. 6.4 Criteria for Converting Manual Tests to Automation Most projects have documented test cases and scripts defining the test steps. When evaluating the migration of tests to automation, the following should be considered: Is the test necessary? Often, tests that were developed a long time ago no longer provide the value they once did as applications evolve and other newer tests may already cover parts of the functionality from older tests. When examining existing tests, evaluate if any or all of a test's functions may already be present in other tests. Is the test complete? Does the test exercise the complete business process so that it provides relevant, meaningful results? If not, the business analyst, manual tester, and test automation engineer will likely need to work together to enhance the capability of the automated test. Is the test effective? Effectiveness for any test can be measured by coverage to the requirement and coverage to the code. Specification based testing techniques help us determine how to construct data sets that will provide us with the least amount of test cases that can generate the highest level of coverage, thus mitigating deployment risk. Code coverage tools can be run by developers in parallel to test execution to verify the code coverage. Automation provides us the vehicle by which additional quality test cases can now be executed which previously were not possible, due to time and resource constraints, under a manual scenario. Is the test efficient? Test efficiency can take on several aspects, from speed of execution to overall construction. Automation by itself will always execute a test faster than can be done manually. However, not all manual tests have been designed in a way that is efficient for automation to process them. For example, for many applications, testing follows a prescribed path through an application. The path of navigation is what allows us to test getting from, say, the login screen to the reports screen. Some applications can have complex or lengthy paths before even getting to test for data. With automation we have the opportunity to create navigational tests that can be combined with functional tests in a manner that allows for reuse. Regarding test construction, the approach used to develop a manual test will often differ from the approach to develop an automated test. This is due to the different ways in which each test is executed. Testers think and act linearly, while computers can be made to jump from one area to another. Decomposition of manual tests and reconstruction is a worthwhile effort in order to maximize the quality of automated tests produced. Depending on how manual tests have been created, they may be: Broken apart and reconstituted across various automated tests Combined into one larger automated test Converted one-for-one as an automated test 6.5 Transitioning Staff to Automation Since the advent of testing tools there has been talk that automation kills testing jobs. For large programs heavily dependent on manual testers we would expect to see a reduction of an inefficient process. However, automation mostly helps testers increase their focus on developing quality tests rather than on time consuming \"key pounding\" test execution activity. It is very important to make sure staff understand that they are valued for their knowledge of systems and their testing acumen. It is also important to acknowledge that in order for automation to be successful, there are different roles, and not all roles require programing skills. Section 3.5 describes the various roles necessary for the tasks required to make automation successful. Outsourced solutions Test automation solutions can be contracted out to firms knowledgeable in creating and maintaining test automation frameworks. In this scenario, a firm would be contracted to assist in automating all or part of a system. Once the capability is developed the government would request documentation, training, and transitioning as part of the final deliverable. Government developed solutions Government developed automation would require the selection of staff with the appropriate skills and training so that the process of building out automation does not become a learn-as-you-go exercise, which ultimately will not show a high return on investment. Appendix A covers many resources that can provide the necessary knowledge for team members. Acquiring automation assets In addition to the wide range of test automation tools described in Section 5.3.1 and listed in Appendix B there exist pre-made frameworks that can be used with existing tools. These frameworks can offer a shortcut to a build-from-scratch strategy, and often one can continue to build additional functionality to the base framework libraries. Regardless of the solution used in migrating to automation, a tool-agnostic approach that abstracts test data from test tools provides the greatest flexibility and overall longevity.","title":"-6 Migrating From Manual to Automated Testing"},{"location":"migrating/#6-migrating-from-manual-to-automated-testing","text":"","title":"6 Migrating From Manual to Automated Testing"},{"location":"migrating/#61-what-makes-sense-to-automate","text":"The process of test case selection and evaluation Complex tests Tests that are complex create the possibility of introducing errors into their execution. Testers, careful as they are, are susceptible to making execution errors while they are testing. Complexity is a quality best delegated to computers. However, the complexity of the test need to be designed and tested before or during the codification to automation. Complex tests may require testing results from calculations and those calculation results are best defined outside of the automated test. Otherwise, the automated test adds additional risk of coding and logic error if it not only has to validate results but also needs to calculate them as well. Long tests Often, business scenarios can take 100 - 300 steps in order to be fully exercised. This creates very long tests which are very time-consuming to execute. The longer the manual execution time, the more time there is to introduce an operator error. Additionally, manually executing a long test introduces risk as it is difficult to document the intervening steps making it difficult to state unequivocally if the test was 100% successful. Automation allows us to codify any number of steps necessary to execute a test, all with the possibility of providing an audit trail of that activity. Repeatable tests Tests that are run on a regular basis in order to validate application functionality are excellent candidates for automation as reuse provides high return on investment. Often, projects will use a risk-based approach to testing that balances the risks with the high-value system functions. In this way the most critical functionality is tested first, while less critical functionality is tested time permitting. Often the first tests that will be automated are the smoke tests, or high-level tests which are run for every release, major and minor, and whose function is to establish the most basic operational requirement. This often will include the ability to access a system with valid credentials, navigate across major application functions, and verify that there is database connectivity. Dependency tests Although we develop tests as stand-alone entities, often tests and test data are dependent on one another. For example, when entering an airplane part into an order, we would expect to find that same airplane part in a shipping manifest for that newly created order. If we have one test that creates the order, we need to somehow pass the new order number to the next test so that it can verify the shipping manifest. When testing this manually, we look at the system generated order number, write it down, and then use that order number on the following test. This can get cumbersome and inefficient. With automation, we merely provide an instruction to capture and store the value that we can access at a later time. Scenario based tests Business scenarios often involve multiple dependencies, as described above. However, for the scenario we have a larger scale process that we need to confirm is working. Automation provides an opportunity to link the various functions that are tested across a business scenario. Automation can help synchronize the starting of tests, passing of data from one test to another, aggregating test results, and monitoring systems while the automated test is running. For example, when testing month-end or quarter-end close activities there are a number of processes that need to be done, in a specific order, and dependent upon each other, in order that the final month-end reporting is correct and accurate.","title":"6.1 What Makes Sense to Automate"},{"location":"migrating/#62-when-should-automation-occur","text":"Ideally, automation of tests should occur once we have a strong understanding of what needs to be tested, when have defined test cases, and when we've developed test data for those test cases. When we've achieved the above, we've likely already tested once or more times manually in order to ensure that our test definitions are correct. This initial manual test activity can save much time for the test automation engineer as that individual will now have valid test conditions and data to work with in developing automated tests. For traditional development methodologies, this would occur after the initial software release and automation would be most helpful in building out a regression test bed. For Agile projects, the automation will likely be scheduled as a 1- or 2-sprint offset as individual sprints themselves do not often allow adequate time to build out the test automation architecture and framework. Automation in Agile becomes a work-in-progress where each successive sprint sees additional functionality and maturation added to the test automation solution.","title":"6.2 When Should Automation Occur"},{"location":"migrating/#63-criteria-for-creating-new-automated-tests","text":"There may be an opportunity to immediately automate tests, rather than prepare for automation with manual tests. In order for this to happen, the test automation framework has to be sufficiently developed to allow its use for new tests. The timing of this is dependent on the complexity of the application and components that need to be tested. For example, if an application has 200 different controls spread across 5 functional areas, the framework must be capable of identifying and interacting with each of those controls, so that no test is off limits for development. If the 200 controls can be mapped out against the 5 functional areas, then as long as the functional areas under test have been verified and are compatible automation can proceed. Often automation teams do not do their due diligence to find all controls and ensure that the test automation solution indeed is compatible with every one of them. It only takes 1 error in trying to interact with a control for testing to fail, thus undermining the automated approach.","title":"6.3 Criteria for Creating New Automated Tests"},{"location":"migrating/#64-criteria-for-converting-manual-tests-to-automation","text":"Most projects have documented test cases and scripts defining the test steps. When evaluating the migration of tests to automation, the following should be considered: Is the test necessary? Often, tests that were developed a long time ago no longer provide the value they once did as applications evolve and other newer tests may already cover parts of the functionality from older tests. When examining existing tests, evaluate if any or all of a test's functions may already be present in other tests. Is the test complete? Does the test exercise the complete business process so that it provides relevant, meaningful results? If not, the business analyst, manual tester, and test automation engineer will likely need to work together to enhance the capability of the automated test. Is the test effective? Effectiveness for any test can be measured by coverage to the requirement and coverage to the code. Specification based testing techniques help us determine how to construct data sets that will provide us with the least amount of test cases that can generate the highest level of coverage, thus mitigating deployment risk. Code coverage tools can be run by developers in parallel to test execution to verify the code coverage. Automation provides us the vehicle by which additional quality test cases can now be executed which previously were not possible, due to time and resource constraints, under a manual scenario. Is the test efficient? Test efficiency can take on several aspects, from speed of execution to overall construction. Automation by itself will always execute a test faster than can be done manually. However, not all manual tests have been designed in a way that is efficient for automation to process them. For example, for many applications, testing follows a prescribed path through an application. The path of navigation is what allows us to test getting from, say, the login screen to the reports screen. Some applications can have complex or lengthy paths before even getting to test for data. With automation we have the opportunity to create navigational tests that can be combined with functional tests in a manner that allows for reuse. Regarding test construction, the approach used to develop a manual test will often differ from the approach to develop an automated test. This is due to the different ways in which each test is executed. Testers think and act linearly, while computers can be made to jump from one area to another. Decomposition of manual tests and reconstruction is a worthwhile effort in order to maximize the quality of automated tests produced. Depending on how manual tests have been created, they may be: Broken apart and reconstituted across various automated tests Combined into one larger automated test Converted one-for-one as an automated test","title":"6.4 Criteria for Converting Manual Tests to Automation"},{"location":"migrating/#65-transitioning-staff-to-automation","text":"Since the advent of testing tools there has been talk that automation kills testing jobs. For large programs heavily dependent on manual testers we would expect to see a reduction of an inefficient process. However, automation mostly helps testers increase their focus on developing quality tests rather than on time consuming \"key pounding\" test execution activity. It is very important to make sure staff understand that they are valued for their knowledge of systems and their testing acumen. It is also important to acknowledge that in order for automation to be successful, there are different roles, and not all roles require programing skills. Section 3.5 describes the various roles necessary for the tasks required to make automation successful. Outsourced solutions Test automation solutions can be contracted out to firms knowledgeable in creating and maintaining test automation frameworks. In this scenario, a firm would be contracted to assist in automating all or part of a system. Once the capability is developed the government would request documentation, training, and transitioning as part of the final deliverable. Government developed solutions Government developed automation would require the selection of staff with the appropriate skills and training so that the process of building out automation does not become a learn-as-you-go exercise, which ultimately will not show a high return on investment. Appendix A covers many resources that can provide the necessary knowledge for team members. Acquiring automation assets In addition to the wide range of test automation tools described in Section 5.3.1 and listed in Appendix B there exist pre-made frameworks that can be used with existing tools. These frameworks can offer a shortcut to a build-from-scratch strategy, and often one can continue to build additional functionality to the base framework libraries. Regardless of the solution used in migrating to automation, a tool-agnostic approach that abstracts test data from test tools provides the greatest flexibility and overall longevity.","title":"6.5 Transitioning Staff to Automation"},{"location":"reporting/","text":"9 Reporting Test reporting is an important part of the process of automated testing. There are a number of different ways by which we can produce reports from automation. Test execution tools Test execution tools represent the most critical component in reporting. It is at the execution of tests that data can be captured which is then used for reporting. Test execution tools will often have one reporting mechanism that shows that occurred during the test execution, including any verification steps and any failure points. The reporting provided by these tools is very focused to the sequence of events that occurred and often do not capture larger trends. Execution tools often have the ability to customize the reporting, often by inserting messages and custom output text to complement default reporting. Additionally, execution tools may have separate logs that show if any errors occurred as functions were called. Custom logs can be created in order to meet specific output requirements, like when data needs to be imported into other tools in a standardized format. Test management tools Test management tools often help to store and aggregate data and can report on an individual or group of tests. This is helpful in order to see trends in application quality. Management tools often have several mechanisms for reporting including standard table reports, summary table reports, and graphical data representations. These can include charts, graphs, and other visual elements that facilitate analyzing high level data without having to look at the more granular data. Test reporting tools Reporting tools exist that are not specific to testing but that can take a variety of input files (e.g. xls, cvs, etc.) and aggregate results for meaningful display. There is overlap with the management tools on some reports. However, reporting tools can also include project management tools that help with scheduling activities and task interdependencies. As tools have a diverse set of reporting functionality, it may take more than one tool to get all reporting requirements in place. Therefore, understanding each tool's import/export capabilities and requirements will ensure that data can freely flow from one reporting tool to another.","title":"-9 Reporting"},{"location":"reporting/#9-reporting","text":"Test reporting is an important part of the process of automated testing. There are a number of different ways by which we can produce reports from automation. Test execution tools Test execution tools represent the most critical component in reporting. It is at the execution of tests that data can be captured which is then used for reporting. Test execution tools will often have one reporting mechanism that shows that occurred during the test execution, including any verification steps and any failure points. The reporting provided by these tools is very focused to the sequence of events that occurred and often do not capture larger trends. Execution tools often have the ability to customize the reporting, often by inserting messages and custom output text to complement default reporting. Additionally, execution tools may have separate logs that show if any errors occurred as functions were called. Custom logs can be created in order to meet specific output requirements, like when data needs to be imported into other tools in a standardized format. Test management tools Test management tools often help to store and aggregate data and can report on an individual or group of tests. This is helpful in order to see trends in application quality. Management tools often have several mechanisms for reporting including standard table reports, summary table reports, and graphical data representations. These can include charts, graphs, and other visual elements that facilitate analyzing high level data without having to look at the more granular data. Test reporting tools Reporting tools exist that are not specific to testing but that can take a variety of input files (e.g. xls, cvs, etc.) and aggregate results for meaningful display. There is overlap with the management tools on some reports. However, reporting tools can also include project management tools that help with scheduling activities and task interdependencies. As tools have a diverse set of reporting functionality, it may take more than one tool to get all reporting requirements in place. Therefore, understanding each tool's import/export capabilities and requirements will ensure that data can freely flow from one reporting tool to another.","title":"9 Reporting"},{"location":"roles/","text":"3 Roles in Test Automation 3.1 Acquisitions Automated testing is an integral part of modern software development. As such, requirements for automated testing should be identified in the requirement document (e.g., Statement of Objective or Statement of Work). It is essential to establish best practices as requirements at the onset of all new software acquisitions, not only to ensure they are delivered during execution, but to ensure quality vendors respond. The DoD 5000 already prescribes such requirements. Specifically, DoD Instruction (DoDI) 5000.02 defines, under DT&E Planning Considerations, the requirement to \"develop a software test automation strategy\" and under OT&E for software, for regression tests to be \"preferably automated.\" This policy, now superseded by policy 5000.75 for business systems and implemented with AFMAN 63-144, includes a directive to \"Employ effective use of integrated testing and automated software test tools.\" Acquisition guidelines should be stated at the objective level, however, they should allow for the ability for the contractor to recommend industry best testing tools that may be implemented with Government approval. The goal is to obtain the best solution for the program and the Government. Many contracting organizations with automation skills have honed the techniques necessary to deliver quality, reliable automated test solutions. If it is envisioned that government will take over the use and management of the automated testing suite, guidelines for training and transitioning of the solution should be required as part of the Statement of Objective or Statement of Work. If a different technology is envisioned for any continuation of test automation, then the test data used to drive the automation should be delivered in a standardized manner that allows for reusability and adaptability to another automated solution. 3.2 Management Support The role of management, at all levels, is key to the success of test automation in AF programs. Management can: Identify projects and programs where automation would likely provide benefits to the overall testing process Identify staff (government or contractors) who can be targeted to deliver automated solutions Identify relevant training and certification to ready staff for automation Anticipate funding requirements for test automation resources (people, tools, environments, process adjustment) Ensure that an adequate assessment of test tools takes place through market research and evaluation Provide equipment and environments in which to develop and execute automation Adopt cross-enterprise \"Best Practices\" for sharing of test automation methods and technology Define contract structures and CLINs that promote use of automation 3.2.1 Identifying and Funding Resources From a funding perspective, there are three areas a manager should consider when planning for test automation. People Who will be tasked to do the automation? And who will be implementing the testing tools framework? Government staff? Contractors? A combination? This needs to be decided on up front as it will affect the process by which these resources are identified and the timeframe under which they can be brought in to accomplish the work, including any training time required prior to project start. If the automation skillset is not easily found within government, a first step might be to contract the work out to an organization with expertise in this area. This will save time and money, and avoid missteps. Test Tools Software test tools have costs associated with licenses, maintenance, training, and support. Even tools that are open source software will require maintenance, training, and possibly support. However, the absence of an initial license fee may provide a significant cost savings (see section 6.3). This playbook will describe industry standard testing tools and the trade-offs between open source versus Commercial-Off-The-Shelf (COTS) software. Test Environments Test environments where automated tools reside can include: - An individual tester's workstation - A test lab with specific computers dedicated to automation - A server with virtual machine images - A cloud-based setup - A cloud-based service (SaaS) 3.3 Technical Support Developer The software developer plays a key role in supporting the test automation team. The developer has first-hand knowledge of the tools and methods used to construct the software and system. This information will help guide the automation team in the evaluation and selection of tools that are compatible with the tools selected by the development team. Often developers can further aid the automation effort (and associated maintenance) by using uniquely identifiable names for objects/controls displayed by the software application. This is equally applicable to client-based or browser-based solutions. The key point here is that a little forethought by the developers can go a long way to facilitate the recognition of objects/controls by the automation team. For example, we avoid a common scenario where the properties of an application for a user \"name\" and user \"account\" show up as U25523 and A00056 within the automation tool rather than USER_NAME and USER_ACCOUNT. Database Administrator Data forms a large part of tests, and test automation amplifies this. The role of the Database Administrator is key in assisting the needs of the automation team. These may include: Assistance in configuring and selecting a database which emulates a production-like database The ability to restore or revert a database to an earlier condition for retesting Assistance in executing direct queries against the database in order to validate application behavior Systems Administrator The System Administrator (SA) ensures that the system, including software, network, and interfaces are available to the test automation team. Additionally, the SA controls the updates (patches, security releases, etc.) that are applied to the servers on which the test automation solution runs. This coordination is very important as any changes to the underlying system may have consequences to the reliability of the test automation solution, with the possibility that it ceases to function. The SA will also help the automation team with any updates to automation software that need to installed on the testing infrastructure and can assist the test automation team by providing an environment that emulates a production-like environment. 3.4 Domain Support The Business User, Business Analyst, Product Owner, and other similar roles are subject matter experts (SMEs) when it comes to understanding how the software should work and what it needs to do in order to meet stated requirements, objectives and defined user stories. Domain knowledge is hard to come by and usually comes from individuals who have had or continue to have direct roles in using the business functionality that a system provides. These are the go-to people when a thorough understanding of use cases is required. 3.5 Automation Team Members The roles of the core automation team are important to define up front. The individuals filling those roles should have the necessary skills and experience to properly implement a maintainable, expandable automation solution. They should also be current on industry standards and have the ability to provide recommendations for changes based on the environment and user needs. The following roles can be assigned to individuals or could be performed by one or more individuals, depending on the complexity of the software project. Test Automation Architect The Automation Architect is the senior Subject Matter Expert (SME) in automation and is responsible for the overall design and implementation of a test automation solution. The automation architecture will be dependent on many factors, including: complexity of the system or software under test (SUT); number of interfaces to other systems or subsystems; richness of the Integrated Development Environment (IDE) controls; and technical level of automation team. The Architect needs to have a broad vision of what current and forthcoming requirements for automation may be based on for overall system architectures. Selecting appropriate tools to meet a diverse set of needs and understanding how multiple tools may need to be integrated for complex testing and reporting requirements will need to be considered as part of the planning process. Test Automation Engineer The Test Automation Engineer is an intermediate-level technical individual who is responsible for developing and maintaining automation components and subsystems. This may include development of purpose-built functions, creation of function libraries, and documentation of the test automation components. As new requirements for automation are defined (e.g. a new \"calendar\" control being added) the automation engineer makes the appropriate updates/additions to the test automation solution, including documentation, to incorporate the new functionality. There may be multiple roles for an automation engineer which may include: Development of input/output (I/O) functions Development of test interfaces to external systems Development of user interface (UI) component test functions Development of navigational paradigms across the application Development of timing and synchronization requirements Development of logging and reporting functions Test Automation User The Test Automation User is the individual or individuals who are the \"customer\" of the test automation solution. They need not be concerned with the technical implementation of a test automation solution, but rather with the ability to use automation to execute tests and report findings. A properly designed test automation solution will allow a user to select a test, the corresponding data set, and run the test. At the conclusion of the test execution, the user should be presented with a report indicating the pass/fail status of the test and any information gathered as a result of a passed/failed test. Often the user of test automation will be a manual tester, a business analyst, or another individual with a strong understanding of the software or system and the requirements that it needs to meet. These individuals may not have programming backgrounds and would not be productive or motivated to take on the role of an automation engineer. By identifying and assigning roles based on skills we are able to keep each individual fully productive and motivated and allow automation to be used by the team, not just by select technical individuals.","title":"-3 Roles in Test Automation"},{"location":"roles/#3-roles-in-test-automation","text":"","title":"3 Roles in Test Automation"},{"location":"roles/#31-acquisitions","text":"Automated testing is an integral part of modern software development. As such, requirements for automated testing should be identified in the requirement document (e.g., Statement of Objective or Statement of Work). It is essential to establish best practices as requirements at the onset of all new software acquisitions, not only to ensure they are delivered during execution, but to ensure quality vendors respond. The DoD 5000 already prescribes such requirements. Specifically, DoD Instruction (DoDI) 5000.02 defines, under DT&E Planning Considerations, the requirement to \"develop a software test automation strategy\" and under OT&E for software, for regression tests to be \"preferably automated.\" This policy, now superseded by policy 5000.75 for business systems and implemented with AFMAN 63-144, includes a directive to \"Employ effective use of integrated testing and automated software test tools.\" Acquisition guidelines should be stated at the objective level, however, they should allow for the ability for the contractor to recommend industry best testing tools that may be implemented with Government approval. The goal is to obtain the best solution for the program and the Government. Many contracting organizations with automation skills have honed the techniques necessary to deliver quality, reliable automated test solutions. If it is envisioned that government will take over the use and management of the automated testing suite, guidelines for training and transitioning of the solution should be required as part of the Statement of Objective or Statement of Work. If a different technology is envisioned for any continuation of test automation, then the test data used to drive the automation should be delivered in a standardized manner that allows for reusability and adaptability to another automated solution.","title":"3.1 Acquisitions"},{"location":"roles/#32-management-support","text":"The role of management, at all levels, is key to the success of test automation in AF programs. Management can: Identify projects and programs where automation would likely provide benefits to the overall testing process Identify staff (government or contractors) who can be targeted to deliver automated solutions Identify relevant training and certification to ready staff for automation Anticipate funding requirements for test automation resources (people, tools, environments, process adjustment) Ensure that an adequate assessment of test tools takes place through market research and evaluation Provide equipment and environments in which to develop and execute automation Adopt cross-enterprise \"Best Practices\" for sharing of test automation methods and technology Define contract structures and CLINs that promote use of automation","title":"3.2 Management Support"},{"location":"roles/#321-identifying-and-funding-resources","text":"From a funding perspective, there are three areas a manager should consider when planning for test automation. People Who will be tasked to do the automation? And who will be implementing the testing tools framework? Government staff? Contractors? A combination? This needs to be decided on up front as it will affect the process by which these resources are identified and the timeframe under which they can be brought in to accomplish the work, including any training time required prior to project start. If the automation skillset is not easily found within government, a first step might be to contract the work out to an organization with expertise in this area. This will save time and money, and avoid missteps. Test Tools Software test tools have costs associated with licenses, maintenance, training, and support. Even tools that are open source software will require maintenance, training, and possibly support. However, the absence of an initial license fee may provide a significant cost savings (see section 6.3). This playbook will describe industry standard testing tools and the trade-offs between open source versus Commercial-Off-The-Shelf (COTS) software. Test Environments Test environments where automated tools reside can include: - An individual tester's workstation - A test lab with specific computers dedicated to automation - A server with virtual machine images - A cloud-based setup - A cloud-based service (SaaS)","title":"3.2.1 Identifying and Funding Resources"},{"location":"roles/#33-technical-support","text":"Developer The software developer plays a key role in supporting the test automation team. The developer has first-hand knowledge of the tools and methods used to construct the software and system. This information will help guide the automation team in the evaluation and selection of tools that are compatible with the tools selected by the development team. Often developers can further aid the automation effort (and associated maintenance) by using uniquely identifiable names for objects/controls displayed by the software application. This is equally applicable to client-based or browser-based solutions. The key point here is that a little forethought by the developers can go a long way to facilitate the recognition of objects/controls by the automation team. For example, we avoid a common scenario where the properties of an application for a user \"name\" and user \"account\" show up as U25523 and A00056 within the automation tool rather than USER_NAME and USER_ACCOUNT. Database Administrator Data forms a large part of tests, and test automation amplifies this. The role of the Database Administrator is key in assisting the needs of the automation team. These may include: Assistance in configuring and selecting a database which emulates a production-like database The ability to restore or revert a database to an earlier condition for retesting Assistance in executing direct queries against the database in order to validate application behavior Systems Administrator The System Administrator (SA) ensures that the system, including software, network, and interfaces are available to the test automation team. Additionally, the SA controls the updates (patches, security releases, etc.) that are applied to the servers on which the test automation solution runs. This coordination is very important as any changes to the underlying system may have consequences to the reliability of the test automation solution, with the possibility that it ceases to function. The SA will also help the automation team with any updates to automation software that need to installed on the testing infrastructure and can assist the test automation team by providing an environment that emulates a production-like environment.","title":"3.3 Technical Support"},{"location":"roles/#34-domain-support","text":"The Business User, Business Analyst, Product Owner, and other similar roles are subject matter experts (SMEs) when it comes to understanding how the software should work and what it needs to do in order to meet stated requirements, objectives and defined user stories. Domain knowledge is hard to come by and usually comes from individuals who have had or continue to have direct roles in using the business functionality that a system provides. These are the go-to people when a thorough understanding of use cases is required.","title":"3.4 Domain Support"},{"location":"roles/#35-automation-team-members","text":"The roles of the core automation team are important to define up front. The individuals filling those roles should have the necessary skills and experience to properly implement a maintainable, expandable automation solution. They should also be current on industry standards and have the ability to provide recommendations for changes based on the environment and user needs. The following roles can be assigned to individuals or could be performed by one or more individuals, depending on the complexity of the software project. Test Automation Architect The Automation Architect is the senior Subject Matter Expert (SME) in automation and is responsible for the overall design and implementation of a test automation solution. The automation architecture will be dependent on many factors, including: complexity of the system or software under test (SUT); number of interfaces to other systems or subsystems; richness of the Integrated Development Environment (IDE) controls; and technical level of automation team. The Architect needs to have a broad vision of what current and forthcoming requirements for automation may be based on for overall system architectures. Selecting appropriate tools to meet a diverse set of needs and understanding how multiple tools may need to be integrated for complex testing and reporting requirements will need to be considered as part of the planning process. Test Automation Engineer The Test Automation Engineer is an intermediate-level technical individual who is responsible for developing and maintaining automation components and subsystems. This may include development of purpose-built functions, creation of function libraries, and documentation of the test automation components. As new requirements for automation are defined (e.g. a new \"calendar\" control being added) the automation engineer makes the appropriate updates/additions to the test automation solution, including documentation, to incorporate the new functionality. There may be multiple roles for an automation engineer which may include: Development of input/output (I/O) functions Development of test interfaces to external systems Development of user interface (UI) component test functions Development of navigational paradigms across the application Development of timing and synchronization requirements Development of logging and reporting functions Test Automation User The Test Automation User is the individual or individuals who are the \"customer\" of the test automation solution. They need not be concerned with the technical implementation of a test automation solution, but rather with the ability to use automation to execute tests and report findings. A properly designed test automation solution will allow a user to select a test, the corresponding data set, and run the test. At the conclusion of the test execution, the user should be presented with a report indicating the pass/fail status of the test and any information gathered as a result of a passed/failed test. Often the user of test automation will be a manual tester, a business analyst, or another individual with a strong understanding of the software or system and the requirements that it needs to meet. These individuals may not have programming backgrounds and would not be productive or motivated to take on the role of an automation engineer. By identifying and assigning roles based on skills we are able to keep each individual fully productive and motivated and allow automation to be used by the team, not just by select technical individuals.","title":"3.5 Automation Team Members"},{"location":"scopetest/","text":"4 Scope of Test Automation 4.1 Tools Target Areas Test automation tools support the full software development lifecycle. There is likely a tool for every aspect of software that is developed. Tools are categorized by types of testing they can perform. Test types can span multiple test levels (e.g. component, integration, system, etc.). The following represents broad categories of functionality that test tools are capable of. 4.1.1 Functional & Regression Testing By far the most common use of automation is to test application functionality. This can include new functionality added in a release or retesting of existing functionality. In both cases the goal is to use automation to test the functionality of an application or system. Applications use a variety of user interfaces. Most common today is the graphical user interface (GUI) that make up a large proportion of existing interfaces. All modern computers, tablets, and smartphones use a GUI (e.g. Windows, MacOS, Android, iOS, etc.). Historically, interfaces to systems were very crude and text based (e.g. Mainframe, Terminal, etc.). Some continue to be that way as the text based interface has very low overhead required by some applications or underlying hardware. Interface (UI or GUI) testing test tools must be able to recognize the interface, graphical or otherwise, and the fields or objects on that interface. Where there is a standard implementation of the interface the test tool can easily recognize and interact with the fields or objects. For fully graphical interfaces there are a number of significant elements which can be tested or verified for each object or control. For example, a simple text field where a user enters a name can have many attributes: value (what data is in the field); focus (is this the field where the cursor is at); enabled (can we interact with this field); etc. Challenges : Not all interfaces are implemented consistently and some objects within an application may not be recognized by automation Use of third-party controls embedded into an interface can cause challenges in automation Some automation may need to be custom-coded. This requires programming skill and knowledge that should be performed by individuals with this level of experience. Migrating from one tool to another can pose challenges Adapting to changing technology (browser, OS, etc.) requires tools that are compatible Best Practices : Understand the full suite of technology that is being implemented for the SUT. Don't take a vendor or a reviewer as the last word on compatibility without the using the tool in its intended environment to ensure it is compatible. Look at tools that are compatible with the level of skill in the organization 4.1.2 API Testing Application Programming Interface (API) testing is testing that does not involve a user interface (e.g., text or graphical interface). This is often referred to as client-less testing or non-GUI testing. It simply means that there is no direct user interface by which the tester interacts. However, data is still passed to the application, the application responds, and these interactions can be measured, verified, and reported on with automated test tools. Most applications that have a UI or GUI also are communicating at the API level. With API testing, rather than interacting with an object or control, we interact with a function or service. In order to do so we need to understand the structure of data that the function or service is expected to respond with. If we send the wrong data structure will it identify it as an error or do something unexpectedly? If we send data and receive no return data, what does that tell us? There needs to be a full understanding of what the function or service is supposed to do so that verification can be effective. This includes an understanding of knowing what the function or service is not supposed to do as well. Challenges : Working with functions and services can be a little like working in the dark because you don't have the familiar context of UI controls Tools that are compatible with the implemented services Tools that can test APIs across devices and operating systems APIs may degrade in performance with multiple simultaneous calls to them Best Practices : Understand what you are testing and get the full specification before you start Make sure that you change only one variable at a time to understand the effect of changes Have some known baseline test interfaces to start with in order to realize the expected behavior Work closely with the API developer or vendor who implemented the function or service to fully understand the expected behavior and to report any unusual or missing return data values. 4.1.3 Performance Testing Performance testing is not concerned directly with testing functionality, but rather testing that functionality under load. Hence it is categorized as non-functional testing. Performance testing is difficult to impossible to do without test tools. In years past, rooms full of people and computers attempted to do performance testing but the cost was high, the consistency was low, and the breadth of testing left much to be desired. Modern performance testing tools simulate those same people on those same computers. Now, however, all the people and all the computers can be contained within one powerful computer. For extremely large simulations with heavy loads, performance testing can be distributed across multiple computers. Performance testing tools have 3 areas of functionality: Main controller The controller function starts tests, ramps them up or down, and stops tests. The controller function schedules the testing event and runs the operational profiles (often referred to as a scenarios) to simulate the conditions of a system in use. User Script The user script is the recorded or programmed sequence of events, or operational profile, that mimic the way in which a user interacts with a system. Running the same user script concurrently is how the test tool simulates multiple users performing the same function. User scripts often contain timing information (or wait statements as they are often called) that help regulate the speed of execution to something representative of a human user. Otherwise, scripts could run at speeds far faster than how real users interact with a system, which would not accurately represent the performance of the system. Reporting and Analysis Performance test tools need to have robust reporting and analysis capabilities in order to accurately convey what has transpired. Reporting will include transactional timing information (e.g., how long it took for a search to return) and aggregate data (e.g., all searches on average took no more than 3 seconds to complete). Reporting can also include system resource utilization (e.g., caching, CPU, queues, etc.) and analysis can help correlate how system resources are impacted by heavy transactional use. Cross analysis reporting aides in comparing subsequent test runs in order to evaluate effects on changes. Challenges : Finding an environment that mimics production is difficult and expensive Test tools can be very expensive Performance testing can affect other users sharing that environment Improperly created setup tests may yield no errors when actual errors exist Erroneous results can result from lack of proper correlation or using the wrong database Best Practices : Work with stakeholders to accurately define the various operational profiles Do not make multiple changes between tests as it makes it difficult to know each impact Test each operational profile individually and under load first before adding other profiles Keep scripts and data updated to reflect current changes in SUT Look at the test logs for errors that may not be present in the reports 4.1.4 Security Testing Security testing tools help analyze and test applications for possible security vulnerabilities. There are 3 areas of interest for security testing: static, dynamic, and interactive [ref: Gartner Magic Quadrant for Application Security Testing]. Static analysis refers to looking at the application code from the inside, not when executed. Dynamic analysis is performing testing against a running application. There is overlap with the security qualities identified in static and dynamic security testing and interactive tests span both. Therefore it is advisable to begin security testing early in the software development lifecycle, as code is developed so that it can be assessed. Static Application Security Testing Static application security testing (SAST) of an application refers to analyzing the underlying application code for vulnerabilities. This may include source code, byte code, or binaries. Due to the visibility of the code these tests are often referred to as \"white box\" tests. Scanning of code can be accomplished before any code is compiled. Examining code can assist in uncovering vulnerabilities including SQL Injection, Buffer Overflows, Cross-Site Scripting, and Cross-Site Request Forgery. Dynamic Application Security Testing Dynamic Application Security Testing (DAST) provides applications that are running to be analyzed for possible security flaws. These may include traditional client server applications of web-based applications. DAST has no visibility to the underlying application and therefore is referred to as \"black box\" testing. Interactive Application Security Testing Interactive Application Security Testing (IAST) allows visibility of possible vulnerabilities simultaneously within the application, often through the use of an agent within the runtime environment, and externally via application interfaces. This can include penetration testing which includes the scanning of ports for vulnerabilities. A set of complementary and overlapping vulnerabilities can be identified with SAST, DAST, and IAST tools at different phase of the software development lifecycle. However, automated tools cannot find all vulnerabilities so be careful to not get a false sense of security. A comprehensive guide for cyber security testing can be found at [ref: DoD Cybersecurity Test and Evaluation Guidebook, version 2.0] Blue team - penetration test. Red team - hacker to break into application. Red team in a box. Additional references for web application security available at: www.owasp.org. Challenges : Getting developers to use SAST tools early on Tools may have difficulty parsing through code Tools will not find all vulnerabilities Understand the tool capabilities and how to properly set up for testing Best Practices : Use tools regularly during development before code base becomes to large Security folks being proactive and helping program early Run tools on test and production environments Use a combination of best-in-class tools Identify and perform tests manually which tools may miss Decide what tests need to be run through automation. Automate when necessary as setup may take a lot of effort 4.1.5 Test Management Test Management tools provide for collaboration and reporting of test events. These tools can be a one-stop repository of manual and automated tests, documentation, data files, and more. Test Management tools should provide a level of configuration management for the many test artifacts in use. Additionally, these tools should allow for the classification of a test event by version so that all components used for a test can be traced back to each individual component version and state. This is extremely important as it's not uncommon to have to revert to an earlier release of testing should the current release find problems. Test Management tools often include a defect management module that allows manual and automated tests to create entries for defects. The defects can be assigned to owners and alerts can be configured to inform owners of the stages the defect is going through (e.g. identification, remediation, re-test, etc.) Challenges : Identifying a solution that meets all your needs (e.g. requirements traceability, test case storage, defect cataloging, version control, etc.) Getting buy-in from team to keep data current Best Practices : Plan out what features, functions, and customization will be necessary for your organization Make fields of important information required entry or selectable to aid in queries searching and reporting Produce targeted reporting and dashboards to meet stakeholder needs Identify areas of commonality to produce consolidated reporting (RTM). Combining so everybody sees the same source of truth. Allow ability to run canned reports as needed. 4.2 Test Levels Government systems cover a broad range of testing levels. Some of these are contractor-focused test deliverables. Others are joint contractor-government, and yet others are government-only tests. Automation can be used at many test levels but provides greater value at lower levels. The following table summarizes the test levels, methods, role, and feasibility of automation: White Box White Box texting defines a testing approach that provides visibility to the program code as part of the test event. This allows the tester to understand the underlying structure of the software system or component under test. Testing at this level exposes design and implementation methods (and potential flaws) used to construct the software. The goal to understand and validate the individual lines of code, to the extent possible. Software construction with excessive levels of branching or nesting often cannot be easily tested. Black Box Black Box testing defines an approach to testing that focuses on the functionality of the software under test. The tester exercises the software much as a user would interact with the software and thus attempts to validate if the expected features and functionality have been properly implemented, based on a specification or user story. This technique complements the White Box approach as the system is tested from two different perspectives. Gray Box Gray Box testing combines elements of White and Black Box testing in order to simultaneously identify any defects in application usage that are due to poorly implemented coding constructs. Feasibility of Automation Software Tests Automation is generally easier to implement earlier in smaller, contained, software components. Developers can make good use of automation to test incremental code updates, functions, and components. There are many tools for developers that not only automate test execution but that also provide assessments of the code quality and complexity. These should be run first while modules are still manageable rather than when software has been integrated and combined with other systems, where analysis becomes much more difficult on a significantly larger code base. System Tests Automated system tests verify and validate system behavior. Although there may be some reuse from automation at the earlier software test level, system tests by and large are Black Box tests where the activity is focused on functional requirements that the software must meet. Often these will focus more on testing the system through interfaces (graphical user interface, API, etc.). Mission Tests As the system grows and becomes integrated with other systems, automation can still play a part. For example in a System-of-Systems test automation can be used to drive the individual subsystems comprising the System-of-Systems solution. This automation solution will help coordinate or synchronize the execution of various systems under test. However, this is a broad vs deep application of test automation, as it pertains to functionality. Finally, as systems are migrated to a production environment we lose the ability to test and automate. 4.3 Test Types Functional Tests Functional tests are those tests that evaluate if a system is performing according to a specification or requirement, often detailed in a use case or user story. Functional tests look at the system behavior as a guide to what the system should be do. Non-Functional Tests Non-Functional Tests evaluate the characteristics of a system and include performance testing, security testing, and other testing that does not directly test system functionality. However, as in the case of performance testing, we do exercise a functional test under load to examine its characteristics for the purposes of acquiring timing/performance information. Structural/architectural Structural/architectural testing is a white box testing activity which is concerned with coverage of code within modules and functions. The structure is tested to meet all possible conditions that the code provides for. Any code not exercised from the tests would require specific test conditions to be created, thus improving the overall coverage of code within the module or function. Retesting/regression When changes are made to a system or component, testing should be done in order to ensure the system or component continues to operate as expected. This can include testing done after a defect has been fixed to ensure that the fix was applied correctly, or testing done as a result new features added to the system and where existing features need to continue to operate as expected. Each these test types can be executed across test levels, where applicable. Also, there may be opportunities to reuse tests if designed as part of the automation strategy. For example, tests developed at the system test level may be re-used at the mission test level.","title":"-4 Scope of Test Automation"},{"location":"scopetest/#4-scope-of-test-automation","text":"","title":"4 Scope of Test Automation"},{"location":"scopetest/#41-tools-target-areas","text":"Test automation tools support the full software development lifecycle. There is likely a tool for every aspect of software that is developed. Tools are categorized by types of testing they can perform. Test types can span multiple test levels (e.g. component, integration, system, etc.). The following represents broad categories of functionality that test tools are capable of.","title":"4.1 Tools Target Areas"},{"location":"scopetest/#411-functional-regression-testing","text":"By far the most common use of automation is to test application functionality. This can include new functionality added in a release or retesting of existing functionality. In both cases the goal is to use automation to test the functionality of an application or system. Applications use a variety of user interfaces. Most common today is the graphical user interface (GUI) that make up a large proportion of existing interfaces. All modern computers, tablets, and smartphones use a GUI (e.g. Windows, MacOS, Android, iOS, etc.). Historically, interfaces to systems were very crude and text based (e.g. Mainframe, Terminal, etc.). Some continue to be that way as the text based interface has very low overhead required by some applications or underlying hardware. Interface (UI or GUI) testing test tools must be able to recognize the interface, graphical or otherwise, and the fields or objects on that interface. Where there is a standard implementation of the interface the test tool can easily recognize and interact with the fields or objects. For fully graphical interfaces there are a number of significant elements which can be tested or verified for each object or control. For example, a simple text field where a user enters a name can have many attributes: value (what data is in the field); focus (is this the field where the cursor is at); enabled (can we interact with this field); etc. Challenges : Not all interfaces are implemented consistently and some objects within an application may not be recognized by automation Use of third-party controls embedded into an interface can cause challenges in automation Some automation may need to be custom-coded. This requires programming skill and knowledge that should be performed by individuals with this level of experience. Migrating from one tool to another can pose challenges Adapting to changing technology (browser, OS, etc.) requires tools that are compatible Best Practices : Understand the full suite of technology that is being implemented for the SUT. Don't take a vendor or a reviewer as the last word on compatibility without the using the tool in its intended environment to ensure it is compatible. Look at tools that are compatible with the level of skill in the organization","title":"4.1.1 Functional &amp; Regression Testing"},{"location":"scopetest/#412-api-testing","text":"Application Programming Interface (API) testing is testing that does not involve a user interface (e.g., text or graphical interface). This is often referred to as client-less testing or non-GUI testing. It simply means that there is no direct user interface by which the tester interacts. However, data is still passed to the application, the application responds, and these interactions can be measured, verified, and reported on with automated test tools. Most applications that have a UI or GUI also are communicating at the API level. With API testing, rather than interacting with an object or control, we interact with a function or service. In order to do so we need to understand the structure of data that the function or service is expected to respond with. If we send the wrong data structure will it identify it as an error or do something unexpectedly? If we send data and receive no return data, what does that tell us? There needs to be a full understanding of what the function or service is supposed to do so that verification can be effective. This includes an understanding of knowing what the function or service is not supposed to do as well. Challenges : Working with functions and services can be a little like working in the dark because you don't have the familiar context of UI controls Tools that are compatible with the implemented services Tools that can test APIs across devices and operating systems APIs may degrade in performance with multiple simultaneous calls to them Best Practices : Understand what you are testing and get the full specification before you start Make sure that you change only one variable at a time to understand the effect of changes Have some known baseline test interfaces to start with in order to realize the expected behavior Work closely with the API developer or vendor who implemented the function or service to fully understand the expected behavior and to report any unusual or missing return data values.","title":"4.1.2 API Testing"},{"location":"scopetest/#413-performance-testing","text":"Performance testing is not concerned directly with testing functionality, but rather testing that functionality under load. Hence it is categorized as non-functional testing. Performance testing is difficult to impossible to do without test tools. In years past, rooms full of people and computers attempted to do performance testing but the cost was high, the consistency was low, and the breadth of testing left much to be desired. Modern performance testing tools simulate those same people on those same computers. Now, however, all the people and all the computers can be contained within one powerful computer. For extremely large simulations with heavy loads, performance testing can be distributed across multiple computers. Performance testing tools have 3 areas of functionality: Main controller The controller function starts tests, ramps them up or down, and stops tests. The controller function schedules the testing event and runs the operational profiles (often referred to as a scenarios) to simulate the conditions of a system in use. User Script The user script is the recorded or programmed sequence of events, or operational profile, that mimic the way in which a user interacts with a system. Running the same user script concurrently is how the test tool simulates multiple users performing the same function. User scripts often contain timing information (or wait statements as they are often called) that help regulate the speed of execution to something representative of a human user. Otherwise, scripts could run at speeds far faster than how real users interact with a system, which would not accurately represent the performance of the system. Reporting and Analysis Performance test tools need to have robust reporting and analysis capabilities in order to accurately convey what has transpired. Reporting will include transactional timing information (e.g., how long it took for a search to return) and aggregate data (e.g., all searches on average took no more than 3 seconds to complete). Reporting can also include system resource utilization (e.g., caching, CPU, queues, etc.) and analysis can help correlate how system resources are impacted by heavy transactional use. Cross analysis reporting aides in comparing subsequent test runs in order to evaluate effects on changes. Challenges : Finding an environment that mimics production is difficult and expensive Test tools can be very expensive Performance testing can affect other users sharing that environment Improperly created setup tests may yield no errors when actual errors exist Erroneous results can result from lack of proper correlation or using the wrong database Best Practices : Work with stakeholders to accurately define the various operational profiles Do not make multiple changes between tests as it makes it difficult to know each impact Test each operational profile individually and under load first before adding other profiles Keep scripts and data updated to reflect current changes in SUT Look at the test logs for errors that may not be present in the reports","title":"4.1.3 Performance Testing"},{"location":"scopetest/#414-security-testing","text":"Security testing tools help analyze and test applications for possible security vulnerabilities. There are 3 areas of interest for security testing: static, dynamic, and interactive [ref: Gartner Magic Quadrant for Application Security Testing]. Static analysis refers to looking at the application code from the inside, not when executed. Dynamic analysis is performing testing against a running application. There is overlap with the security qualities identified in static and dynamic security testing and interactive tests span both. Therefore it is advisable to begin security testing early in the software development lifecycle, as code is developed so that it can be assessed. Static Application Security Testing Static application security testing (SAST) of an application refers to analyzing the underlying application code for vulnerabilities. This may include source code, byte code, or binaries. Due to the visibility of the code these tests are often referred to as \"white box\" tests. Scanning of code can be accomplished before any code is compiled. Examining code can assist in uncovering vulnerabilities including SQL Injection, Buffer Overflows, Cross-Site Scripting, and Cross-Site Request Forgery. Dynamic Application Security Testing Dynamic Application Security Testing (DAST) provides applications that are running to be analyzed for possible security flaws. These may include traditional client server applications of web-based applications. DAST has no visibility to the underlying application and therefore is referred to as \"black box\" testing. Interactive Application Security Testing Interactive Application Security Testing (IAST) allows visibility of possible vulnerabilities simultaneously within the application, often through the use of an agent within the runtime environment, and externally via application interfaces. This can include penetration testing which includes the scanning of ports for vulnerabilities. A set of complementary and overlapping vulnerabilities can be identified with SAST, DAST, and IAST tools at different phase of the software development lifecycle. However, automated tools cannot find all vulnerabilities so be careful to not get a false sense of security. A comprehensive guide for cyber security testing can be found at [ref: DoD Cybersecurity Test and Evaluation Guidebook, version 2.0] Blue team - penetration test. Red team - hacker to break into application. Red team in a box. Additional references for web application security available at: www.owasp.org. Challenges : Getting developers to use SAST tools early on Tools may have difficulty parsing through code Tools will not find all vulnerabilities Understand the tool capabilities and how to properly set up for testing Best Practices : Use tools regularly during development before code base becomes to large Security folks being proactive and helping program early Run tools on test and production environments Use a combination of best-in-class tools Identify and perform tests manually which tools may miss Decide what tests need to be run through automation. Automate when necessary as setup may take a lot of effort","title":"4.1.4 Security Testing"},{"location":"scopetest/#415-test-management","text":"Test Management tools provide for collaboration and reporting of test events. These tools can be a one-stop repository of manual and automated tests, documentation, data files, and more. Test Management tools should provide a level of configuration management for the many test artifacts in use. Additionally, these tools should allow for the classification of a test event by version so that all components used for a test can be traced back to each individual component version and state. This is extremely important as it's not uncommon to have to revert to an earlier release of testing should the current release find problems. Test Management tools often include a defect management module that allows manual and automated tests to create entries for defects. The defects can be assigned to owners and alerts can be configured to inform owners of the stages the defect is going through (e.g. identification, remediation, re-test, etc.) Challenges : Identifying a solution that meets all your needs (e.g. requirements traceability, test case storage, defect cataloging, version control, etc.) Getting buy-in from team to keep data current Best Practices : Plan out what features, functions, and customization will be necessary for your organization Make fields of important information required entry or selectable to aid in queries searching and reporting Produce targeted reporting and dashboards to meet stakeholder needs Identify areas of commonality to produce consolidated reporting (RTM). Combining so everybody sees the same source of truth. Allow ability to run canned reports as needed.","title":"4.1.5 Test Management"},{"location":"scopetest/#42-test-levels","text":"Government systems cover a broad range of testing levels. Some of these are contractor-focused test deliverables. Others are joint contractor-government, and yet others are government-only tests. Automation can be used at many test levels but provides greater value at lower levels. The following table summarizes the test levels, methods, role, and feasibility of automation: White Box White Box texting defines a testing approach that provides visibility to the program code as part of the test event. This allows the tester to understand the underlying structure of the software system or component under test. Testing at this level exposes design and implementation methods (and potential flaws) used to construct the software. The goal to understand and validate the individual lines of code, to the extent possible. Software construction with excessive levels of branching or nesting often cannot be easily tested. Black Box Black Box testing defines an approach to testing that focuses on the functionality of the software under test. The tester exercises the software much as a user would interact with the software and thus attempts to validate if the expected features and functionality have been properly implemented, based on a specification or user story. This technique complements the White Box approach as the system is tested from two different perspectives. Gray Box Gray Box testing combines elements of White and Black Box testing in order to simultaneously identify any defects in application usage that are due to poorly implemented coding constructs. Feasibility of Automation Software Tests Automation is generally easier to implement earlier in smaller, contained, software components. Developers can make good use of automation to test incremental code updates, functions, and components. There are many tools for developers that not only automate test execution but that also provide assessments of the code quality and complexity. These should be run first while modules are still manageable rather than when software has been integrated and combined with other systems, where analysis becomes much more difficult on a significantly larger code base. System Tests Automated system tests verify and validate system behavior. Although there may be some reuse from automation at the earlier software test level, system tests by and large are Black Box tests where the activity is focused on functional requirements that the software must meet. Often these will focus more on testing the system through interfaces (graphical user interface, API, etc.). Mission Tests As the system grows and becomes integrated with other systems, automation can still play a part. For example in a System-of-Systems test automation can be used to drive the individual subsystems comprising the System-of-Systems solution. This automation solution will help coordinate or synchronize the execution of various systems under test. However, this is a broad vs deep application of test automation, as it pertains to functionality. Finally, as systems are migrated to a production environment we lose the ability to test and automate. 4.3 Test Types Functional Tests Functional tests are those tests that evaluate if a system is performing according to a specification or requirement, often detailed in a use case or user story. Functional tests look at the system behavior as a guide to what the system should be do. Non-Functional Tests Non-Functional Tests evaluate the characteristics of a system and include performance testing, security testing, and other testing that does not directly test system functionality. However, as in the case of performance testing, we do exercise a functional test under load to examine its characteristics for the purposes of acquiring timing/performance information. Structural/architectural Structural/architectural testing is a white box testing activity which is concerned with coverage of code within modules and functions. The structure is tested to meet all possible conditions that the code provides for. Any code not exercised from the tests would require specific test conditions to be created, thus improving the overall coverage of code within the module or function. Retesting/regression When changes are made to a system or component, testing should be done in order to ensure the system or component continues to operate as expected. This can include testing done after a defect has been fixed to ensure that the fix was applied correctly, or testing done as a result new features added to the system and where existing features need to continue to operate as expected. Each these test types can be executed across test levels, where applicable. Also, there may be opportunities to reuse tests if designed as part of the automation strategy. For example, tests developed at the system test level may be re-used at the mission test level.","title":"4.2 Test Levels"},{"location":"test/","text":"2 Test Automation 2.1 What is Automation? Automation, in its simplest form, is the mechanization of a manual process that allows for that process to operate automatically. There are many applications of automation, and there are many ways in which we can test. Using automation allows us to mechanize an otherwise manual process for testing. There are many additional uses for automation, that are not specifically for testing, that can be performed with automated test tools. Examples of these may include pre-test activities such as creating user accounts and building data sets, which will ultimately be used in automated testing. Functional and regression test activities are those most frequently targeted for the use of automated testing. Additional uses for automation of tests include API testing, performance testing, security testing, and automation of test management activities. 2.2 Automation in Software Lifecycle Methodologies Software development methodologies are evolving from traditional Waterfall to more recent Agile approaches. Testing is part of the overall software development process. When implementing testing automation, it must align with and conform to the project management methodology. In projects using Waterfall project management, cycles for development are long and the automation team can plan accordingly. This would include setting up many of the functions and components and building an automation framework. With Agile projects, there isn't as much time within each sprint to build out a complete automation framework so alternate solutions should be identified. 2.3 Benfits to AF The AF is continually enhancing, upgrading, and/or replacing software systems and applications to meet mission and user needs. This presents an ideal opportunity to introduce test automation practices so that future iterations of the software development process will show greater: testing efficiency through faster execution testing effectiveness through additional functional coverage test repeatability through programmed execution improvement in timely reporting of system quality 2.4 Avoiding Pitfalls Individuals involved in testing and test automation come from various walks of life. There is no Tester University or College of Automation. This creates a situation where there is a lack of consistency and knowledge that someone in this line of work should know in order to perform effectively. Over time there have been training and certification programs in DoD and industry that support the need for skills and knowledge of career testers and automators. However, success in automation is not solely a technical challenge. It is a multifaceted discipline requiring management, engineering, and other disciplines. This Test Automation Playbook will guide the AF on how to best approach automated testing.","title":"-2 Test Automation"},{"location":"test/#2-test-automation","text":"","title":"2 Test Automation"},{"location":"test/#21-what-is-automation","text":"Automation, in its simplest form, is the mechanization of a manual process that allows for that process to operate automatically. There are many applications of automation, and there are many ways in which we can test. Using automation allows us to mechanize an otherwise manual process for testing. There are many additional uses for automation, that are not specifically for testing, that can be performed with automated test tools. Examples of these may include pre-test activities such as creating user accounts and building data sets, which will ultimately be used in automated testing. Functional and regression test activities are those most frequently targeted for the use of automated testing. Additional uses for automation of tests include API testing, performance testing, security testing, and automation of test management activities.","title":"2.1 What is Automation?"},{"location":"test/#22-automation-in-software-lifecycle-methodologies","text":"Software development methodologies are evolving from traditional Waterfall to more recent Agile approaches. Testing is part of the overall software development process. When implementing testing automation, it must align with and conform to the project management methodology. In projects using Waterfall project management, cycles for development are long and the automation team can plan accordingly. This would include setting up many of the functions and components and building an automation framework. With Agile projects, there isn't as much time within each sprint to build out a complete automation framework so alternate solutions should be identified.","title":"2.2 Automation in Software Lifecycle Methodologies"},{"location":"test/#23-benfits-to-af","text":"The AF is continually enhancing, upgrading, and/or replacing software systems and applications to meet mission and user needs. This presents an ideal opportunity to introduce test automation practices so that future iterations of the software development process will show greater: testing efficiency through faster execution testing effectiveness through additional functional coverage test repeatability through programmed execution improvement in timely reporting of system quality","title":"2.3 Benfits to AF"},{"location":"test/#24-avoiding-pitfalls","text":"Individuals involved in testing and test automation come from various walks of life. There is no Tester University or College of Automation. This creates a situation where there is a lack of consistency and knowledge that someone in this line of work should know in order to perform effectively. Over time there have been training and certification programs in DoD and industry that support the need for skills and knowledge of career testers and automators. However, success in automation is not solely a technical challenge. It is a multifaceted discipline requiring management, engineering, and other disciplines. This Test Automation Playbook will guide the AF on how to best approach automated testing.","title":"2.4 Avoiding Pitfalls"}]}